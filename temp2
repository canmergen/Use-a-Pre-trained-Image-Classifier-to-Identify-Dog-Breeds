from typing import Any, Dict, List, Optional, Tuple, Union
import re

# ---- opsiyonel hızlı fuzzy (RapidFuzz varsa kullan) ----
try:
    from rapidfuzz.fuzz import ratio as _fz_ratio
except Exception:
    from difflib import SequenceMatcher
    def _fz_ratio(a, b):  # 0-100
        return int(100 * SequenceMatcher(None, a, b).ratio())

def extract_name_boxes_ner_only_strong(
    pages_scaled: Any,
    spacy_nlp,                              # ZORUNLU: spaCy NER (tr_core_news_trf önerilir)
    ocr_conf_min: float = 0.80,             # 0.80 başla; ihtiyaç olursa yükselt
    fuzzy_blacklist: Optional[List[str]] = None,
    fuzzy_cutoff: int = 92,                 # yalnız bağımsız token eşleşirse sil
    min_tokens: Tuple[int, int] = (2, 3),   # ad-soyad veya ad-ikinciad-soyad
    debug: bool = False
) -> List[Dict[str, Any]]:
    """
    Yalnızca spaCy NER ile ad-soyad (PERSON) çıkarır.
    Ham metin + truecase normalize metinde NER çalıştırır, güvenli filtre uygular.
    Dönüş: [{page_index, role='ad_soyad', name, matched_text, name_span, bbox, confidence, source}]
    """

    if fuzzy_blacklist is None:
        fuzzy_blacklist = ["imza","mühür","muhur","kaşe","kase","stamp","seal",
                           "signature","tasdik","tescil","onay","noter"]

    # ---- yardımcılar ----
    def _ensure_pages(pages_obj: Any) -> List[List[Dict[str, Any]]]:
        if not pages_obj:
            return []
        if isinstance(pages_obj, list) and pages_obj and isinstance(pages_obj[0], dict):
            return [pages_obj]
        return pages_obj

    def _norm_ws(s: str) -> str:
        s = s.replace("|","l").replace("’","'").replace("“",'"').replace("”",'"')
        return re.sub(r"\s+", " ", s).strip()

    def _safe_blacklist_clean(text: str) -> str:
        # Sadece bağımsız token’ı (harf/rakam dizisi) sil; alt-dizini silme.
        toks = re.findall(r"\w+|\W+", text, flags=re.UNICODE)
        out = []
        for t in toks:
            core = t.strip()
            if core and re.fullmatch(r"\w+", core, flags=re.UNICODE):
                low = core.lower()
                if any(_fz_ratio(low, b) >= fuzzy_cutoff for b in fuzzy_blacklist):
                    if debug: print(f"[blk] drop token '{core}'")
                    continue
            out.append(t)
        return "".join(out)

    def _truecase_for_ocr(s: str) -> str:
        # ALLCAPS blokları Title Case yap (kısaltma/tek harf kısaltmaya dokunma)
        words = s.split()
        fixed = []
        for w in words:
            if len(w) > 1 and re.fullmatch(r"[A-ZÇĞİÖŞÜ]+", w):
                fixed.append(w.title())
            else:
                fixed.append(w)
        return " ".join(fixed)

    def _looks_like_name(frag: str) -> bool:
        if any(ch.isdigit() for ch in frag):
            return False
        parts = frag.split()
        if not (min_tokens[0] <= len(parts) <= min_tokens[1]):
            return False
        return all(re.fullmatch(r"[A-Za-zÇĞİÖŞÜçğıöşü.]+", p) for p in parts)

    def _casefix(s: str) -> str:
        out = []
        for p in s.split():
            if len(p) == 2 and p.endswith(".") and p[0].isalpha():
                out.append(p.upper())
            elif p.isupper():
                out.append(p.title())
            else:
                out.append(p)
        return " ".join(out)

    def _run_spacy(text: str) -> List[Tuple[int,int]]:
        ents = []
        try:
            doc = spacy_nlp(text)
            for e in getattr(doc, "ents", []):
                if str(e.label_).upper() == "PERSON":
                    ents.append((int(e.start_char), int(e.end_char)))
        except Exception as ex:
            if debug: print("[spacy] error:", ex)
        return ents

    pages = _ensure_pages(pages_scaled)
    results: List[Dict[str, Any]] = []

    for pi, items in enumerate(pages):
        for it in items:
            # OCR güveni
            conf = it.get("confidence")
            if isinstance(conf, (int, float)) and conf < ocr_conf_min:
                if debug: print(f"[skip] low conf {conf:.2f}")
                continue

            raw = it.get("text") or it.get("txt") or ""
            if not raw:
                continue

            text0 = _norm_ws(raw)
            text0 = _safe_blacklist_clean(text0)

            # 1) Ham metinde NER
            spans = _run_spacy(text0)

            # 2) Hiç bulamazsa: truecase ile yeniden dene
            source = "ner_raw"
            if not spans:
                text1 = _truecase_for_ocr(text0)
                if text1 != text0:
                    spans = _run_spacy(text1)
                    source = "ner_truecase"
                else:
                    source = "ner_raw"

            if not spans:
                continue

            bbox = it.get("bbox") or {}
            for s, e in spans:
                frag = (text1 if source == "ner_truecase" else text0)[s:e].strip()
                if not _looks_like_name(frag):
                    if debug: print(f"[skip] not name '{frag}'")
                    continue
                results.append({
                    "page_index": pi,
                    "role": "ad_soyad",
                    "name": _casefix(frag),
                    "matched_text": frag,
                    "name_span": (s, e),
                    "bbox": {
                        "x_min": bbox.get("x_min"),
                        "y_min": bbox.get("y_min"),
                        "width": bbox.get("width"),
                        "height": bbox.get("height"),
                    },
                    "confidence": float(conf) if isinstance(conf, (int, float)) else None,
                    "source": source
                })

    # tekilleştir
    def _k(r):
        b = r["bbox"]; s = r["name_span"]
        return (r["page_index"], r["name"], b.get("x_min"), b.get("y_min"), b.get("width"), b.get("height"), s)
    uniq, seen = [], set()
    for r in results:
        k = _k(r)
        if k not in seen:
            seen.add(k); uniq.append(r)
    return uniq

import spacy
nlp = spacy.load("tr_core_news_trf")

names = extract_name_boxes_ner_only_strong(
    pages_scaled,
    spacy_nlp=nlp,
    ocr_conf_min=0.80,   # önce 0.80 dene; fazla gelirse 0.85–0.90
    fuzzy_cutoff=92,
    debug=False
)