from typing import Any, Dict, List, Optional, Tuple, Union
import re, unicodedata

def extract_name_boxes_ner_only_strong(
    pages_scaled: Union[List[Dict[str, Any]], Dict[str, Any]],
    nlp,                               # spaCy model (tr_core_news_trf önerilir)
    ocr_conf_min: float = 0.70,
    min_tokens: Tuple[int, int] = (2, 4),
    fuzzy_blacklist: Optional[List[str]] = None,
    debug: bool = False,
) -> List[Dict[str, Any]]:
    """Her OCR kutusunu bağımsız değerlendirir; kutu PERSON ise aynı bbox ile döndürür."""

    # ---------------- Yardımcılar ----------------
    def _ensure_pages(obj):
        if not obj:
            return []
        if isinstance(obj, dict):
            if "items" in obj and isinstance(obj["items"], list):
                return [obj]
            return [{"items": obj if isinstance(obj, list) else []}]
        out = []
        for p in obj:
            if isinstance(p, dict) and "items" in p:
                out.append(p)
            elif isinstance(p, list):
                out.append({"items": p})
        return out

    def _norm_ws(s: str) -> str:
        s = (s or "").replace("|", " ").replace("•", " ").replace("—", "-").replace("–", "-")
        s = re.sub(r"\s+", " ", s).strip()
        return s

    def _fold_tr(s: str) -> str:
        # Unicode -> NFKD, diakritik temizle, TR harf eşlemeleri
        s = s or ""
        s = s.lower()
        s = unicodedata.normalize("NFKD", s)
        s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")  # diacritics off
        # bazı ortamlarda 'i̇' birleşik gelir; aşağıdaki replace garanti eder
        s = s.replace("ı", "i").replace("İ", "i").replace("i̇", "i")
        s = (s.replace("ş","s").replace("ç","c").replace("ğ","g")
               .replace("ö","o").replace("ü","u"))
        return s

    if fuzzy_blacklist is None:
        fuzzy_blacklist = [
            "imza","mühür","muhur","kaşe","kase","stamp","seal",
            "signature","tasdik","tescil","onay","noter","sayfa","page",
        ]
    BLK_SINGLE = set(_norm_ws(b).lower() for b in fuzzy_blacklist)

    STOP_TOKENS = {
        # roller/kurumlar
        "toplanti","toplantı","baskan","başkan","baskani","başkanı","divan",
        "katip","kâtip","uyesi","üyesi","temsilcisi","noter","müdürü","muduru",
        "mudurlugu","müdürlüğü","müdürlük","mudurluk","ticaret","sicil","şirketi","sirketi",
        "limited","anonim","bankasi","bankası","merkezi",
        # metrik/başlıklar
        "toplam","pay","adedi","adet","mevcut","nisap","nisabı","nisabi",
        "hesap","hesabi","hesabı","tutar","tutarı","sayisi","sayısı","oran","yuzde","yüzde",
    }

    # Hızlı ham-pattern (fold etmeden) – İ/ı varyantlarına duyarsız
    ASLI_GIBIDIR_RAW = re.compile(r"\bA[SŞ]L[İIıi]\b.*\bG[İIıi]B[İIıi]D[İIıi]R\b", re.IGNORECASE)
    ASLINA_UYGUNDUR_RAW = re.compile(r"\bA[SŞ]L[İIıi]NA\b.*\bUYGUNDUR\b", re.IGNORECASE)

    def _has_stop(text: str) -> bool:
        # Ham metinde damga ibarelerini yakala
        if ASLI_GIBIDIR_RAW.search(text or "") or ASLINA_UYGUNDUR_RAW.search(text or ""):
            return True

        t_norm = _norm_ws(text)
        t_fold = _fold_tr(t_norm)

        # Fold edilmiş metinde de kontrol et (ek güvenlik)
        if re.search(r"\basli\s+gibidir\b", t_fold) or re.search(r"\baslina\s+uygundur\b", t_fold):
            return True

        toks = t_fold.split()
        for tok in toks:
            if tok in STOP_TOKENS:
                # 'asli' tek başına isim olabilir; 'gibidir/uygundur' eşlik etmiyorsa eleme
                if tok == "asli" and not any(x in t_fold for x in ["asli gibidir","asli uygundur"]):
                    continue
                return True
        return False

    def _is_blacklisted_token(tok: str) -> bool:
        return tok.lower() in BLK_SINGLE

    def _title_soft(s: str) -> str:
        words = s.split()
        out = []
        for w in words:
            if len(w) == 1:
                out.append(w)
            elif re.fullmatch(r"[A-ZÇĞİÖŞÜ]+", w):
                out.append(w.title())
            else:
                out.append(w)
        return " ".join(out)

    def _name_like_by_rule(raw: str) -> bool:
        txt = _norm_ws(raw)
        if not txt or _has_stop(txt):
            return False

        parts = txt.split()
        all_caps = all(re.fullmatch(r"[A-ZÇĞİÖŞÜ]+\.?", p) for p in parts) and len(parts) > 0
        title_parts = _title_soft(txt).split()
        n_title = sum(1 for p in title_parts if re.fullmatch(r"[A-ZÇĞİÖŞÜ][a-zçğıöşüİ]+\.?", p))

        if not (min_tokens[0] <= len(parts) <= min_tokens[1]):
            return False
        if any(any(ch.isdigit() for ch in p) for p in parts):
            return False
        if any(_is_blacklisted_token(p) for p in parts):
            return False
        if not all(re.fullmatch(r"[A-Za-zÇĞİÖŞÜçğıöşüİ]+\.?", p) for p in parts):
            return False

        return True if all_caps else (n_title >= 2)

    # ---------------- Ana döngü ----------------
    pages = _ensure_pages(pages_scaled)
    results: List[Dict[str, Any]] = []

    for pi, page in enumerate(pages):
        for it in page.get("items", []) or []:
            if not isinstance(it, dict):
                continue

            raw = _norm_ws(it.get("text") or it.get("txt") or "")
            if not raw:
                continue
            if _has_stop(raw):
                if debug: print(f"[stop] {raw}")
                continue

            conf = it.get("confidence")
            bbox = it.get("bbox") or {}

            # 1) spaCy NER
            is_person = False
            matched_text = raw
            try:
                doc = nlp(_title_soft(raw))
                for ent in doc.ents:
                    if str(ent.label_).upper() != "PERSON":
                        continue
                    ent_txt = doc.text[ent.start_char:ent.end_char]
                    cov = len(ent_txt.replace(" ", "")) / max(1, len(doc.text.replace(" ", "")))
                    if cov >= 0.8:
                        is_person = True
                        matched_text = raw[ent.start_char:ent.end_char] if (0 <= ent.start_char < len(raw)) else raw
                        break
            except Exception as ex:
                if debug: print("spaCy error:", ex)

            # 2) Kural
            if not is_person and _name_like_by_rule(raw):
                is_person = True
                matched_text = raw

            # 3) Conf filtresi (emin olamadıysak)
            if not is_person:
                if isinstance(conf, (int, float)) and conf < ocr_conf_min:
                    if debug: print(f"[skip-conf] {conf:.2f} -> {raw}")
                    continue
                if not _name_like_by_rule(raw):
                    continue
                is_person = True
                matched_text = raw

            results.append({
                "page_index": pi,
                "role": "PERSON",
                "name": _norm_ws(matched_text).upper(),
                "matched_text": matched_text,
                "name_span": None,
                "boxes": {
                    "x_min": int(bbox.get("x_min", bbox.get("x", 0))),
                    "y_min": int(bbox.get("y_min", bbox.get("y", 0))),
                    "width": int(bbox.get("width", bbox.get("w", 0))),
                    "height": int(bbox.get("height", bbox.get("h", 0))),
                },
                "confidence": float(conf) if isinstance(conf, (int, float)) else None,
                "source": "box_ner" if matched_text != raw else "box_rule",
            })

    # ---------------- Dedup ----------------
    seen, uniq = set(), []
    for r in results:
        b = r["boxes"]
        k = (r["page_index"], r["name"], b["x_min"], b["y_min"], b["width"], b["height"])
        if k in seen: 
            continue
        seen.add(k); uniq.append(r)
    return uniq