from typing import Any, Dict, List, Optional, Tuple, Union
import re

def extract_name_boxes_ner_only_strong(
    pages_scaled: Union[List[Dict[str, Any]], Dict[str, Any]],
    nlp,                               # spaCy model: tr_core_news_trf önerilir
    ocr_conf_min: float = 0.70,        # düşük konf. kutuları at
    min_tokens: Tuple[int,int] = (2,4),# ad-soyad(+ikinci ad) 2–4 kelime
    fuzzy_blacklist: Optional[List[str]] = None,  # single-token blacklist
    line_merge_px: Optional[int] = None,          # KULLANILMAZ (geriye uyum için)
    x_gap_max_px: Optional[int] = None,           # KULLANILMAZ (geriye uyum için)
    debug: bool = False,
) -> List[Dict[str, Any]]:
    """
    Her OCR kutusunu bağımsız değerlendirir; kutu metni PERSON ise aynı bbox ile döndürür.
    Bbox üzerinde hiçbir genişletme/birleştirme yapılmaz.
    Dönüş: [{page_index, role:'PERSON', name, matched_text, boxes, confidence, source}]
    """

    # --- yardımcılar ---
    def _ensure_pages(obj):
        if not obj: return []
        if isinstance(obj, dict):
            if "items" in obj and isinstance(obj["items"], list):
                return [obj]
            return [{"items": obj if isinstance(obj, list) else []}]
        out = []
        for p in obj:
            if isinstance(p, dict) and "items" in p: out.append(p)
            elif isinstance(p, list): out.append({"items": p})
        return out

    def _norm_ws(s: str) -> str:
        s = (s or "").replace("|"," ").replace("•"," ").replace("—","-").replace("–","-")
        s = re.sub(r"\s+", " ", s).strip()
        return s

    # tekil token blacklist (sadece tek kelime halinde görünürse elenir)
    if fuzzy_blacklist is None:
        fuzzy_blacklist = [
            "imza","mühür","muhur","kaşe","kase","stamp","seal",
            "signature","tasdik","tescil","onay","noter","sayfa","page"
        ]
    BLK = set(_norm_ws(b).lower() for b in fuzzy_blacklist)

    def _is_blacklisted_token(tok: str) -> bool:
        return tok.lower() in BLK

    def _looks_like_name(text: str) -> bool:
        # Rakam içermez
        if any(ch.isdigit() for ch in text): return False
        parts = [p for p in text.split() if p]
        if not (min_tokens[0] <= len(parts) <= min_tokens[1]): return False
        if any(_is_blacklisted_token(p) for p in parts): return False
        # Her parça (Türkçe harfler dahil) harflerden oluşmalı
        return all(re.fullmatch(r"[A-Za-zÇĞİÖŞÜçğıöşüİ]+\.?", p) for p in parts)

    def _title_soft(s: str) -> str:
        # ALLCAPS metni TitleCase'e yumuşatır (HÜSEYİN -> Hüseyin)
        words = s.split()
        out = []
        for w in words:
            if len(w) == 1:
                out.append(w)
            elif re.fullmatch(r"[A-ZÇĞİÖŞÜ]+", w):
                out.append(w.title())
            else:
                out.append(w)
        return " ".join(out)

    pages = _ensure_pages(pages_scaled)
    results: List[Dict[str,Any]] = []

    for pi, page in enumerate(pages):
        for it in page.get("items", []) or []:
            if not isinstance(it, dict): continue
            conf = it.get("confidence")
            if isinstance(conf, (int,float)) and conf < ocr_conf_min:
                if debug: print(f"[skip] low conf {conf:.2f}")
                continue

            raw = _norm_ws(it.get("text") or it.get("txt") or "")
            if not raw: continue
            bbox = it.get("bbox") or {}

            # 1) spaCy NER: kutu metninin tamamında PERSON var mı?
            text_for_ner = _title_soft(raw)
            is_person = False
            matched_text = raw
            try:
                doc = nlp(text_for_ner)
                # PERSON etiketli tek bir entity çıkarsa ve metnin büyük kısmını kapsıyorsa kabul
                for ent in doc.ents:
                    if str(ent.label_).upper() != "PERSON":
                        continue
                    # kapsama oranı (boşluklar hariç) >= 0.8
                    ent_txt = text_for_ner[ent.start_char:ent.end_char]
                    cov = len(ent_txt.replace(" ","")) / max(1, len(text_for_ner.replace(" ","")))
                    if cov >= 0.8:
                        is_person = True
                        matched_text = raw[ent.start_char:ent.end_char] if (0 <= ent.start_char < len(raw)) else raw
                        break
            except Exception as ex:
                if debug: print("spaCy error:", ex)

            # 2) NER kaçırırsa: kural tabanlı kontrol (yalnızca KUTU metninde)
            if not is_person and _looks_like_name(_title_soft(raw)):
                is_person = True
                matched_text = raw

            if not is_person:
                continue

            results.append({
                "page_index": pi,
                "role": "PERSON",
                "name": _norm_ws(matched_text).upper(),
                "matched_text": matched_text,
                "name_span": None,   # kutu bazlı çalıştığımız için karakter aralığı opsiyonel
                "boxes": {
                    "x_min": int(bbox.get("x_min", bbox.get("x", 0))),
                    "y_min": int(bbox.get("y_min", bbox.get("y", 0))),
                    "width": int(bbox.get("width", bbox.get("w", 0))),
                    "height": int(bbox.get("height", bbox.get("h", 0))),
                },
                "confidence": float(conf) if isinstance(conf, (int,float)) else None,
                "source": "box_ner" if matched_text != raw else "box_rule_or_ner",
            })

    # Dedup: aynı sayfa + aynı isim + aynı bbox
    seen, uniq = set(), []
    for r in results:
        b = r["boxes"]
        k = (r["page_index"], r["name"], b["x_min"], b["y_min"], b["width"], b["height"])
        if k in seen: 
            continue
        seen.add(k); uniq.append(r)
    return uniq