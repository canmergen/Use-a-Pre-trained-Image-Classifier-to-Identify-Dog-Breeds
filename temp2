import re
import unicodedata
from typing import List, Dict, Tuple
import pandas as pd
from rapidfuzz import fuzz

def standardize_table_columns(table_dfs_cleaned: List[pd.DataFrame],
                              match_threshold: float = 70.0
                              ) -> List[pd.DataFrame]:
    """
    table_dfs_cleaned içindeki df'lerin kolon adlarını tek şemaya çevirir.

    Dönüş: Aynı uzunlukta liste, her df'te kolonlar:
        [
            "page_index",
            "pay_sahibinin_adi_soyadi_unvani",
            "pay_sahibi_tckn",
            "pay_sahibi_uyrugu",
            "pay_sahibi_adresi",
            "paylarin_toplam_itibari_degeri",
            "paylarin_edinin_sekli_tarihi",
            "katilim_sekli",
            "temsilci_adi_soyadi_unvani",
            "temsilci_tckn",
            "tablo_imza_var_mi",
        ]
    Olmayan kolonlar NaN ile doldurulur.
    """

    # ---- 1) Türkçe normalizasyon & token çıkarma ----
    TR_MAP = str.maketrans({
        "İ": "I", "I": "I", "ı": "i", "i": "i",
        "Ş": "S", "ş": "s",
        "Ğ": "G", "ğ": "g",
        "Ü": "U", "ü": "u",
        "Ö": "O", "ö": "o",
        "Ç": "C", "ç": "c",
    })

    def normalize(s: str) -> str:
        if not isinstance(s, str):
            return ""
        s = s.translate(TR_MAP)
        s = unicodedata.normalize("NFKD", s)
        s = "".join(ch for ch in s if not unicodedata.combining(ch))
        s = s.lower()
        # harf, rakam ve boşluk dışını at
        s = re.sub(r"[^a-z0-9\s]+", " ", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    def token_set(s: str) -> set:
        return set(normalize(s).split())

    def jaccard(a: str, b: str) -> float:
        ta, tb = token_set(a), token_set(b)
        if not ta or not tb:
            return 0.0
        inter = len(ta & tb)
        union = len(ta | tb)
        return inter / union

    # ---- 2) Hedef şema & hedef açıklamaları ----
    # Açıklamaları geniş tuttum ki fuzzy + jaccard iyi yakalasın
    canonical_schema: List[Tuple[str, str]] = [
        ("page_index",
         "page index sayfa no sira"),
        ("pay_sahibinin_adi_soyadi_unvani",
         "pay sahibinin adi soyadi unvani isim pay sahibi ad soyad"),
        ("pay_sahibi_tckn",
         "pay sahibi tckn tc kimlik vergi kimlik numarasi t c vkn"),
        ("pay_sahibi_uyrugu",
         "pay sahibi uyrugu uyruk nationality"),
        ("pay_sahibi_adresi",
         "pay sahibi adresi adres"),
        ("paylarin_toplam_itibari_degeri",
         "paylarin toplam itibari degeri nominal tutar pay toplam nominal"),
        ("paylarin_edinin_sekli_tarihi",
         "paylarin edinim sekli tarihi edinim sekli edinim tarihi"),
        ("katilim_sekli",
         "katilim sekli katilma sekli"),
        ("temsilci_adi_soyadi_unvani",
         "temsilci adi soyadi unvani vekil ad soyad"),
        ("temsilci_tckn",
         "temsilci tckn tc kimlik vergi kimlik numarasi vekil t c vkn"),
        ("tablo_imza_var_mi",
         "imza tablo imza var mi signature"),
    ]

    canonical_names = [c[0] for c in canonical_schema]

    # ---- 3) Tek kolon için en iyi hedefi bul ----
    def best_match(col_name: str) -> Tuple[str, float]:
        best_canon = None
        best_score = -1.0
        norm_col = normalize(col_name)

        for canon_name, canon_desc in canonical_schema:
            norm_canon = normalize(canon_desc)
            # fuzzy + jaccard birleştir
            fuzzy_score = fuzz.token_set_ratio(norm_col, norm_canon)  # 0–100
            j_score = jaccard(norm_col, norm_canon) * 100.0           # 0–100
            score = 0.6 * fuzzy_score + 0.4 * j_score
            if score > best_score:
                best_score = score
                best_canon = canon_name
        return best_canon, best_score

    standardized_dfs: List[pd.DataFrame] = []

    # ---- 4) Her df için kolonları map et ----
    for df in table_dfs_cleaned:
        col_map: Dict[str, str] = {}  # src_col -> canonical_name

        for src_col in df.columns:
            canon, score = best_match(src_col)
            if score >= match_threshold:
                # Aynı canonical'a iki kolon map olursa, en yüksek skorluya öncelik verilebilir.
                # Basitçe ilk geleni koruyoruz; istersen skor bazlı seçim de ekleyebilirsin.
                if canon not in col_map.values():
                    col_map[src_col] = canon

        # Yeni df'yi hedef şemaya göre kur
        new_df = pd.DataFrame()

        for canon_name in canonical_names:
            # bu canonical için bir kaynak kolon bulunmuş mu?
            src_candidates = [s for s, c in col_map.items() if c == canon_name]
            if src_candidates:
                src = src_candidates[0]
                new_df[canon_name] = df[src]
            else:
                # yoksa boş kolon
                new_df[canon_name] = pd.NA

        standardized_dfs.append(new_df)

    return standardized_dfs

std_table_dfs = standardize_table_columns(table_dfs_cleaned, match_threshold=70)

# Örnek: ilk tabloyu gör
display(std_table_dfs[0].head())