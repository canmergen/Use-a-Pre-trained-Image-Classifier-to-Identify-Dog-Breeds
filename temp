# -*- coding: utf-8 -*-
import re
import json
from dataclasses import dataclass, asdict
from typing import Optional, List, Tuple

PDF_PATH = "/mnt/data/Tebliğ.pdf"
OUT_JSONL = "/mnt/data/docling_chunks.jsonl"

DOC_ID = "teblig_2021_31676"
DOC_NAME = "ÖDEME VE ELEKTRONİK PARA KURULUŞLARI ... TEBLİĞ"

# -----------------------------
# 1) Docling ile dönüştür
# -----------------------------
def docling_to_markdown(pdf_path: str) -> str:
    # Docling quickstart örneği: DocumentConverter().convert(...).document :contentReference[oaicite:4]{index=4}
    from docling.document_converter import DocumentConverter

    converter = DocumentConverter()
    result = converter.convert(pdf_path)
    doc = result.document

    # Docling farklı exportlar destekler; en pratik yol markdown export (API sürüme göre değişebilir).
    # Eğer doc.export_to_markdown() yoksa, doc.export(...) gibi bir fonksiyon olabilir.
    # Aşağıdaki satırı kendi ortamında doc objesinin metodlarına göre uyarlayacaksın.
    if hasattr(doc, "export_to_markdown"):
        md = doc.export_to_markdown()
    elif hasattr(doc, "to_markdown"):
        md = doc.to_markdown()
    else:
        # fallback: düz text
        md = getattr(doc, "text", "") or str(doc)
    return md


# -----------------------------
# 2) Markdown normalize
# -----------------------------
def normalize_md(md: str) -> str:
    md = md.replace("\u00ad", "")
    md = re.sub(r"[ \t]+", " ", md)
    md = re.sub(r"\n{3,}", "\n\n", md)
    return md.strip()


# -----------------------------
# 3) Regülasyon için madde bazlı parse (Markdown üzerinden)
# -----------------------------
MADDE_RE = re.compile(
    r"(?mi)^\s*(MADDE)\s+(\d+)\s*[\-–—]\s*(.*?)\s*$"
)

FIKRA_RE = re.compile(r"(\(\d+\))")

@dataclass
class Chunk:
    chunk_id: str
    doc_id: str
    doc_name: str
    source_file: str
    article_no: int
    article_title: str
    paragraph_no: Optional[int]
    chunk_type: str  # paragraph/article
    text: str
    char_len: int


def split_articles_from_md(md: str) -> List[Tuple[int, str, str]]:
    """
    Döner: [(article_no, article_title, article_body)]
    """
    lines = md.splitlines()
    # MADDE başlıklarının line indexlerini bul
    idxs = []
    for i, ln in enumerate(lines):
        m = MADDE_RE.match(ln.strip())
        if m:
            art_no = int(m.group(2))
            title = (m.group(3) or "").strip()
            idxs.append((i, art_no, title))

    if not idxs:
        raise RuntimeError("Markdown içinde 'MADDE X – ...' patterni bulunamadı.")

    articles = []
    for k, (start_i, art_no, title) in enumerate(idxs):
        end_i = idxs[k+1][0] if k+1 < len(idxs) else len(lines)
        body = "\n".join(lines[start_i+1:end_i]).strip()
        articles.append((art_no, title or f"MADDE {art_no}", body))
    return articles


def split_fikra(text: str) -> List[Tuple[Optional[int], str]]:
    """
    (1) ... (2) ... şeklinde ayırır. Yoksa tek parça.
    """
    t = text.strip()
    if not t:
        return []

    markers = list(FIKRA_RE.finditer(t))
    if not markers:
        return [(None, t)]

    out = []
    for i, mk in enumerate(markers):
        s = mk.start()
        e = markers[i+1].start() if i+1 < len(markers) else len(t)
        seg = t[s:e].strip()
        mnum = re.match(r"\((\d+)\)", seg)
        pno = int(mnum.group(1)) if mnum else None
        out.append((pno, seg))
    return out


def sentence_pack(text: str, max_chars: int = 1100, overlap_sentences: int = 1) -> List[str]:
    """
    Çok uzun fıkralar için kelime kırmadan cümle bazlı paketleme.
    """
    sents = re.split(r"(?<=[\.\!\?;:])\s+", text.strip())
    sents = [s.strip() for s in sents if s.strip()]

    chunks = []
    buf = []
    cur = 0
    for s in sents:
        if cur + len(s) + 1 > max_chars and buf:
            chunks.append(" ".join(buf).strip())
            buf = buf[-overlap_sentences:] if overlap_sentences > 0 else []
            cur = sum(len(x) for x in buf) + len(buf)
        buf.append(s)
        cur += len(s) + 1
    if buf:
        chunks.append(" ".join(buf).strip())
    return chunks


def build_chunks(md: str) -> List[Chunk]:
    md = normalize_md(md)
    articles = split_articles_from_md(md)

    chunks: List[Chunk] = []
    for art_no, title, body in articles:
        fikralar = split_fikra(body)

        # Eğer body çok kısaysa madde bazlı tek chunk da tutmak isteyebilirsin (opsiyonel)
        for (pno, ptext) in fikralar:
            # çok uzunsa cümle bazlı parçala
            parts = [ptext] if len(ptext) <= 1100 else sentence_pack(ptext, max_chars=1100, overlap_sentences=1)
            for j, part in enumerate(parts):
                cid = f"{DOC_ID}::madde_{art_no}::fikra_{pno if pno is not None else 'na'}::{j}"
                chunks.append(Chunk(
                    chunk_id=cid,
                    doc_id=DOC_ID,
                    doc_name=DOC_NAME,
                    source_file="Tebliğ.pdf",
                    article_no=art_no,
                    article_title=title,
                    paragraph_no=pno,
                    chunk_type="paragraph",
                    text=part.strip(),
                    char_len=len(part.strip())
                ))
    return chunks


def write_jsonl(chunks: List[Chunk], path: str):
    with open(path, "w", encoding="utf-8") as f:
        for c in chunks:
            f.write(json.dumps(asdict(c), ensure_ascii=False) + "\n")


def main():
    md = docling_to_markdown(PDF_PATH)
    chunks = build_chunks(md)
    write_jsonl(chunks, OUT_JSONL)

    print("Markdown length:", len(md))
    print("Chunks:", len(chunks))
    print("Output:", OUT_JSONL)
    print("\n--- First 3 chunks ---")
    for c in chunks[:3]:
        print(c.chunk_id, c.article_no, c.paragraph_no, c.char_len)
        print(c.text[:200].replace("\n", " "), "...\n")


if __name__ == "__main__":
    main()