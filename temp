# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import re
import json
from dataclasses import dataclass, asdict
from typing import List, Optional, Dict, Any, Tuple

import fitz  # PyMuPDF


# -------------------------
# Config
# -------------------------
PDF_PATH = "/mnt/data/tcmb_teblig.pdf"
OUT_JSONL = "/mnt/data/tcmb_teblig_chunks.jsonl"

DOC_ID = "tcmb_teblig"
DOC_NAME = "TCMB TEBLİĞ"

# Chunk control: ONLY split inside the same item if it is too long
MAX_CHARS = 1800
OVERLAP_CHARS = 200  # only used when an item text is split into multiple parts

# -------------------------
# Regex
# -------------------------
BOLUM_RE = re.compile(
    r"^\s*((?:[IVXLC]+)|BİRİNCİ|İKİNCİ|ÜÇÜNCÜ|DÖRDÜNCÜ|BEŞİNCİ|ALTINCI|YEDİNCİ|SEKİZİNCİ|DOKUZUNCU|ONUNCU)\s+BÖLÜM\s*$",
    re.IGNORECASE
)

# Article start (MADDE / GEÇİCİ MADDE) - handles "MADDE 1 – ..." in same line
ARTICLE_RE = re.compile(
    r"^\s*(GEÇİCİ\s+)?MADDE\s+(\d+)\s*(?:[-–]\s*)?(.*)\s*$",
    re.IGNORECASE
)

# EK lines (e.g. "Ek-1 ..." or "EK-1 ..." or "EK 1 ...")
EK_RE = re.compile(
    r"^\s*EK\s*[-–]?\s*(\d+)\s*(.*)\s*$",
    re.IGNORECASE
)

# Item markers inside an article:
#  a) / b) / ç) ...
ITEM_LETTER_RE = re.compile(r"^\s*([a-zçğıöşü])\)\s+(.*)$", re.IGNORECASE)
#  1) / 2) / 10)
ITEM_NUM_RE = re.compile(r"^\s*(\d+)\)\s+(.*)$")
#  (1) / (2) / (10)
ITEM_PAREN_NUM_RE = re.compile(r"^\s*\((\d+)\)\s+(.*)$")
#  1. / 2. (useful in Ek lists sometimes)
ITEM_DOT_NUM_RE = re.compile(r"^\s*(\d+)\.\s+(.*)$")

# -------------------------
# Utils
# -------------------------
def normalize_ws(s: str) -> str:
    s = s.replace("\u00ad", "")  # soft hyphen
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def safe_slug(s: str) -> str:
    s = s.lower().strip()
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"[^a-z0-9çğıöşü_]+", "", s)
    return s

def split_with_overlap(text: str, max_chars: int, overlap: int) -> List[str]:
    """
    Split long text into parts with overlap, without crossing item boundary.
    Tries not to cut mid-word too aggressively.
    """
    text = text.strip()
    if len(text) <= max_chars:
        return [text]

    parts = []
    start = 0
    n = len(text)

    while start < n:
        end = min(n, start + max_chars)
        chunk = text[start:end]

        # try to backtrack to a whitespace to avoid ugly cuts
        if end < n:
            back = chunk.rfind(" ")
            if back > max_chars * 0.6:
                end = start + back
                chunk = text[start:end]

        parts.append(chunk.strip())

        if end >= n:
            break

        start = max(0, end - overlap)

    # de-dup tiny overlaps
    cleaned = []
    for p in parts:
        if p and (not cleaned or p != cleaned[-1]):
            cleaned.append(p)
    return cleaned

def detect_item_start(line: str) -> Optional[Tuple[str, str, str]]:
    """
    Returns (item_key, item_type, rest_text) if line starts a new item.
    item_key examples: "a)", "1)", "(1)", "1."
    """
    t = line.strip()
    m = ITEM_LETTER_RE.match(t)
    if m:
        return f"{m.group(1).lower()})", "letter", m.group(2).strip()

    m = ITEM_NUM_RE.match(t)
    if m:
        return f"{m.group(1)})", "num", m.group(2).strip()

    m = ITEM_PAREN_NUM_RE.match(t)
    if m:
        return f"({m.group(1)})", "paren_num", m.group(2).strip()

    m = ITEM_DOT_NUM_RE.match(t)
    if m:
        return f"{m.group(1)}.", "dot_num", m.group(2).strip()

    return None

# -------------------------
# PDF line extraction (stable reading order)
# -------------------------
@dataclass
class Line:
    page: int
    text: str
    x0: float
    y0: float

def extract_lines(doc: fitz.Document) -> List[Line]:
    lines: List[Line] = []
    for pi in range(len(doc)):
        page = doc.load_page(pi)
        d = page.get_text("dict")
        page_no = pi + 1

        for b in d.get("blocks", []):
            if b.get("type", 0) != 0:
                continue
            for ln in b.get("lines", []):
                spans = ln.get("spans", [])
                if not spans:
                    continue
                txt = "".join(sp.get("text", "") for sp in spans)
                txt = normalize_ws(txt)
                if not txt:
                    continue
                x0 = min(sp.get("bbox", [0, 0, 0, 0])[0] for sp in spans)
                y0 = min(sp.get("bbox", [0, 0, 0, 0])[1] for sp in spans)
                lines.append(Line(page=page_no, text=txt, x0=float(x0), y0=float(y0)))

    lines.sort(key=lambda L: (L.page, L.y0, L.x0))
    return lines

# -------------------------
# Chunk schema
# -------------------------
@dataclass
class Chunk:
    chunk_id: str
    doc_id: str
    doc_name: str
    source_file: str
    page_start: int
    page_end: int

    bolum: Optional[str]
    bolum_no: Optional[str]

    article_kind: Optional[str]     # "MADDE" / "GEÇİCİ MADDE" / "EK"
    article_no: Optional[int]
    article_title: Optional[str]    # text after dash if present (optional)

    item_key: Optional[str]         # "(1)" / "a)" / "1)" ...
    item_type: Optional[str]        # paren_num / letter / num / dot_num
    part_no: int                    # 0..n (only if long split)

    text: str
    char_len: int

# -------------------------
# Main parser (MADDE -> item chunks)
# -------------------------
def chunk_pdf_legal_items(pdf_path: str, out_jsonl: str) -> List[Chunk]:
    assert os.path.exists(pdf_path), f"PDF yok: {pdf_path}"
    doc = fitz.open(pdf_path)

    # sanity: is text-based?
    total_chars = sum(len(doc.load_page(i).get_text("text").strip()) for i in range(len(doc)))
    if total_chars == 0:
        raise RuntimeError("PDF scanned görünüyor (text çıkarılamadı). OCR olmadan chunk üretilemez.")

    lines = extract_lines(doc)
    if not lines:
        raise RuntimeError("Hiç line çıkarılamadı (PyMuPDF dict boş).")

    source_file = os.path.basename(pdf_path)

    chunks: List[Chunk] = []

    cur_bolum = None
    cur_bolum_no = None

    cur_article_kind = None
    cur_article_no = None
    cur_article_title = None
    cur_article_page_start = None
    cur_article_page_end = None

    cur_item_key = None
    cur_item_type = None
    cur_item_buf: List[str] = []
    cur_item_page_start = None
    cur_item_page_end = None

    def flush_item():
        nonlocal cur_item_key, cur_item_type, cur_item_buf, cur_item_page_start, cur_item_page_end
        if cur_article_kind is None or cur_article_no is None:
            # no active article -> do not emit
            cur_item_key = None
            cur_item_type = None
            cur_item_buf = []
            cur_item_page_start = None
            cur_item_page_end = None
            return

        text = normalize_ws("\n".join(cur_item_buf))
        if not text:
            cur_item_key = None
            cur_item_type = None
            cur_item_buf = []
            cur_item_page_start = None
            cur_item_page_end = None
            return

        # split long item into parts (still same item)
        parts = split_with_overlap(text, MAX_CHARS, OVERLAP_CHARS)

        for part_no, part_text in enumerate(parts):
            cid = f"{DOC_ID}::"
            cid += f"{cur_article_kind.lower().replace(' ','_')}_{cur_article_no:03d}"
            if cur_item_key:
                cid += f"::item_{safe_slug(cur_item_key)}"
            cid += f"::part_{part_no:02d}"

            chunks.append(Chunk(
                chunk_id=cid,
                doc_id=DOC_ID,
                doc_name=DOC_NAME,
                source_file=source_file,
                page_start=cur_item_page_start or cur_article_page_start or 1,
                page_end=cur_item_page_end or cur_article_page_end or (cur_item_page_start or 1),

                bolum=cur_bolum,
                bolum_no=cur_bolum_no,

                article_kind=cur_article_kind,
                article_no=cur_article_no,
                article_title=cur_article_title,

                item_key=cur_item_key,
                item_type=cur_item_type,
                part_no=part_no,

                text=part_text,
                char_len=len(part_text)
            ))

        # reset item
        cur_item_key = None
        cur_item_type = None
        cur_item_buf = []
        cur_item_page_start = None
        cur_item_page_end = None

    def flush_article_remaining_as_single_item_if_needed():
        """
        If an article had no detected items, we still want one chunk (item_key=None)
        """
        nonlocal cur_item_key, cur_item_type
        if cur_article_kind is None or cur_article_no is None:
            return
        if chunks and any((c.article_kind == cur_article_kind and c.article_no == cur_article_no) for c in chunks):
            return  # already emitted at least one item for this article

        # Create a fallback buffer from whatever is in cur_item_buf (if any)
        # If empty, do nothing.
        if cur_item_buf:
            cur_item_key = None
            cur_item_type = None
            flush_item()

    def start_new_article(kind: str, no: int, title: str, page_no: int, inline_rest: str):
        nonlocal cur_article_kind, cur_article_no, cur_article_title
        nonlocal cur_article_page_start, cur_article_page_end
        nonlocal cur_item_key, cur_item_type, cur_item_buf, cur_item_page_start, cur_item_page_end

        # close previous item/article
        flush_item()
        flush_article_remaining_as_single_item_if_needed()

        # set new article
        cur_article_kind = kind
        cur_article_no = no
        cur_article_title = normalize_ws(title) if title else None
        cur_article_page_start = page_no
        cur_article_page_end = page_no

        # reset item buffer for this article
        cur_item_key = None
        cur_item_type = None
        cur_item_buf = []
        cur_item_page_start = None
        cur_item_page_end = None

        # handle inline remainder (e.g. "MADDE 1 – (1) ....")
        inline_rest = normalize_ws(inline_rest)
        if inline_rest:
            item = detect_item_start(inline_rest)
            if item:
                k, t, rest = item
                cur_item_key, cur_item_type = k, t
                cur_item_buf = [rest] if rest else []
                cur_item_page_start = page_no
                cur_item_page_end = page_no
            else:
                # no item marker -> keep as article body (fallback chunk if no other items appear)
                cur_item_buf = [inline_rest]
                cur_item_page_start = page_no
                cur_item_page_end = page_no

    # iterate lines
    for L in lines:
        txt = normalize_ws(L.text)
        if not txt:
            continue

        # update article page_end as we move
        if cur_article_page_end is not None:
            cur_article_page_end = max(cur_article_page_end, L.page)

        # BÖLÜM (context only)
        m = BOLUM_RE.match(txt.upper())
        if m:
            # do not close article; just update context
            cur_bolum = txt
            cur_bolum_no = m.group(1).upper()
            continue

        # EK start
        m = EK_RE.match(txt.upper())
        if m:
            ek_no = int(m.group(1))
            ek_title = m.group(2).strip()
            # treat EK as "article"
            start_new_article("EK", ek_no, f"EK {ek_no}", L.page, ek_title)
            continue

        # MADDE start
        m = ARTICLE_RE.match(txt.upper())
        if m:
            is_gecici = bool(m.group(1))
            madde_no = int(m.group(2))
            rest = m.group(3) or ""
            kind = "GEÇİCİ MADDE" if is_gecici else "MADDE"
            title = f"{kind} {madde_no}"
            start_new_article(kind, madde_no, title, L.page, rest)
            continue

        # if no active article, skip (preamble lines etc.)
        if cur_article_kind is None or cur_article_no is None:
            continue

        # item start?
        item = detect_item_start(txt)
        if item:
            # new item begins -> flush previous item and start new
            flush_item()
            k, t, rest = item
            cur_item_key, cur_item_type = k, t
            cur_item_buf = [rest] if rest else []
            cur_item_page_start = L.page
            cur_item_page_end = L.page
            continue

        # normal continuation line inside current item/article
        if cur_item_page_start is None:
            cur_item_page_start = L.page
        cur_item_page_end = L.page
        cur_item_buf.append(txt)

    # end flush
    flush_item()
    flush_article_remaining_as_single_item_if_needed()

    # write JSONL
    with open(out_jsonl, "w", encoding="utf-8") as f:
        for c in chunks:
            f.write(json.dumps(asdict(c), ensure_ascii=False) + "\n")

    return chunks


# -------------------------
# Run
# -------------------------
chunks = chunk_pdf_legal_items(PDF_PATH, OUT_JSONL)
print("pages:", fitz.open(PDF_PATH).page_count)
print("chunks:", len(chunks))
print("wrote:", OUT_JSONL)

# preview
for c in chunks[:8]:
    print("=" * 90)
    print(c.chunk_id, f"pages {c.page_start}-{c.page_end}", "len", c.char_len)
    print("bolum:", c.bolum)
    print("article:", c.article_kind, c.article_no, "title:", c.article_title)
    print("item:", c.item_key, c.item_type, "part:", c.part_no)
    print(c.text[:700], "..." if len(c.text) > 700 else "")



# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import re
import json
from dataclasses import dataclass, asdict
from typing import List, Optional, Dict, Any, Tuple

import fitz  # pymupdf


# =========================
# Config
# =========================
PDF_PATH = "/mnt/data/tcmb_teblig.pdf"
OUT_JSONL = "/mnt/data/tcmb_teblig_chunks.jsonl"

DOC_ID = "tcmb_teblig"
DOC_NAME = "TCMB TEBLİĞ"

# RAG chunk controls (char-based; token bazlı istersen sonra tokenizer bağlanır)
MAX_CHARS = 1800
MIN_CHARS = 450
OVERLAP_CHARS = 200  # sadece aynı MADDE içinde uygulanır

# =========================
# Regex patterns
# =========================
# Madde satırı bazen içerikle aynı satırda geliyor: "MADDE 1 – (1) ...."
MADDE_INLINE_RE = re.compile(
    r"^\s*(GEÇİCİ\s+)?MADDE\s+(\d+)\s*[-–]\s*(.*)\s*$",
    re.IGNORECASE
)
MADDE_ONLY_RE = re.compile(
    r"^\s*(GEÇİCİ\s+)?MADDE\s+(\d+)\s*$",
    re.IGNORECASE
)

# EK (Ek-1, EK 1 vb)
EK_RE = re.compile(r"^\s*EK\s*[-–]?\s*(\d+)\s*[:\-–]?\s*(.*)\s*$", re.IGNORECASE)

# Bölüm başlığı (metadata/context için, chunk title yapmıyoruz)
BOLUM_RE = re.compile(
    r"^\s*([IVXLC]+|BİRİNCİ|İKİNCİ|ÜÇÜNCÜ|DÖRDÜNCÜ|BEŞİNCİ|ALTINCI|YEDİNCİ|SEKİZİNCİ|DOKUZUNCU|ONUNCU)\s+BÖLÜM\s*$",
    re.IGNORECASE
)

# Subtitle / item marker'ları
# a) b) c) (Türkçe harfler dahil)
LETTER_ITEM_RE = re.compile(r"^\s*([a-zçğıöşü])\)\s+(.*)\s*$", re.IGNORECASE)
NUM_ITEM_RE = re.compile(r"^\s*(\d+)\)\s+(.*)\s*$")
PAREN_NUM_ITEM_RE = re.compile(r"^\s*\((\d+)\)\s+(.*)\s*$")

# “MADDE 4 – (1) ...” gibi inline satırda item marker yakalamak için
ITEM_PREFIX_INLINE_RES = [
    ("letter", LETTER_ITEM_RE),
    ("num", NUM_ITEM_RE),
    ("paren_num", PAREN_NUM_ITEM_RE),
]

# =========================
# Utils
# =========================
def normalize_ws(s: str) -> str:
    s = s.replace("\u00ad", "")  # soft hyphen
    s = s.replace("\r", "\n")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def slugify(s: str) -> str:
    s = s.strip().lower()
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"[^a-z0-9_çğıöşü]+", "", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s

def safe_tail_overlap(text: str, n: int) -> str:
    if n <= 0 or len(text) <= n:
        return text
    tail = text[-n:]
    # kelime ortasında kesmeyi azalt
    if " " in tail:
        tail = tail.split(" ", 1)[-1]
    return tail.strip()

def detect_item_marker(line: str) -> Optional[Tuple[str, str, str]]:
    """
    Returns (item_type, item_key, item_rest_text)
    item_type: 'letter'|'num'|'paren_num'
    item_key: 'a' or '1'
    """
    m = LETTER_ITEM_RE.match(line)
    if m:
        return ("letter", m.group(1).lower(), m.group(2).strip())
    m = NUM_ITEM_RE.match(line)
    if m:
        return ("num", m.group(1), m.group(2).strip())
    m = PAREN_NUM_ITEM_RE.match(line)
    if m:
        return ("paren_num", m.group(1), m.group(2).strip())
    return None

# =========================
# Data models
# =========================
@dataclass
class RawLine:
    page: int
    text: str

@dataclass
class Item:
    item_type: str            # letter/num/paren_num/none
    item_key: Optional[str]   # a/1/...
    text: str                 # content
    page_start: int
    page_end: int

@dataclass
class Article:
    article_kind: str         # MADDE/GEÇİCİ MADDE/EK
    article_no: int
    article_title: str        # "MADDE 7" / "GEÇİCİ MADDE 1" / "EK 1"
    bolum: Optional[str]
    bolum_topic: Optional[str]  # varsa bölüm alt başlığını burada tutmak istersen
    page_start: int
    page_end: int
    items: List[Item]

@dataclass
class Chunk:
    chunk_id: str
    doc_id: str
    doc_name: str
    source_file: str
    page_start: int
    page_end: int
    bolum: Optional[str]
    bolum_topic: Optional[str]
    article_kind: str
    article_no: int
    article_title: str
    item_range: Optional[str]        # ör: "(1)-(3)" veya "a)-c)" veya "1)-2)"
    item_keys: List[str]             # ["(1)","(2)"] veya ["a)","b)"]
    text: str
    char_len: int


# =========================
# Extraction
# =========================
def extract_text_lines(pdf_path: str) -> Tuple[List[RawLine], int, int]:
    doc = fitz.open(pdf_path)
    all_lines: List[RawLine] = []
    total_chars = 0
    for i in range(len(doc)):
        page = doc.load_page(i)
        t = page.get_text("text") or ""
        t = t.replace("\x00", "")
        total_chars += len(t)
        # satır bazında tut
        for ln in t.split("\n"):
            ln2 = normalize_ws(ln)
            if ln2:
                all_lines.append(RawLine(page=i+1, text=ln2))
    return all_lines, len(doc), total_chars


# =========================
# Parsing (state machine)
# =========================
def parse_articles(lines: List[RawLine]) -> List[Article]:
    articles: List[Article] = []

    cur_bolum: Optional[str] = None
    cur_bolum_topic: Optional[str] = None  # istersen burada "Amaç, Kapsam..." gibi satırı yakalayabilirsin

    cur_article: Optional[Article] = None
    cur_item: Optional[Item] = None

    def flush_item():
        nonlocal cur_article, cur_item
        if cur_article is None or cur_item is None:
            return
        cur_item.text = normalize_ws(cur_item.text)
        if cur_item.text:
            cur_article.items.append(cur_item)
        cur_item = None

    def flush_article():
        nonlocal cur_article, cur_item
        if cur_article is None:
            return
        flush_item()
        # madde içinde item yoksa, boş kalmasın diye “none” item üret
        if not cur_article.items:
            cur_article.items.append(Item(
                item_type="none",
                item_key=None,
                text="",
                page_start=cur_article.page_start,
                page_end=cur_article.page_end
            ))
        cur_article.page_end = max(cur_article.page_end, cur_article.items[-1].page_end)
        articles.append(cur_article)
        cur_article = None

    for rl in lines:
        txt = rl.text

        # Bölüm yakala (chunk title değil, sadece metadata)
        if BOLUM_RE.match(txt):
            cur_bolum = txt.strip()
            cur_bolum_topic = None
            continue

        # EK yakala
        m_ek = EK_RE.match(txt)
        if m_ek:
            flush_article()
            ek_no = int(m_ek.group(1))
            ek_rest = (m_ek.group(2) or "").strip()
            title = f"EK {ek_no}"
            cur_article = Article(
                article_kind="EK",
                article_no=ek_no,
                article_title=title if not ek_rest else f"{title} – {ek_rest}",
                bolum=cur_bolum,
                bolum_topic=cur_bolum_topic,
                page_start=rl.page,
                page_end=rl.page,
                items=[]
            )
            # EK satırının devamı varsa item gibi yaz
            if ek_rest:
                cur_item = Item(item_type="none", item_key=None, text=ek_rest, page_start=rl.page, page_end=rl.page)
            continue

        # MADDE inline
        m_inline = MADDE_INLINE_RE.match(txt)
        if m_inline:
            flush_article()
            is_gecici = bool(m_inline.group(1))
            no = int(m_inline.group(2))
            rest = (m_inline.group(3) or "").strip()

            kind = "GEÇİCİ MADDE" if is_gecici else "MADDE"
            title = f"{kind} {no}"

            cur_article = Article(
                article_kind=kind,
                article_no=no,
                article_title=title,
                bolum=cur_bolum,
                bolum_topic=cur_bolum_topic,
                page_start=rl.page,
                page_end=rl.page,
                items=[]
            )

            # aynı satırda (1) / a) vs başlıyorsa item olarak başlat
            if rest:
                marker = detect_item_marker(rest)
                if marker:
                    itype, ikey, irest = marker
                    cur_item = Item(
                        item_type=itype,
                        item_key=ikey,
                        text=irest,
                        page_start=rl.page,
                        page_end=rl.page
                    )
                else:
                    cur_item = Item(
                        item_type="none",
                        item_key=None,
                        text=rest,
                        page_start=rl.page,
                        page_end=rl.page
                    )
            continue

        # MADDE tek satır
        m_madde = MADDE_ONLY_RE.match(txt)
        if m_madde:
            flush_article()
            is_gecici = bool(m_madde.group(1))
            no = int(m_madde.group(2))
            kind = "GEÇİCİ MADDE" if is_gecici else "MADDE"
            title = f"{kind} {no}"
            cur_article = Article(
                article_kind=kind,
                article_no=no,
                article_title=title,
                bolum=cur_bolum,
                bolum_topic=cur_bolum_topic,
                page_start=rl.page,
                page_end=rl.page,
                items=[]
            )
            continue

        # Article yoksa metni atla (genelde kapak/başlık sayfaları vs)
        if cur_article is None:
            # İstersen burada “BİRİNCİ BÖLÜM” altındaki konu satırlarını yakalamaya çalışabilirsin
            # ör: "Amaç, Kapsam, Dayanak ve Tanımlar"
            # basit yaklaşım: bolum görüldükten sonra ilk "çok kısa ama cümle gibi" satırı topic say.
            if cur_bolum and cur_bolum_topic is None:
                if 5 <= len(txt) <= 120 and not txt.endswith((".", ":", ";")):
                    # MADDE/EK değilse
                    if not (MADDE_ONLY_RE.match(txt) or MADDE_INLINE_RE.match(txt) or EK_RE.match(txt)):
                        cur_bolum_topic = txt
            continue

        # Article içindeyiz
        cur_article.page_end = max(cur_article.page_end, rl.page)

        # Yeni item marker?
        marker = detect_item_marker(txt)
        if marker:
            flush_item()
            itype, ikey, irest = marker
            cur_item = Item(
                item_type=itype,
                item_key=ikey,
                text=irest,
                page_start=rl.page,
                page_end=rl.page
            )
            continue

        # mevcut item yoksa başlat (none)
        if cur_item is None:
            cur_item = Item(
                item_type="none",
                item_key=None,
                text=txt,
                page_start=rl.page,
                page_end=rl.page
            )
        else:
            # devam satırı: item’a ekle
            cur_item.text += "\n" + txt
            cur_item.page_end = max(cur_item.page_end, rl.page)

    flush_article()
    return articles


# =========================
# Chunking
# =========================
def item_label(item: Item) -> Optional[str]:
    if item.item_type == "letter" and item.item_key:
        return f"{item.item_key})"
    if item.item_type == "num" and item.item_key:
        return f"{item.item_key})"
    if item.item_type == "paren_num" and item.item_key:
        return f"({item.item_key})"
    return None

def compute_item_range(labels: List[str]) -> Optional[str]:
    if not labels:
        return None
    if len(labels) == 1:
        return labels[0]
    return f"{labels[0]}-{labels[-1]}"

def split_long_text_by_paragraphs(text: str, max_chars: int) -> List[str]:
    text = normalize_ws(text)
    if len(text) <= max_chars:
        return [text] if text else []
    paras = [p.strip() for p in re.split(r"\n\s*\n", text) if p.strip()]
    out: List[str] = []
    buf = ""
    for p in paras:
        if not buf:
            buf = p
        elif len(buf) + 2 + len(p) <= max_chars:
            buf += "\n\n" + p
        else:
            out.append(buf)
            buf = p
    if buf:
        out.append(buf)
    # hala çok uzun tek paragraf varsa zorunlu kes
    final: List[str] = []
    for x in out:
        if len(x) <= max_chars:
            final.append(x)
        else:
            start = 0
            while start < len(x):
                final.append(x[start:start+max_chars].strip())
                start += max_chars
    return [z for z in final if z]

def make_chunks_for_article(article: Article, source_file: str, chunk_index_start: int) -> Tuple[List[Chunk], int]:
    chunks: List[Chunk] = []
    idx = chunk_index_start

    # item bazında chunk pack (aynı MADDE içinde)
    cur_parts: List[str] = []
    cur_labels: List[str] = []
    cur_item_keys: List[str] = []
    cur_ps: Optional[int] = None
    cur_pe: Optional[int] = None

    def flush_current():
        nonlocal idx, cur_parts, cur_labels, cur_item_keys, cur_ps, cur_pe
        if not cur_parts:
            return
        text = "\n\n".join([p for p in cur_parts if p]).strip()
        text = normalize_ws(text)
        if not text:
            cur_parts, cur_labels, cur_item_keys, cur_ps, cur_pe = [], [], [], None, None
            return

        labels_for_range = [l for l in cur_labels if l]
        rng = compute_item_range(labels_for_range) if labels_for_range else None

        chunk_id = f"{DOC_ID}::{slugify(article.article_kind)}_{article.article_no}::chunk_{idx:05d}"
        idx += 1

        chunks.append(Chunk(
            chunk_id=chunk_id,
            doc_id=DOC_ID,
            doc_name=DOC_NAME,
            source_file=source_file,
            page_start=cur_ps if cur_ps is not None else article.page_start,
            page_end=cur_pe if cur_pe is not None else article.page_end,
            bolum=article.bolum,
            bolum_topic=article.bolum_topic,
            article_kind=article.article_kind,
            article_no=article.article_no,
            article_title=article.article_title,
            item_range=rng,
            item_keys=cur_item_keys[:],
            text=text,
            char_len=len(text)
        ))
        cur_parts, cur_labels, cur_item_keys, cur_ps, cur_pe = [], [], [], None, None

    for it in article.items:
        it_text = normalize_ws(it.text)
        lbl = item_label(it)
        # none item: label yok, ama metin var
        # item formatı: label + içerik
        if lbl:
            rendered = f"{lbl} {it_text}".strip()
            key_str = lbl
        else:
            rendered = it_text
            key_str = ""

        # item metni çok uzunsa: önce mevcut pack’i flush, sonra item’ı paragraf bazında böl
        if len(rendered) > int(MAX_CHARS * 1.35):
            flush_current()
            parts = split_long_text_by_paragraphs(rendered, MAX_CHARS)
            for part in parts:
                chunk_id = f"{DOC_ID}::{slugify(article.article_kind)}_{article.article_no}::chunk_{idx:05d}"
                idx += 1
                chunks.append(Chunk(
                    chunk_id=chunk_id,
                    doc_id=DOC_ID,
                    doc_name=DOC_NAME,
                    source_file=source_file,
                    page_start=it.page_start,
                    page_end=it.page_end,
                    bolum=article.bolum,
                    bolum_topic=article.bolum_topic,
                    article_kind=article.article_kind,
                    article_no=article.article_no,
                    article_title=article.article_title,
                    item_range=lbl if lbl else None,
                    item_keys=[key_str] if key_str else [],
                    text=part,
                    char_len=len(part)
                ))
            continue

        # normal packing (sadece aynı MADDE içinde)
        proposed_len = (len("\n\n".join(cur_parts)) + (2 if cur_parts else 0) + len(rendered))
        if cur_parts and proposed_len > MAX_CHARS and len("\n\n".join(cur_parts)) >= MIN_CHARS:
            flush_current()

        if cur_ps is None:
            cur_ps = it.page_start
        cur_pe = it.page_end

        if rendered:
            cur_parts.append(rendered)
        if lbl:
            cur_labels.append(lbl)
            cur_item_keys.append(lbl)

    flush_current()

    # Overlap: sadece aynı article içindeki ardışık chunk’lar arasında
    if OVERLAP_CHARS > 0 and len(chunks) >= 2:
        fixed = [chunks[0]]
        for prev, nxt in zip(chunks[:-1], chunks[1:]):
            tail = safe_tail_overlap(prev.text, OVERLAP_CHARS)
            merged = normalize_ws((tail + "\n\n" + nxt.text).strip())
            nxt2 = Chunk(**{**asdict(nxt), "text": merged, "char_len": len(merged)})
            fixed.append(nxt2)
        chunks = fixed

    return chunks, idx

def chunk_articles(articles: List[Article], pdf_path: str) -> List[Chunk]:
    source_file = os.path.basename(pdf_path)
    out: List[Chunk] = []
    idx = 0
    for a in articles:
        chs, idx = make_chunks_for_article(a, source_file, idx)
        out.extend(chs)
    return out


# =========================
# End-to-end
# =========================
def chunk_pdf_legal(pdf_path: str, out_jsonl: str) -> List[Chunk]:
    assert os.path.exists(pdf_path), f"PDF yok: {pdf_path}"

    lines, n_pages, total_chars = extract_text_lines(pdf_path)
    if total_chars == 0:
        raise RuntimeError("PDF text-based değil (scanned). OCR gerekir.")

    articles = parse_articles(lines)
    chunks = chunk_articles(articles, pdf_path)

    with open(out_jsonl, "w", encoding="utf-8") as f:
        for c in chunks:
            f.write(json.dumps(asdict(c), ensure_ascii=False) + "\n")

    print(f"pages: {n_pages}")
    print(f"total extracted chars: {total_chars}")
    print(f"articles: {len(articles)}")
    print(f"chunks: {len(chunks)}")
    print(f"wrote: {out_jsonl}")

    return chunks


# =========================
# Run + quick preview
# =========================
chunks = chunk_pdf_legal(PDF_PATH, OUT_JSONL)

for c in chunks[:8]:
    print("=" * 100)
    print(c.chunk_id, f"pages {c.page_start}-{c.page_end}", "len", c.char_len)
    print("bolum:", c.bolum)
    print("bolum_topic:", c.bolum_topic)
    print("article:", c.article_kind, c.article_no, "|", c.article_title)
    print("item_range:", c.item_range, "| item_keys:", c.item_keys[:6], ("..." if len(c.item_keys) > 6 else ""))
    print(c.text[:900], "..." if len(c.text) > 900 else "")