# -*- coding: utf-8 -*-
"""
Hierarchical Legal PDF Chunker (TR mevzuat/tebliğ)
- Extract line-level text with font/layout signals (PyMuPDF rawdict)
- Detect headings: BÖLÜM / Alt Başlık / MADDE / Fıkra / Bent
- Build a hierarchy tree and generate RAG-friendly chunks
- Never split tables (detected by layout heuristics)
- Chunk by legal boundaries first (madde->fıkra->bent), then by sentences if needed
"""

from __future__ import annotations
import re
import json
from dataclasses import dataclass, asdict
from typing import List, Optional, Dict, Any, Tuple

import fitz  # PyMuPDF


# =========================
# Config
# =========================
PDF_PATH = "/mnt/data/tcmb_teblig.pdf"
DOC_ID = "tcmb_teblig"
DOC_NAME = "TCMB TEBLİĞ"
SOURCE_FILE = "tcmb_teblig.pdf"

MAX_CHUNK_TOKENS = 450
OVERLAP_TOKENS = 60

# If you have an HF tokenizer, use it. Otherwise fallback to a word-based approx.
HF_TOKENIZER_NAME = None  # e.g. "BAAI/bge-m3" or "BAAI/bge-base-en-v1.5"


# =========================
# Token counter
# =========================
class TokenCounter:
    def count(self, text: str) -> int:
        raise NotImplementedError

class WordApproxCounter(TokenCounter):
    def count(self, text: str) -> int:
        # Turkish: ~1 token ~= 0.75-1.0 word (rough). Keep conservative.
        return max(1, int(len(text.split()) * 1.1))

class HFTokenCounter(TokenCounter):
    def __init__(self, model_name: str):
        from transformers import AutoTokenizer
        self.tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    def count(self, text: str) -> int:
        return len(self.tok.encode(text, add_special_tokens=False))


# =========================
# Data structures
# =========================
@dataclass
class Line:
    page: int
    text: str
    font_size: float
    is_bold: bool
    x0: float
    x1: float
    y0: float
    y1: float
    center_x: float

@dataclass
class Node:
    level: str                    # "doc"|"bolum"|"subhead"|"madde"|"fikra"|"bent"|"ek"|"table"
    title: str
    page_start: int
    page_end: int
    children: List["Node"]
    content: List[str]            # plain text lines accumulated
    tables: List[str]             # atomic table text blocks
    meta: Dict[str, Any]

@dataclass
class Chunk:
    chunk_id: str
    doc_id: str
    doc_name: str
    source_file: str
    page_start: int
    page_end: int
    path: List[str]               # hierarchical headers
    article_no: Optional[int]
    article_title: Optional[str]
    paragraph_no: Optional[int]
    item_key: Optional[str]
    chunk_type: str               # "madde"|"fikra"|"bent"|"ek"|"table"|"mixed"
    text: str
    token_len: int


# =========================
# Helpers
# =========================
def clean_text(t: str) -> str:
    t = t.replace("\u00ad", "")
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def is_centered(line: Line, page_width: float, tol: float = 0.12) -> bool:
    # centered if its center is near page center
    return abs(line.center_x - (page_width / 2)) <= page_width * tol

def looks_like_heading(text: str) -> bool:
    # short, title-like
    if len(text) > 80:
        return False
    # avoid pure punctuation
    if re.fullmatch(r"[\W_]+", text):
        return False
    return True

# patterns (supportive, not sole authority)
RE_BOLUM = re.compile(r"^(BİRİNCİ|İKİNCİ|ÜÇÜNCÜ|DÖRDÜNCÜ|BEŞİNCİ|ALTINCI|YEDİNCİ|SEKİZİNCİ|DOKUZUNCU|ONUNCU)\s+BÖLÜM\b")
RE_MADDE = re.compile(r"^MADDE\s+(\d+)\b")
RE_FIKRA = re.compile(r"^\((\d+)\)\s+")
RE_BENT  = re.compile(r"^([a-zçğıöşü])\)\s+", re.IGNORECASE)
RE_EK    = re.compile(r"^EK[-\s]?\d+\b|^Ek[-\s]?\d+\b")


# =========================
# Table detection (page block-level)
# =========================
def is_table_like_block(block: Dict[str, Any]) -> bool:
    # text blocks only
    if block.get("type", 0) != 0:
        return False
    lines = block.get("lines", [])
    if len(lines) < 3:
        return False

    x_positions = []
    raw = []
    for ln in lines:
        for sp in ln.get("spans", []):
            bbox = sp.get("bbox", [0,0,0,0])
            x_positions.append(round(bbox[0], 1))
            raw.append(sp.get("text", ""))

    txt = " ".join(raw).strip()
    if not txt:
        return False

    distinct_x = len(set(x_positions))
    digits = sum(ch.isdigit() for ch in txt)
    density = digits / max(1, len(txt))
    multi_space = bool(re.search(r"\s{3,}", txt))

    # good enough heuristic
    return (distinct_x >= 5 and len(lines) >= 3) or (multi_space and density >= 0.20)


def extract_tables_by_page(doc: fitz.Document) -> Dict[int, List[str]]:
    """
    Return per-page list of atomic table texts.
    We'll later attach them to nearest node by order of appearance.
    """
    out: Dict[int, List[str]] = {}
    for pi in range(len(doc)):
        page = doc.load_page(pi)
        d = page.get_text("dict")
        tables = []
        for b in d.get("blocks", []):
            if is_table_like_block(b):
                parts = []
                for ln in b.get("lines", []):
                    line_txt = "".join(sp.get("text", "") for sp in ln.get("spans", []))
                    parts.append(line_txt)
                t = clean_text("\n".join(parts))
                if t:
                    tables.append(t)
        if tables:
            out[pi+1] = tables
    return out


# =========================
# Line extraction (rawdict -> lines with font/layout)
# =========================
def extract_lines(doc: fitz.Document) -> Tuple[List[Line], Dict[int, float]]:
    lines_out: List[Line] = []
    page_widths: Dict[int, float] = {}

    for pi in range(len(doc)):
        page = doc.load_page(pi)
        page_no = pi + 1
        page_widths[page_no] = page.rect.width

        rd = page.get_text("rawdict")
        for b in rd.get("blocks", []):
            if b.get("type", 0) != 0:
                continue
            for ln in b.get("lines", []):
                spans = ln.get("spans", [])
                if not spans:
                    continue
                text = "".join(sp.get("text", "") for sp in spans).strip()
                text = clean_text(text)
                if not text:
                    continue

                # aggregate layout
                sizes = [sp.get("size", 0.0) for sp in spans]
                font_flags = [sp.get("flags", 0) for sp in spans]
                # "bold" heuristic: flag bit OR font name includes Bold
                font_names = [sp.get("font", "") for sp in spans]
                is_bold = any("Bold" in fn for fn in font_names) or any((fl & 16) != 0 for fl in font_flags)

                bbox = ln.get("bbox", [0,0,0,0])
                x0, y0, x1, y1 = bbox
                center_x = (x0 + x1) / 2

                lines_out.append(Line(
                    page=page_no,
                    text=text,
                    font_size=float(sum(sizes) / max(1, len(sizes))),
                    is_bold=bool(is_bold),
                    x0=float(x0), x1=float(x1), y0=float(y0), y1=float(y1),
                    center_x=float(center_x)
                ))

    # keep reading order: page, y0, x0
    lines_out.sort(key=lambda l: (l.page, l.y0, l.x0))
    return lines_out, page_widths


# =========================
# Heading classifier (uses layout + patterns)
# =========================
def classify_line(line: Line, page_width: float, base_font: float) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:
    t = line.text.strip()

    # BÖLÜM (centered, bold-ish, caps-ish)
    if RE_BOLUM.match(t) and looks_like_heading(t):
        return "bolum", {}

    # EK (appendix)
    if RE_EK.match(t) and looks_like_heading(t):
        return "ek", {}

    # MADDE
    m = RE_MADDE.match(t)
    if m:
        return "madde", {"article_no": int(m.group(1)), "article_title": f"MADDE {m.group(1)}"}

    # Fıkra
    m = RE_FIKRA.match(t)
    if m:
        return "fikra", {"paragraph_no": int(m.group(1))}

    # Bent (a), b) ...
    m = RE_BENT.match(t)
    if m:
        return "bent", {"item_key": m.group(1).lower()}

    # Subhead (Amaç/Kapsam/Dayanak/Tanımlar, Ücretlerin iadesi, Bilgilendirme, vb.)
    # Use layout signals: short + larger than base font OR bold OR centered.
    if looks_like_heading(t):
        size_signal = line.font_size >= base_font * 1.08
        center_signal = is_centered(line, page_width, tol=0.18)
        bold_signal = line.is_bold

        # avoid misclassifying normal sentences
        if (bold_signal and size_signal) or (center_signal and size_signal) or (bold_signal and center_signal):
            return "subhead", {}
        # also allow very short bold titles
        if bold_signal and len(t) <= 35:
            return "subhead", {}

    return None, None


def estimate_base_font(lines: List[Line]) -> float:
    # robust base font: median of non-heading-ish lines
    sizes = [l.font_size for l in lines if 7.0 <= l.font_size <= 14.0]
    sizes.sort()
    if not sizes:
        return 10.0
    mid = len(sizes) // 2
    return float(sizes[mid])


# =========================
# Build hierarchy tree
# =========================
def build_tree(lines: List[Line], page_widths: Dict[int, float], tables_by_page: Dict[int, List[str]]) -> Node:
    base_font = estimate_base_font(lines)

    root = Node(
        level="doc",
        title=DOC_NAME,
        page_start=1,
        page_end=max(l.page for l in lines) if lines else 1,
        children=[],
        content=[],
        tables=[],
        meta={"doc_id": DOC_ID, "doc_name": DOC_NAME, "source_file": SOURCE_FILE}
    )

    cur_bolum: Optional[Node] = None
    cur_sub: Optional[Node] = None
    cur_madde: Optional[Node] = None
    cur_fikra: Optional[Node] = None

    # helper to open new node
    def add_child(parent: Node, node: Node) -> Node:
        parent.children.append(node)
        return node

    # attach tables in reading order: simplest approach → when page changes, push page tables into current scope
    def flush_page_tables(page_no: int):
        nonlocal cur_bolum, cur_sub, cur_madde, cur_fikra
        tbs = tables_by_page.get(page_no, [])
        if not tbs:
            return
        # attach to deepest open node
        target = cur_fikra or cur_madde or cur_sub or cur_bolum or root
        target.tables.extend(tbs)
        target.page_end = max(target.page_end, page_no)

    last_page = 1
    for ln in lines:
        if ln.page != last_page:
            flush_page_tables(last_page)
            last_page = ln.page

        page_w = page_widths.get(ln.page, 600.0)
        kind, meta = classify_line(ln, page_w, base_font)

        if kind == "bolum":
            cur_bolum = add_child(root, Node(
                level="bolum",
                title=ln.text,
                page_start=ln.page,
                page_end=ln.page,
                children=[],
                content=[],
                tables=[],
                meta={}
            ))
            cur_sub = None
            cur_madde = None
            cur_fikra = None
            continue

        if kind == "ek":
            # treat as a new top-level under current bolum if any, else root
            parent = cur_bolum or root
            cur_sub = add_child(parent, Node(
                level="ek",
                title=ln.text,
                page_start=ln.page,
                page_end=ln.page,
                children=[],
                content=[],
                tables=[],
                meta={}
            ))
            cur_madde = None
            cur_fikra = None
            continue

        if kind == "subhead":
            parent = cur_bolum or root
            cur_sub = add_child(parent, Node(
                level="subhead",
                title=ln.text,
                page_start=ln.page,
                page_end=ln.page,
                children=[],
                content=[],
                tables=[],
                meta={}
            ))
            cur_madde = None
            cur_fikra = None
            continue

        if kind == "madde":
            parent = cur_sub or cur_bolum or root
            cur_madde = add_child(parent, Node(
                level="madde",
                title=ln.text,  # keep original line (contains MADDE N - ...)
                page_start=ln.page,
                page_end=ln.page,
                children=[],
                content=[],
                tables=[],
                meta=meta or {}
            ))
            cur_fikra = None
            continue

        if kind == "fikra":
            parent = cur_madde or cur_sub or cur_bolum or root
            cur_fikra = add_child(parent, Node(
                level="fikra",
                title=ln.text[:30],  # short marker, content stored below
                page_start=ln.page,
                page_end=ln.page,
                children=[],
                content=[],
                tables=[],
                meta=meta or {}
            ))
            # store full line as content (fıkra text)
            cur_fikra.content.append(ln.text)
            continue

        if kind == "bent":
            parent = cur_fikra or cur_madde or cur_sub or cur_bolum or root
            bent = add_child(parent, Node(
                level="bent",
                title=ln.text[:20],
                page_start=ln.page,
                page_end=ln.page,
                children=[],
                content=[],
                tables=[],
                meta=meta or {}
            ))
            bent.content.append(ln.text)
            continue

        # normal content line → attach to deepest open node
        target = cur_fikra or cur_madde or cur_sub or cur_bolum or root
        target.content.append(ln.text)
        target.page_end = max(target.page_end, ln.page)

    flush_page_tables(last_page)
    return root


# =========================
# Chunk generation
# =========================
def node_path(node: Node, ancestors: List[str]) -> List[str]:
    return ancestors + ([node.title] if node.level in ("bolum", "subhead", "madde", "ek") else [])

def format_with_path(path: List[str], body: str) -> str:
    # Markdown-ish header stack (stable for RAG)
    hdr = []
    for i, p in enumerate(path):
        level = min(4, i + 1)
        hdr.append(f"{'#' * level} {p}")
    return clean_text("\n".join(hdr) + "\n\n" + body)

def split_by_sentences(text: str) -> List[str]:
    # conservative TR sentence split
    parts = re.split(r"(?<=[\.\!\?])\s+", text.strip())
    return [p.strip() for p in parts if p.strip()]

def pack_units(units: List[Tuple[str, Dict[str, Any]]], path: List[str], base_meta: Dict[str, Any], counter: TokenCounter) -> List[Chunk]:
    """
    units: list of (unit_text, unit_meta)
    packs them into chunks with overlap by token approx (tail text overlap)
    """
    chunks: List[Chunk] = []
    cur = []
    cur_tokens = 0

    def finalize():
        nonlocal cur, cur_tokens
        if not cur:
            return
        texts = [u[0] for u in cur]
        meta_merge = {}
        # keep last non-null for paragraph/item where relevant
        for _, m in cur:
            meta_merge.update({k: v for k, v in m.items() if v is not None})
        body = "\n".join(texts).strip()
        full = format_with_path(path, body)
        tok = counter.count(full)

        ps = meta_merge.get("page_start", base_meta.get("page_start"))
        pe = meta_merge.get("page_end", base_meta.get("page_end"))
        chunk = Chunk(
            chunk_id=f"{DOC_ID}::{base_meta.get('anchor','node')}::{len(chunks)}",
            doc_id=DOC_ID,
            doc_name=DOC_NAME,
            source_file=SOURCE_FILE,
            page_start=int(ps),
            page_end=int(pe),
            path=path,
            article_no=meta_merge.get("article_no"),
            article_title=meta_merge.get("article_title"),
            paragraph_no=meta_merge.get("paragraph_no"),
            item_key=meta_merge.get("item_key"),
            chunk_type=base_meta.get("chunk_type", "mixed"),
            text=full,
            token_len=tok
        )
        chunks.append(chunk)
        cur = []
        cur_tokens = 0

    for txt, meta in units:
        t = counter.count(txt)
        if cur and cur_tokens + t > MAX_CHUNK_TOKENS:
            finalize()

        if not cur and t > MAX_CHUNK_TOKENS:
            # too large single unit -> sentence split fallback
            sents = split_by_sentences(txt)
            buf = ""
            buf_meta = meta.copy()
            for s in sents:
                st = counter.count(s)
                if buf and counter.count(buf) + st > MAX_CHUNK_TOKENS:
                    units2 = [(buf, buf_meta)]
                    # recurse one-level without infinite loop
                    cur.append(units2[0])
                    finalize()
                    buf = ""
                buf = (buf + " " + s).strip()
            if buf:
                cur.append((buf, buf_meta))
                finalize()
            continue

        cur.append((txt, meta))
        cur_tokens += t

    finalize()

    # overlap (text tail overlap)
    if OVERLAP_TOKENS > 0 and len(chunks) >= 2:
        out = [chunks[0]]
        for prev, nxt in zip(chunks[:-1], chunks[1:]):
            words = prev.text.split()
            tail_words = min(len(words), max(20, int(OVERLAP_TOKENS * 0.9)))
            tail = " ".join(words[-tail_words:])
            nxt2 = Chunk(**{**asdict(nxt), "text": clean_text(tail + "\n\n" + nxt.text), "token_len": counter.count(clean_text(tail + "\n\n" + nxt.text))})
            out.append(nxt2)
        chunks = out

    return chunks


def gather_units(node: Node, ancestors: List[str], counter: TokenCounter) -> List[Chunk]:
    """
    Convert hierarchy to chunks.
    Strategy:
      - For each MADDE node: chunk at fıkra boundary (preferred)
      - For EK and subhead without madde: chunk by paragraphs
      - Always include node tables in the same scope but never split them
    """
    chunks: List[Chunk] = []
    path = node_path(node, ancestors)

    # Build units from this node depending on level
    if node.level == "madde":
        article_no = node.meta.get("article_no")
        article_title = node.meta.get("article_title") or node.title

        units: List[Tuple[str, Dict[str, Any]]] = []

        # If node has direct content (rare), include it first
        if node.content:
            units.append(("\n".join(node.content), {
                "article_no": article_no,
                "article_title": article_title,
                "page_start": node.page_start,
                "page_end": node.page_end,
                "paragraph_no": None,
                "item_key": None
            }))

        # Prefer children fıkra/bent as units
        for ch in node.children:
            if ch.level == "fikra":
                units.append(("\n".join(ch.content), {
                    "article_no": article_no,
                    "article_title": article_title,
                    "page_start": ch.page_start,
                    "page_end": ch.page_end,
                    "paragraph_no": ch.meta.get("paragraph_no"),
                    "item_key": None
                }))
                # bents inside fikra
                for b in ch.children:
                    if b.level == "bent":
                        units.append(("\n".join(b.content), {
                            "article_no": article_no,
                            "article_title": article_title,
                            "page_start": b.page_start,
                            "page_end": b.page_end,
                            "paragraph_no": ch.meta.get("paragraph_no"),
                            "item_key": b.meta.get("item_key")
                        }))
            elif ch.level == "bent":
                units.append(("\n".join(ch.content), {
                    "article_no": article_no,
                    "article_title": article_title,
                    "page_start": ch.page_start,
                    "page_end": ch.page_end,
                    "paragraph_no": None,
                    "item_key": ch.meta.get("item_key")
                }))

        # Tables attached to this madde scope
        for tb in node.tables:
            units.append((tb, {
                "article_no": article_no,
                "article_title": article_title,
                "page_start": node.page_start,
                "page_end": node.page_end,
                "paragraph_no": None,
                "item_key": None
            }))

        base_meta = {"page_start": node.page_start, "page_end": node.page_end, "anchor": f"madde_{article_no}", "chunk_type": "madde"}
        chunks.extend(pack_units(units, path, base_meta, counter))
        return chunks

    # For subhead/ek/bolum/doc: make paragraph-ish units from content, then recurse children
    if node.content or node.tables:
        units = []
        if node.content:
            # split by blank-line intent (we only have lines; simulate paragraphs by join then split)
            body = clean_text("\n".join(node.content))
            paras = [p.strip() for p in re.split(r"\n{2,}", body) if p.strip()]
            for p in paras:
                units.append((p, {"page_start": node.page_start, "page_end": node.page_end}))
        for tb in node.tables:
            units.append((tb, {"page_start": node.page_start, "page_end": node.page_end}))

        base_meta = {"page_start": node.page_start, "page_end": node.page_end, "anchor": node.level, "chunk_type": node.level}
        chunks.extend(pack_units(units, path, base_meta, counter))

    for ch in node.children:
        chunks.extend(gather_units(ch, path, counter))

    return chunks


def chunk_pdf(pdf_path: str) -> List[Chunk]:
    doc = fitz.open(pdf_path)

    tables_by_page = extract_tables_by_page(doc)
    lines, page_widths = extract_lines(doc)

    tree = build_tree(lines, page_widths, tables_by_page)

    counter: TokenCounter
    if HF_TOKENIZER_NAME:
        counter = HFTokenCounter(HF_TOKENIZER_NAME)
    else:
        counter = WordApproxCounter()

    chunks = gather_units(tree, [], counter)
    return chunks


if __name__ == "__main__":
    chunks = chunk_pdf(PDF_PATH)

    out_path = "/mnt/data/tcmb_teblig_chunks.jsonl"
    with open(out_path, "w", encoding="utf-8") as f:
        for c in chunks:
            f.write(json.dumps(asdict(c), ensure_ascii=False) + "\n")

    # quick sanity print
    print("chunks:", len(chunks))
    for c in chunks[:5]:
        print("=" * 80)
        print(c.chunk_id, c.page_start, c.page_end, c.article_no, c.paragraph_no, c.item_key, c.token_len)
        print(c.text[:600])