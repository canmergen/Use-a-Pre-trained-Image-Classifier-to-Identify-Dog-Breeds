# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import re
import json
from dataclasses import dataclass, asdict
from typing import List, Optional, Dict, Any, Tuple

import fitz  # pymupdf


# =========================
# Config
# =========================
PDF_PATH = "/mnt/data/tcmb_teblig.pdf"
OUT_JSONL = "/mnt/data/tcmb_teblig_chunks.jsonl"

DOC_ID = "tcmb_teblig"
DOC_NAME = "TCMB TEBLİĞ"

# RAG chunk controls (char-based; token bazlı istersen sonra tokenizer bağlanır)
MAX_CHARS = 1800
MIN_CHARS = 450
OVERLAP_CHARS = 200  # sadece aynı MADDE içinde uygulanır

# =========================
# Regex patterns
# =========================
# Madde satırı bazen içerikle aynı satırda geliyor: "MADDE 1 – (1) ...."
MADDE_INLINE_RE = re.compile(
    r"^\s*(GEÇİCİ\s+)?MADDE\s+(\d+)\s*[-–]\s*(.*)\s*$",
    re.IGNORECASE
)
MADDE_ONLY_RE = re.compile(
    r"^\s*(GEÇİCİ\s+)?MADDE\s+(\d+)\s*$",
    re.IGNORECASE
)

# EK (Ek-1, EK 1 vb)
EK_RE = re.compile(r"^\s*EK\s*[-–]?\s*(\d+)\s*[:\-–]?\s*(.*)\s*$", re.IGNORECASE)

# Bölüm başlığı (metadata/context için, chunk title yapmıyoruz)
BOLUM_RE = re.compile(
    r"^\s*([IVXLC]+|BİRİNCİ|İKİNCİ|ÜÇÜNCÜ|DÖRDÜNCÜ|BEŞİNCİ|ALTINCI|YEDİNCİ|SEKİZİNCİ|DOKUZUNCU|ONUNCU)\s+BÖLÜM\s*$",
    re.IGNORECASE
)

# Subtitle / item marker'ları
# a) b) c) (Türkçe harfler dahil)
LETTER_ITEM_RE = re.compile(r"^\s*([a-zçğıöşü])\)\s+(.*)\s*$", re.IGNORECASE)
NUM_ITEM_RE = re.compile(r"^\s*(\d+)\)\s+(.*)\s*$")
PAREN_NUM_ITEM_RE = re.compile(r"^\s*\((\d+)\)\s+(.*)\s*$")

# “MADDE 4 – (1) ...” gibi inline satırda item marker yakalamak için
ITEM_PREFIX_INLINE_RES = [
    ("letter", LETTER_ITEM_RE),
    ("num", NUM_ITEM_RE),
    ("paren_num", PAREN_NUM_ITEM_RE),
]

# =========================
# Utils
# =========================
def normalize_ws(s: str) -> str:
    s = s.replace("\u00ad", "")  # soft hyphen
    s = s.replace("\r", "\n")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def slugify(s: str) -> str:
    s = s.strip().lower()
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"[^a-z0-9_çğıöşü]+", "", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s

def safe_tail_overlap(text: str, n: int) -> str:
    if n <= 0 or len(text) <= n:
        return text
    tail = text[-n:]
    # kelime ortasında kesmeyi azalt
    if " " in tail:
        tail = tail.split(" ", 1)[-1]
    return tail.strip()

def detect_item_marker(line: str) -> Optional[Tuple[str, str, str]]:
    """
    Returns (item_type, item_key, item_rest_text)
    item_type: 'letter'|'num'|'paren_num'
    item_key: 'a' or '1'
    """
    m = LETTER_ITEM_RE.match(line)
    if m:
        return ("letter", m.group(1).lower(), m.group(2).strip())
    m = NUM_ITEM_RE.match(line)
    if m:
        return ("num", m.group(1), m.group(2).strip())
    m = PAREN_NUM_ITEM_RE.match(line)
    if m:
        return ("paren_num", m.group(1), m.group(2).strip())
    return None

# =========================
# Data models
# =========================
@dataclass
class RawLine:
    page: int
    text: str

@dataclass
class Item:
    item_type: str            # letter/num/paren_num/none
    item_key: Optional[str]   # a/1/...
    text: str                 # content
    page_start: int
    page_end: int

@dataclass
class Article:
    article_kind: str         # MADDE/GEÇİCİ MADDE/EK
    article_no: int
    article_title: str        # "MADDE 7" / "GEÇİCİ MADDE 1" / "EK 1"
    bolum: Optional[str]
    bolum_topic: Optional[str]  # varsa bölüm alt başlığını burada tutmak istersen
    page_start: int
    page_end: int
    items: List[Item]

@dataclass
class Chunk:
    chunk_id: str
    doc_id: str
    doc_name: str
    source_file: str
    page_start: int
    page_end: int
    bolum: Optional[str]
    bolum_topic: Optional[str]
    article_kind: str
    article_no: int
    article_title: str
    item_range: Optional[str]        # ör: "(1)-(3)" veya "a)-c)" veya "1)-2)"
    item_keys: List[str]             # ["(1)","(2)"] veya ["a)","b)"]
    text: str
    char_len: int


# =========================
# Extraction
# =========================
def extract_text_lines(pdf_path: str) -> Tuple[List[RawLine], int, int]:
    doc = fitz.open(pdf_path)
    all_lines: List[RawLine] = []
    total_chars = 0
    for i in range(len(doc)):
        page = doc.load_page(i)
        t = page.get_text("text") or ""
        t = t.replace("\x00", "")
        total_chars += len(t)
        # satır bazında tut
        for ln in t.split("\n"):
            ln2 = normalize_ws(ln)
            if ln2:
                all_lines.append(RawLine(page=i+1, text=ln2))
    return all_lines, len(doc), total_chars


# =========================
# Parsing (state machine)
# =========================
def parse_articles(lines: List[RawLine]) -> List[Article]:
    articles: List[Article] = []

    cur_bolum: Optional[str] = None
    cur_bolum_topic: Optional[str] = None  # istersen burada "Amaç, Kapsam..." gibi satırı yakalayabilirsin

    cur_article: Optional[Article] = None
    cur_item: Optional[Item] = None

    def flush_item():
        nonlocal cur_article, cur_item
        if cur_article is None or cur_item is None:
            return
        cur_item.text = normalize_ws(cur_item.text)
        if cur_item.text:
            cur_article.items.append(cur_item)
        cur_item = None

    def flush_article():
        nonlocal cur_article, cur_item
        if cur_article is None:
            return
        flush_item()
        # madde içinde item yoksa, boş kalmasın diye “none” item üret
        if not cur_article.items:
            cur_article.items.append(Item(
                item_type="none",
                item_key=None,
                text="",
                page_start=cur_article.page_start,
                page_end=cur_article.page_end
            ))
        cur_article.page_end = max(cur_article.page_end, cur_article.items[-1].page_end)
        articles.append(cur_article)
        cur_article = None

    for rl in lines:
        txt = rl.text

        # Bölüm yakala (chunk title değil, sadece metadata)
        if BOLUM_RE.match(txt):
            cur_bolum = txt.strip()
            cur_bolum_topic = None
            continue

        # EK yakala
        m_ek = EK_RE.match(txt)
        if m_ek:
            flush_article()
            ek_no = int(m_ek.group(1))
            ek_rest = (m_ek.group(2) or "").strip()
            title = f"EK {ek_no}"
            cur_article = Article(
                article_kind="EK",
                article_no=ek_no,
                article_title=title if not ek_rest else f"{title} – {ek_rest}",
                bolum=cur_bolum,
                bolum_topic=cur_bolum_topic,
                page_start=rl.page,
                page_end=rl.page,
                items=[]
            )
            # EK satırının devamı varsa item gibi yaz
            if ek_rest:
                cur_item = Item(item_type="none", item_key=None, text=ek_rest, page_start=rl.page, page_end=rl.page)
            continue

        # MADDE inline
        m_inline = MADDE_INLINE_RE.match(txt)
        if m_inline:
            flush_article()
            is_gecici = bool(m_inline.group(1))
            no = int(m_inline.group(2))
            rest = (m_inline.group(3) or "").strip()

            kind = "GEÇİCİ MADDE" if is_gecici else "MADDE"
            title = f"{kind} {no}"

            cur_article = Article(
                article_kind=kind,
                article_no=no,
                article_title=title,
                bolum=cur_bolum,
                bolum_topic=cur_bolum_topic,
                page_start=rl.page,
                page_end=rl.page,
                items=[]
            )

            # aynı satırda (1) / a) vs başlıyorsa item olarak başlat
            if rest:
                marker = detect_item_marker(rest)
                if marker:
                    itype, ikey, irest = marker
                    cur_item = Item(
                        item_type=itype,
                        item_key=ikey,
                        text=irest,
                        page_start=rl.page,
                        page_end=rl.page
                    )
                else:
                    cur_item = Item(
                        item_type="none",
                        item_key=None,
                        text=rest,
                        page_start=rl.page,
                        page_end=rl.page
                    )
            continue

        # MADDE tek satır
        m_madde = MADDE_ONLY_RE.match(txt)
        if m_madde:
            flush_article()
            is_gecici = bool(m_madde.group(1))
            no = int(m_madde.group(2))
            kind = "GEÇİCİ MADDE" if is_gecici else "MADDE"
            title = f"{kind} {no}"
            cur_article = Article(
                article_kind=kind,
                article_no=no,
                article_title=title,
                bolum=cur_bolum,
                bolum_topic=cur_bolum_topic,
                page_start=rl.page,
                page_end=rl.page,
                items=[]
            )
            continue

        # Article yoksa metni atla (genelde kapak/başlık sayfaları vs)
        if cur_article is None:
            # İstersen burada “BİRİNCİ BÖLÜM” altındaki konu satırlarını yakalamaya çalışabilirsin
            # ör: "Amaç, Kapsam, Dayanak ve Tanımlar"
            # basit yaklaşım: bolum görüldükten sonra ilk "çok kısa ama cümle gibi" satırı topic say.
            if cur_bolum and cur_bolum_topic is None:
                if 5 <= len(txt) <= 120 and not txt.endswith((".", ":", ";")):
                    # MADDE/EK değilse
                    if not (MADDE_ONLY_RE.match(txt) or MADDE_INLINE_RE.match(txt) or EK_RE.match(txt)):
                        cur_bolum_topic = txt
            continue

        # Article içindeyiz
        cur_article.page_end = max(cur_article.page_end, rl.page)

        # Yeni item marker?
        marker = detect_item_marker(txt)
        if marker:
            flush_item()
            itype, ikey, irest = marker
            cur_item = Item(
                item_type=itype,
                item_key=ikey,
                text=irest,
                page_start=rl.page,
                page_end=rl.page
            )
            continue

        # mevcut item yoksa başlat (none)
        if cur_item is None:
            cur_item = Item(
                item_type="none",
                item_key=None,
                text=txt,
                page_start=rl.page,
                page_end=rl.page
            )
        else:
            # devam satırı: item’a ekle
            cur_item.text += "\n" + txt
            cur_item.page_end = max(cur_item.page_end, rl.page)

    flush_article()
    return articles


# =========================
# Chunking
# =========================
def item_label(item: Item) -> Optional[str]:
    if item.item_type == "letter" and item.item_key:
        return f"{item.item_key})"
    if item.item_type == "num" and item.item_key:
        return f"{item.item_key})"
    if item.item_type == "paren_num" and item.item_key:
        return f"({item.item_key})"
    return None

def compute_item_range(labels: List[str]) -> Optional[str]:
    if not labels:
        return None
    if len(labels) == 1:
        return labels[0]
    return f"{labels[0]}-{labels[-1]}"

def split_long_text_by_paragraphs(text: str, max_chars: int) -> List[str]:
    text = normalize_ws(text)
    if len(text) <= max_chars:
        return [text] if text else []
    paras = [p.strip() for p in re.split(r"\n\s*\n", text) if p.strip()]
    out: List[str] = []
    buf = ""
    for p in paras:
        if not buf:
            buf = p
        elif len(buf) + 2 + len(p) <= max_chars:
            buf += "\n\n" + p
        else:
            out.append(buf)
            buf = p
    if buf:
        out.append(buf)
    # hala çok uzun tek paragraf varsa zorunlu kes
    final: List[str] = []
    for x in out:
        if len(x) <= max_chars:
            final.append(x)
        else:
            start = 0
            while start < len(x):
                final.append(x[start:start+max_chars].strip())
                start += max_chars
    return [z for z in final if z]

def make_chunks_for_article(article: Article, source_file: str, chunk_index_start: int) -> Tuple[List[Chunk], int]:
    chunks: List[Chunk] = []
    idx = chunk_index_start

    # item bazında chunk pack (aynı MADDE içinde)
    cur_parts: List[str] = []
    cur_labels: List[str] = []
    cur_item_keys: List[str] = []
    cur_ps: Optional[int] = None
    cur_pe: Optional[int] = None

    def flush_current():
        nonlocal idx, cur_parts, cur_labels, cur_item_keys, cur_ps, cur_pe
        if not cur_parts:
            return
        text = "\n\n".join([p for p in cur_parts if p]).strip()
        text = normalize_ws(text)
        if not text:
            cur_parts, cur_labels, cur_item_keys, cur_ps, cur_pe = [], [], [], None, None
            return

        labels_for_range = [l for l in cur_labels if l]
        rng = compute_item_range(labels_for_range) if labels_for_range else None

        chunk_id = f"{DOC_ID}::{slugify(article.article_kind)}_{article.article_no}::chunk_{idx:05d}"
        idx += 1

        chunks.append(Chunk(
            chunk_id=chunk_id,
            doc_id=DOC_ID,
            doc_name=DOC_NAME,
            source_file=source_file,
            page_start=cur_ps if cur_ps is not None else article.page_start,
            page_end=cur_pe if cur_pe is not None else article.page_end,
            bolum=article.bolum,
            bolum_topic=article.bolum_topic,
            article_kind=article.article_kind,
            article_no=article.article_no,
            article_title=article.article_title,
            item_range=rng,
            item_keys=cur_item_keys[:],
            text=text,
            char_len=len(text)
        ))
        cur_parts, cur_labels, cur_item_keys, cur_ps, cur_pe = [], [], [], None, None

    for it in article.items:
        it_text = normalize_ws(it.text)
        lbl = item_label(it)
        # none item: label yok, ama metin var
        # item formatı: label + içerik
        if lbl:
            rendered = f"{lbl} {it_text}".strip()
            key_str = lbl
        else:
            rendered = it_text
            key_str = ""

        # item metni çok uzunsa: önce mevcut pack’i flush, sonra item’ı paragraf bazında böl
        if len(rendered) > int(MAX_CHARS * 1.35):
            flush_current()
            parts = split_long_text_by_paragraphs(rendered, MAX_CHARS)
            for part in parts:
                chunk_id = f"{DOC_ID}::{slugify(article.article_kind)}_{article.article_no}::chunk_{idx:05d}"
                idx += 1
                chunks.append(Chunk(
                    chunk_id=chunk_id,
                    doc_id=DOC_ID,
                    doc_name=DOC_NAME,
                    source_file=source_file,
                    page_start=it.page_start,
                    page_end=it.page_end,
                    bolum=article.bolum,
                    bolum_topic=article.bolum_topic,
                    article_kind=article.article_kind,
                    article_no=article.article_no,
                    article_title=article.article_title,
                    item_range=lbl if lbl else None,
                    item_keys=[key_str] if key_str else [],
                    text=part,
                    char_len=len(part)
                ))
            continue

        # normal packing (sadece aynı MADDE içinde)
        proposed_len = (len("\n\n".join(cur_parts)) + (2 if cur_parts else 0) + len(rendered))
        if cur_parts and proposed_len > MAX_CHARS and len("\n\n".join(cur_parts)) >= MIN_CHARS:
            flush_current()

        if cur_ps is None:
            cur_ps = it.page_start
        cur_pe = it.page_end

        if rendered:
            cur_parts.append(rendered)
        if lbl:
            cur_labels.append(lbl)
            cur_item_keys.append(lbl)

    flush_current()

    # Overlap: sadece aynı article içindeki ardışık chunk’lar arasında
    if OVERLAP_CHARS > 0 and len(chunks) >= 2:
        fixed = [chunks[0]]
        for prev, nxt in zip(chunks[:-1], chunks[1:]):
            tail = safe_tail_overlap(prev.text, OVERLAP_CHARS)
            merged = normalize_ws((tail + "\n\n" + nxt.text).strip())
            nxt2 = Chunk(**{**asdict(nxt), "text": merged, "char_len": len(merged)})
            fixed.append(nxt2)
        chunks = fixed

    return chunks, idx

def chunk_articles(articles: List[Article], pdf_path: str) -> List[Chunk]:
    source_file = os.path.basename(pdf_path)
    out: List[Chunk] = []
    idx = 0
    for a in articles:
        chs, idx = make_chunks_for_article(a, source_file, idx)
        out.extend(chs)
    return out


# =========================
# End-to-end
# =========================
def chunk_pdf_legal(pdf_path: str, out_jsonl: str) -> List[Chunk]:
    assert os.path.exists(pdf_path), f"PDF yok: {pdf_path}"

    lines, n_pages, total_chars = extract_text_lines(pdf_path)
    if total_chars == 0:
        raise RuntimeError("PDF text-based değil (scanned). OCR gerekir.")

    articles = parse_articles(lines)
    chunks = chunk_articles(articles, pdf_path)

    with open(out_jsonl, "w", encoding="utf-8") as f:
        for c in chunks:
            f.write(json.dumps(asdict(c), ensure_ascii=False) + "\n")

    print(f"pages: {n_pages}")
    print(f"total extracted chars: {total_chars}")
    print(f"articles: {len(articles)}")
    print(f"chunks: {len(chunks)}")
    print(f"wrote: {out_jsonl}")

    return chunks


# =========================
# Run + quick preview
# =========================
chunks = chunk_pdf_legal(PDF_PATH, OUT_JSONL)

for c in chunks[:8]:
    print("=" * 100)
    print(c.chunk_id, f"pages {c.page_start}-{c.page_end}", "len", c.char_len)
    print("bolum:", c.bolum)
    print("bolum_topic:", c.bolum_topic)
    print("article:", c.article_kind, c.article_no, "|", c.article_title)
    print("item_range:", c.item_range, "| item_keys:", c.item_keys[:6], ("..." if len(c.item_keys) > 6 else ""))
    print(c.text[:900], "..." if len(c.text) > 900 else "")