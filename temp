# -*- coding: utf-8 -*-
from __future__ import annotations

import re
import json
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional, Tuple

import fitz  # PyMuPDF


PDF_PATH = "/mnt/data/Tebliğ.pdf"
OUT_JSONL = "/mnt/data/tcmb_teblig_chunks.jsonl"

DOC_ID = "tcmb_teblig"
DOC_NAME = "TCMB TEBLİĞ"

# ----------------------------
# Token counter (lightweight)
# ----------------------------
# Hackathon için HF tokenizer şart değil; hızlı yaklaşım:
# 1 token ~ 0.75 kelime -> kelime sayısından tahmin.
def approx_tokens(text: str) -> int:
    w = len(text.split())
    return int(w / 0.75) if w else 0


# ----------------------------
# Normalization
# ----------------------------
def normalize_text(s: str) -> str:
    if not s:
        return ""
    s = s.replace("\u00ad", "")  # soft hyphen
    # hyphenation join: "hazır-\nlanan" -> "hazırlanan"
    s = re.sub(r"(\w)-\s*\n\s*(\w)", r"\1\2", s)
    # normalize whitespace
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()


def is_allcaps_tr(s: str) -> bool:
    # Türkçe büyük harf içeren kabaca kontrol
    letters = re.sub(r"[^A-Za-zÇĞİÖŞÜçğıöşü]", "", s)
    if len(letters) < 4:
        return False
    upp = sum(1 for ch in letters if ch == ch.upper())
    return upp / max(1, len(letters)) >= 0.85


# ----------------------------
# Line extraction with font/layout signals
# ----------------------------
@dataclass
class Line:
    page_no: int
    line_id: str
    text: str
    bbox: Tuple[float, float, float, float]
    font_max: float
    font_avg: float
    bold_ratio: float
    x_centered: bool


def extract_lines(pdf_path: str) -> List[Line]:
    doc = fitz.open(pdf_path)
    out: List[Line] = []

    for pi in range(len(doc)):
        page = doc.load_page(pi)
        W = float(page.rect.width)
        d = page.get_text("dict")
        page_no = pi + 1

        for bi, b in enumerate(d.get("blocks", [])):
            if b.get("type", 0) != 0:
                continue
            for li, ln in enumerate(b.get("lines", [])):
                spans = ln.get("spans", [])
                if not spans:
                    continue

                # line text: concatenate spans
                parts = [sp.get("text", "") for sp in spans]
                txt = normalize_text("".join(parts))
                if not txt:
                    continue

                # bbox union
                x0 = min(sp["bbox"][0] for sp in spans)
                y0 = min(sp["bbox"][1] for sp in spans)
                x1 = max(sp["bbox"][2] for sp in spans)
                y1 = max(sp["bbox"][3] for sp in spans)
                bbox = (float(x0), float(y0), float(x1), float(y1))

                sizes = [float(sp.get("size", 0.0)) for sp in spans if sp.get("size")]
                font_max = max(sizes) if sizes else 0.0
                font_avg = sum(sizes) / len(sizes) if sizes else 0.0

                # bold heuristic: flags bit or font name contains Bold
                def is_bold_span(sp: Dict[str, Any]) -> bool:
                    fn = (sp.get("font") or "").lower()
                    if "bold" in fn:
                        return True
                    flags = int(sp.get("flags", 0))
                    # in many PDFs, "bold" not consistently encoded; keep font-name check primary
                    return (flags & 2) == 2

                bold = sum(1 for sp in spans if is_bold_span(sp))
                bold_ratio = bold / max(1, len(spans))

                # centered line? (within margins)
                cx = (x0 + x1) / 2.0
                x_centered = (abs(cx - (W / 2.0)) <= W * 0.08) and (x1 - x0 <= W * 0.85)

                out.append(Line(
                    page_no=page_no,
                    line_id=f"p{page_no:03d}_b{bi:03d}_l{li:03d}",
                    text=txt,
                    bbox=bbox,
                    font_max=font_max,
                    font_avg=font_avg,
                    bold_ratio=bold_ratio,
                    x_centered=x_centered,
                ))
    return out


# ----------------------------
# Header & Article detection
# ----------------------------
MADDE_RE = re.compile(r"\bMADDE\s+(\d+)\b", re.IGNORECASE)
BOLUM_RE = re.compile(r"\b(BİRİNCİ|İKİNCİ|ÜÇÜNCÜ|DÖRDÜNCÜ|BEŞİNCİ|ALTINCI|YEDİNCİ|SEKİZİNCİ|DOKUZUNCU|ONUNCU)\s+BÖLÜM\b",
                      re.IGNORECASE)

def compute_font_thresholds(lines: List[Line]) -> Dict[str, float]:
    # robust percentiles without numpy
    sizes = sorted([ln.font_max for ln in lines if ln.font_max > 0.0])
    if not sizes:
        return {"p80": 0.0, "p90": 0.0}
    def perc(p: float) -> float:
        idx = int(round((len(sizes) - 1) * p))
        return sizes[max(0, min(len(sizes) - 1, idx))]
    return {"p80": perc(0.80), "p90": perc(0.90)}


def is_header_line(ln: Line, p80: float, p90: float) -> bool:
    t = ln.text.strip()
    if len(t) < 2:
        return False

    # exclude obvious body lines
    if t.endswith(".") and len(t) > 40:
        return False

    # very short candidate
    shortish = len(t) <= 60

    # strong signals
    big_font = ln.font_max >= max(p90, p80 + 0.5)
    semi_big = ln.font_max >= p80
    boldish = ln.bold_ratio >= 0.50
    centered = ln.x_centered
    allcaps = is_allcaps_tr(t)

    # "MADDE" lines are treated separately as article lines (not section headers)
    if MADDE_RE.search(t):
        return False

    # "BÖLÜM" line: always header if matches
    if BOLUM_RE.search(t):
        return True

    # Header rule:
    # - short + (semi_big or boldish or centered or allcaps)
    # - or big_font alone
    if big_font:
        return True
    if shortish and (semi_big or boldish or centered or allcaps):
        return True

    return False


def header_level(ln: Line) -> int:
    """
    1: major (BÖLÜM / centered allcaps big)
    2: sub (Amaç/Kapsam/Dayanak/Tanımlar gibi kısa başlık)
    """
    t = ln.text.strip()
    if BOLUM_RE.search(t):
        return 1
    if is_allcaps_tr(t) and ln.x_centered:
        return 1
    return 2


# ----------------------------
# Build sections: (major header) -> (subheader) -> body until next header
# plus track article (MADDE n) boundaries as metadata inside body.
# ----------------------------
@dataclass
class SectionBlock:
    section_id: str
    page_start: int
    page_end: int
    major_header: Optional[str]
    sub_header: Optional[str]
    text: str
    article_nos: List[int]
    token_len: int


def build_sections(lines: List[Line]) -> List[SectionBlock]:
    if not lines:
        return []

    # keep reading order: page_no then y0 then x0
    lines_sorted = sorted(lines, key=lambda x: (x.page_no, x.bbox[1], x.bbox[0]))

    th = compute_font_thresholds(lines_sorted)
    p80, p90 = th["p80"], th["p90"]

    # pass 1: label lines as header/article/body
    tags: List[Tuple[Line, str, Optional[int], Optional[int]]] = []
    # tuple: (line, tag, header_level, article_no)
    for ln in lines_sorted:
        t = ln.text.strip()

        m = MADDE_RE.search(t)
        if m:
            art_no = int(m.group(1))
            tags.append((ln, "article", None, art_no))
            continue

        if is_header_line(ln, p80, p90):
            lvl = header_level(ln)
            tags.append((ln, "header", lvl, None))
        else:
            tags.append((ln, "body", None, None))

    sections: List[SectionBlock] = []

    cur_major: Optional[str] = None
    cur_sub: Optional[str] = None
    buf: List[str] = []
    buf_pages: List[int] = []
    buf_articles: List[int] = []

    def flush():
        nonlocal buf, buf_pages, buf_articles, cur_major, cur_sub
        if not buf:
            return
        text = normalize_text("\n".join(buf))
        if not text:
            buf, buf_pages, buf_articles = [], [], []
            return

        ps = min(buf_pages) if buf_pages else 1
        pe = max(buf_pages) if buf_pages else ps

        sec_id = f"{DOC_ID}::sec_{len(sections):04d}"
        sections.append(SectionBlock(
            section_id=sec_id,
            page_start=ps,
            page_end=pe,
            major_header=cur_major,
            sub_header=cur_sub,
            text=text,
            article_nos=sorted(list(set(buf_articles))),
            token_len=approx_tokens(text),
        ))
        buf, buf_pages, buf_articles = [], [], []

    for ln, tag, lvl, art_no in tags:
        if tag == "header":
            # new header starts a new section (flush current)
            flush()

            if lvl == 1:
                cur_major = ln.text.strip()
                cur_sub = None
            else:
                # lvl 2
                cur_sub = ln.text.strip()

            # do NOT include header line in body; header stored as metadata
            continue

        # body/article lines become content
        buf.append(ln.text)
        buf_pages.append(ln.page_no)
        if tag == "article" and art_no is not None:
            buf_articles.append(art_no)

    flush()
    return sections


# ----------------------------
# Chunk packing: keep section headers in each chunk prefix
# ----------------------------
@dataclass
class Chunk:
    chunk_id: str
    doc_id: str
    doc_name: str
    source_file: str
    page_start: int
    page_end: int
    major_header: Optional[str]
    sub_header: Optional[str]
    article_nos: List[int]
    text: str
    token_len: int


def pack_sections_to_chunks(
    sections: List[SectionBlock],
    max_tokens: int = 450,
    overlap_tokens: int = 60
) -> List[Chunk]:

    chunks: List[Chunk] = []
    prev_tail: str = ""

    def header_prefix(major: Optional[str], sub: Optional[str]) -> str:
        parts = []
        if major:
            parts.append(major)
        if sub:
            parts.append(sub)
        return "\n".join(parts).strip()

    for si, sec in enumerate(sections):
        prefix = header_prefix(sec.major_header, sec.sub_header)
        body = sec.text.strip()

        # split large section body by paragraphs if needed
        paras = [p.strip() for p in re.split(r"\n\s*\n", body) if p.strip()]
        if not paras:
            paras = [body]

        cur = ""
        cur_ps, cur_pe = sec.page_start, sec.page_end
        cur_articles = set(sec.article_nos)

        def emit(text_block: str):
            nonlocal prev_tail
            if not text_block.strip():
                return
            full = text_block.strip()

            # overlap (tail of previous chunk)
            if overlap_tokens > 0 and prev_tail:
                full = (prev_tail + "\n\n" + full).strip()

            # compute next tail
            words = full.split()
            tail_word_count = min(len(words), max(20, int(overlap_tokens * 0.9)))
            prev_tail = " ".join(words[-tail_word_count:]) if tail_word_count > 0 else ""

            c = Chunk(
                chunk_id=f"{DOC_ID}::chunk_{len(chunks):05d}",
                doc_id=DOC_ID,
                doc_name=DOC_NAME,
                source_file=PDF_PATH.split("/")[-1],
                page_start=cur_ps,
                page_end=cur_pe,
                major_header=sec.major_header,
                sub_header=sec.sub_header,
                article_nos=sorted(list(cur_articles)),
                text=full,
                token_len=approx_tokens(full),
            )
            chunks.append(c)

        # pack paras under max_tokens
        base_prefix = (prefix + "\n\n") if prefix else ""
        cur = base_prefix

        for p in paras:
            candidate = (cur + ("\n\n" if cur.strip() != base_prefix.strip() else "") + p).strip()
            if approx_tokens(candidate) <= max_tokens:
                cur = candidate
                continue

            # flush existing
            emit(cur)

            # start new with header prefix
            cur = (base_prefix + p).strip()

            # if still too big, hard split by sentences (rare for tebliğ, but safe)
            if approx_tokens(cur) > max_tokens:
                sents = re.split(r"(?<=[\.\!\?;:])\s+", p)
                cur = base_prefix
                for s in sents:
                    cand2 = (cur + (" " if cur.strip() != base_prefix.strip() else "") + s).strip()
                    if approx_tokens(cand2) <= max_tokens:
                        cur = cand2
                    else:
                        emit(cur)
                        cur = (base_prefix + s).strip()

        emit(cur)

    return chunks


# ----------------------------
# End-to-end
# ----------------------------
def run(pdf_path: str, out_jsonl: str):
    lines = extract_lines(pdf_path)
    sections = build_sections(lines)

    chunks = pack_sections_to_chunks(
        sections,
        max_tokens=450,
        overlap_tokens=60
    )

    with open(out_jsonl, "w", encoding="utf-8") as f:
        for c in chunks:
            f.write(json.dumps(asdict(c), ensure_ascii=False) + "\n")

    print("Lines:", len(lines))
    print("Sections:", len(sections))
    print("Chunks:", len(chunks))
    print("Wrote:", out_jsonl)

    # Show first 10 chunk previews
    for c in chunks[:10]:
        print("=" * 90)
        print(c.chunk_id, f"pages {c.page_start}-{c.page_end}", f"tok~{c.token_len}",
              "major:", c.major_header, "| sub:", c.sub_header, "| maddeler:", c.article_nos)
        print(c.text[:600], "..." if len(c.text) > 600 else "")


if __name__ == "__main__":
    run(PDF_PATH, OUT_JSONL)
