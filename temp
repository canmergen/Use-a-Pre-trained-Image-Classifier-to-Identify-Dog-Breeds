# -*- coding: utf-8 -*-
from __future__ import annotations

import os, re, json
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional, Tuple

import fitz  # pymupdf

# =========================
# Config
# =========================
PDF_PATH = "/mnt/data/tcmb_teblig.pdf"
OUT_JSONL = "/mnt/data/tcmb_teblig_chunks.jsonl"

DOC_ID = "tcmb_teblig"
DOC_NAME = "TCMB TEBLİĞ"

# Prefer token budgets; fallback to chars if tokenizer yoksa
MAX_TOKENS = 450
MIN_TOKENS = 180
OVERLAP_TOKENS = 80

MAX_CHARS_FALLBACK = 1800
MIN_CHARS_FALLBACK = 400
OVERLAP_CHARS_FALLBACK = 250

KNOWN_SUBHEADINGS = {
    "AMAÇ","KAPSAM","DAYANAK","TANIMLAR","BİLGİLENDİRME","YÜRÜRLÜK","YÜRÜTME",
    "ÜCRETLERİN İADESİ","ÜCRETLERİN SINIFLANDIRILMASI","ÜCRETLERİN DEĞİŞTİRİLMESİ",
    "SÖZLEŞME ESASLARI","KAMPANYALAR VE ÖZEL HİZMETLER","SON HÜKÜMLER",
    "ÜCRETLER","ÜCRETLERİN BELİRLENMESİ"
}

BOLUM_RE = re.compile(r"^\s*(BİRİNCİ|İKİNCİ|ÜÇÜNCÜ|DÖRDÜNCÜ|BEŞİNCİ|ALTINCI|YEDİNCİ|SEKİZİNCİ|DOKUZUNCU|ONUNCU)\s+BÖLÜM\s*$", re.IGNORECASE)
MADDE_RE = re.compile(r"^\s*(GEÇİCİ\s+)?MADDE\s+(\d+)\s*[-–]?\s*$", re.IGNORECASE)
EK_RE    = re.compile(r"^\s*EK\s*[-–]?\s*(\d+)\s*.*$", re.IGNORECASE)

# Fıkra tespiti: satır başı "(1)" "(2)" ...
FIKRA_RE = re.compile(r"^\s*\((\d+)\)\s+")

# Bent tespiti (opsiyonel): "a)" "b)" ...
BENT_RE  = re.compile(r"^\s*([a-zçğıöşü])\)\s+", re.IGNORECASE)

# =========================
# Token counter
# =========================
class TokenCounter:
    def count(self, text: str) -> int:
        raise NotImplementedError

class HFTokenCounter(TokenCounter):
    def __init__(self, model_name: str):
        from transformers import AutoTokenizer
        self.tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    def count(self, text: str) -> int:
        return len(self.tok.encode(text, add_special_tokens=False))

class CharFallbackCounter(TokenCounter):
    def count(self, text: str) -> int:
        # TR için kaba yaklaşım: ~4 char/token varsayımı
        return max(1, len(text) // 4)

def norm(s: str) -> str:
    s = s.replace("\u00ad", "")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def up(s: str) -> str:
    return re.sub(r"\s+", " ", norm(s)).upper()

def is_known_subheading_line(t: str) -> bool:
    return up(t) in KNOWN_SUBHEADINGS

# =========================
# Layout extraction
# =========================
@dataclass
class Line:
    page: int
    text: str
    size: float
    bold: bool
    x0: float
    x1: float
    y0: float
    y1: float

def extract_lines(doc: fitz.Document) -> List[Line]:
    out: List[Line] = []
    for pi in range(len(doc)):
        page = doc.load_page(pi)
        d = page.get_text("dict")
        pno = pi + 1
        for b in d.get("blocks", []):
            if b.get("type", 0) != 0:
                continue
            for ln in b.get("lines", []):
                spans = ln.get("spans", [])
                if not spans:
                    continue
                txt = "".join(sp.get("text", "") for sp in spans)
                txt = norm(txt)
                if not txt:
                    continue
                size = max(float(sp.get("size", 0.0)) for sp in spans)
                # font flags: 16 is bold in many PDFs; not guaranteed but useful
                bold = any(int(sp.get("flags", 0)) & 16 for sp in spans)

                x0 = min(sp.get("bbox", [0,0,0,0])[0] for sp in spans)
                y0 = min(sp.get("bbox", [0,0,0,0])[1] for sp in spans)
                x1 = max(sp.get("bbox", [0,0,0,0])[2] for sp in spans)
                y1 = max(sp.get("bbox", [0,0,0,0])[3] for sp in spans)
                out.append(Line(pno, txt, size, bold, float(x0), float(x1), float(y0), float(y1)))

    out.sort(key=lambda L: (L.page, L.y0, L.x0))
    return out

def page_font_thresholds(lines: List[Line]) -> Dict[int, float]:
    """
    Sayfa bazında 'büyük font' eşiği: 85p quantile.
    Böylece ilerleyen sayfalarda font değişse bile başlık yakalanır.
    """
    by_page: Dict[int, List[float]] = {}
    for l in lines:
        by_page.setdefault(l.page, []).append(l.size)
    th = {}
    for p, sizes in by_page.items():
        sizes = sorted(sizes)
        if not sizes:
            th[p] = 0.0
            continue
        idx = int(len(sizes) * 0.85)
        th[p] = sizes[min(max(idx, 0), len(sizes)-1)]
    return th

def looks_like_heading(doc: fitz.Document, l: Line, big_th: float) -> bool:
    t = l.text
    u = up(t)

    # Hard rules
    if BOLUM_RE.match(u) or MADDE_RE.match(u) or EK_RE.match(u):
        return True
    if is_known_subheading_line(t):
        return True

    # Soft rules
    if len(t) < 2 or len(t) > 90:
        return False
    if t.endswith((".", ";", ":", ",")):
        return False

    page = doc.load_page(l.page - 1)
    w = page.rect.width
    center = (l.x0 + l.x1) / 2.0
    centered = abs(center - w/2.0) <= (w * 0.16)

    # uppercase ratio
    letters = [c for c in t if c.isalpha()]
    ur = 0.0
    if letters:
        upp = sum(1 for c in letters if c.upper() == c and c.lower() != c)
        ur = upp / len(letters)

    big = l.size >= big_th
    return (big and centered) or (l.bold and centered) or (centered and ur >= 0.70)

# =========================
# Structural model
# =========================
@dataclass
class HeaderState:
    bolum: Optional[str] = None
    bolum_topic: Optional[str] = None   # "Amaç, Kapsam, Dayanak ve Tanımlar" gibi
    article_kind: Optional[str] = None  # "MADDE"/"GEÇİCİ MADDE"/"EK"
    article_no: Optional[int] = None
    article_title: Optional[str] = None # "Amaç" gibi madde üstü başlık

@dataclass
class Atom:
    """
    RAG için en küçük güvenli parça: genelde fıkra.
    """
    atom_id: str
    page_start: int
    page_end: int
    h: HeaderState
    text: str

@dataclass
class Chunk:
    chunk_id: str
    doc_id: str
    doc_name: str
    source_file: str
    page_start: int
    page_end: int
    bolum: Optional[str]
    bolum_topic: Optional[str]
    article_kind: Optional[str]
    article_no: Optional[int]
    article_title: Optional[str]
    atom_ids: List[str]
    text: str
    token_len: int

def parse_heading(line_text: str) -> Tuple[str, Dict[str, Any]]:
    t = norm(line_text)
    u = up(t)

    m = BOLUM_RE.match(u)
    if m:
        return "BOLUM", {"bolum": t}

    m = MADDE_RE.match(u)
    if m:
        is_gecici = bool(m.group(1))
        no = int(m.group(2))
        return "MADDE", {"article_kind": "GEÇİCİ MADDE" if is_gecici else "MADDE", "article_no": no}

    m = EK_RE.match(u)
    if m:
        no = int(m.group(1))
        return "EK", {"article_kind": "EK", "article_no": no}

    # subheading candidate
    if is_known_subheading_line(t) or (2 <= len(t) <= 90):
        return "SUB", {"subheading": t}

    return "NONE", {}

def split_article_to_atoms(article_text_lines: List[Line], state: HeaderState, base_atom_id: str) -> List[Atom]:
    """
    MADDE içini fıkra atomlarına böler. Fıkra yoksa tek atom.
    """
    # satırları metne çevir
    lines = [l.text for l in article_text_lines]
    lines = [norm(x) for x in lines if norm(x)]
    if not lines:
        return []

    atoms: List[Atom] = []
    buf: List[str] = []
    buf_ps = article_text_lines[0].page
    buf_pe = article_text_lines[0].page
    cur_idx = 0

    def flush():
        nonlocal cur_idx, buf, buf_ps, buf_pe
        if not buf:
            return
        txt = norm("\n".join(buf))
        if txt:
            atoms.append(Atom(
                atom_id=f"{base_atom_id}::atom_{cur_idx:03d}",
                page_start=buf_ps,
                page_end=buf_pe,
                h=HeaderState(**asdict(state)),
                text=txt
            ))
            cur_idx += 1
        buf = []

    for L in article_text_lines:
        t = norm(L.text)
        if not t:
            continue
        # yeni fıkra başlangıcı
        if FIKRA_RE.match(t) and buf:
            flush()
            buf_ps = L.page
        buf_pe = L.page
        buf.append(t)

    flush()

    # hiç fıkra ayrımı yoksa tek atom yap
    if len(atoms) == 1:
        return atoms
    # bazı maddelerde ilk fıkra numarası kaçabilir; yine de bu yaklaşım metin kaybettirmez
    return atoms if atoms else [Atom(
        atom_id=f"{base_atom_id}::atom_000",
        page_start=article_text_lines[0].page,
        page_end=article_text_lines[-1].page,
        h=HeaderState(**asdict(state)),
        text=norm("\n".join(lines))
    )]

# =========================
# Main segmentation
# =========================
def build_atoms(doc: fitz.Document, lines: List[Line]) -> List[Atom]:
    big_th = page_font_thresholds(lines)

    atoms: List[Atom] = []
    h = HeaderState()

    i = 0
    # buffer for current article content lines
    cur_article_lines: List[Line] = []
    cur_article_head_line: Optional[Line] = None

    # temporary: subheading that should attach to next MADDE as article_title
    pending_madde_title: Optional[str] = None

    def flush_article():
        nonlocal cur_article_lines, cur_article_head_line, pending_madde_title, h

        if not cur_article_lines:
            return

        # build article-level atoms
        base = f"{DOC_ID}::{h.article_kind or 'NA'}_{h.article_no or 0}"
        if h.article_title:
            base += f"::{re.sub(r'\\s+','_', up(h.article_title))}"

        atoms.extend(split_article_to_atoms(cur_article_lines, h, base_atom_id=base))
        cur_article_lines = []
        cur_article_head_line = None

    while i < len(lines):
        L = lines[i]
        t = L.text
        u = up(t)

        is_head = looks_like_heading(doc, L, big_th.get(L.page, 0.0))

        if is_head:
            kind, meta = parse_heading(t)

            if kind == "BOLUM":
                # flush previous article
                flush_article()
                h.bolum = meta["bolum"]
                h.bolum_topic = None
                h.article_kind = None
                h.article_no = None
                h.article_title = None
                pending_madde_title = None
                i += 1
                continue

            if kind == "SUB":
                sub = meta["subheading"]

                # Eğer bu SUB'dan kısa süre sonra MADDE geliyorsa, bunu madde_title yap.
                # Lookahead: sonraki 6 satır içinde MADDE varsa.
                lookahead = 6
                will_attach_to_madde = False
                for j in range(1, lookahead+1):
                    if i+j >= len(lines):
                        break
                    uu = up(lines[i+j].text)
                    if MADDE_RE.match(uu):
                        will_attach_to_madde = True
                        break
                    # araya başka güçlü heading (BÖLÜM/EK) girerse bağlama
                    if BOLUM_RE.match(uu) or EK_RE.match(uu):
                        break

                if will_attach_to_madde:
                    pending_madde_title = sub
                else:
                    # bölüm topic gibi davranabilir (ör. "Amaç, Kapsam, Dayanak ve Tanımlar")
                    # büyük/ortalanmış ise bolum_topic’a yaz
                    if h.article_kind is None:
                        h.bolum_topic = sub
                    else:
                        # madde içi alt başlık: bunu article_title olarak da set edebiliriz
                        h.article_title = sub
                i += 1
                continue

            if kind in ("MADDE", "EK"):
                flush_article()
                h.article_kind = meta["article_kind"]
                h.article_no = meta["article_no"]
                h.article_title = pending_madde_title
                pending_madde_title = None

                # MADDE başlık satırını içerik buffer’ına dahil et (RAG için iyi)
                cur_article_lines = [L]
                cur_article_head_line = L
                i += 1
                continue

        # normal line
        if h.article_kind is not None:
            cur_article_lines.append(L)
        else:
            # Preamble / heading dışı: bolum başlamadan metin varsa, onu bolum_topic gibi sakla
            # ama kaybetme: “preamble atom” üret
            if norm(t):
                pre_h = HeaderState(**asdict(h))
                atoms.append(Atom(
                    atom_id=f"{DOC_ID}::preamble::p{L.page:03d}::i{i:05d}",
                    page_start=L.page,
                    page_end=L.page,
                    h=pre_h,
                    text=norm(t)
                ))
        i += 1

    flush_article()
    return atoms

# =========================
# Chunk packing (atom-based, no text loss)
# =========================
def pack_atoms(atoms: List[Atom], source_file: str, counter: TokenCounter) -> List[Chunk]:
    chunks: List[Chunk] = []
    cur_atoms: List[Atom] = []
    cur_tokens = 0

    def meta_of(a: Atom):
        return a.h

    def flush():
        nonlocal cur_atoms, cur_tokens
        if not cur_atoms:
            return
        text = "\n\n".join(a.text for a in cur_atoms).strip()
        ps = min(a.page_start for a in cur_atoms)
        pe = max(a.page_end for a in cur_atoms)
        h = meta_of(cur_atoms[-1])

        chunks.append(Chunk(
            chunk_id=f"{DOC_ID}::chunk_{len(chunks):05d}",
            doc_id=DOC_ID,
            doc_name=DOC_NAME,
            source_file=source_file,
            page_start=ps,
            page_end=pe,
            bolum=h.bolum,
            bolum_topic=h.bolum_topic,
            article_kind=h.article_kind,
            article_no=h.article_no,
            article_title=h.article_title,
            atom_ids=[a.atom_id for a in cur_atoms],
            text=text,
            token_len=counter.count(text)
        ))
        cur_atoms = []
        cur_tokens = 0

    def take_overlap_tail(prev_atoms: List[Atom]) -> List[Atom]:
        if OVERLAP_TOKENS <= 0:
            return []
        tail: List[Atom] = []
        toks = 0
        for a in reversed(prev_atoms):
            at = counter.count(a.text)
            if tail and toks + at > OVERLAP_TOKENS:
                break
            tail.append(a)
            toks += at
        return list(reversed(tail))

    for a in atoms:
        a_tok = counter.count(a.text)

        # If single atom is huge (rare), split by paragraphs safely
        if a_tok > MAX_TOKENS * 1.7:
            flush()
            paras = [p.strip() for p in re.split(r"\n\s*\n", a.text) if p.strip()]
            buf = []
            bt = 0
            for p in paras:
                pt = counter.count(p)
                if buf and bt + pt > MAX_TOKENS:
                    tmp = "\n\n".join(buf).strip()
                    chunks.append(Chunk(
                        chunk_id=f"{DOC_ID}::chunk_{len(chunks):05d}",
                        doc_id=DOC_ID,
                        doc_name=DOC_NAME,
                        source_file=source_file,
                        page_start=a.page_start,
                        page_end=a.page_end,
                        bolum=a.h.bolum,
                        bolum_topic=a.h.bolum_topic,
                        article_kind=a.h.article_kind,
                        article_no=a.h.article_no,
                        article_title=a.h.article_title,
                        atom_ids=[a.atom_id],
                        text=tmp,
                        token_len=counter.count(tmp)
                    ))
                    buf = []
                    bt = 0
                buf.append(p)
                bt += pt
            if buf:
                tmp = "\n\n".join(buf).strip()
                chunks.append(Chunk(
                    chunk_id=f"{DOC_ID}::chunk_{len(chunks):05d}",
                    doc_id=DOC_ID,
                    doc_name=DOC_NAME,
                    source_file=source_file,
                    page_start=a.page_start,
                    page_end=a.page_end,
                    bolum=a.h.bolum,
                    bolum_topic=a.h.bolum_topic,
                    article_kind=a.h.article_kind,
                    article_no=a.h.article_no,
                    article_title=a.h.article_title,
                    atom_ids=[a.atom_id],
                    text=tmp,
                    token_len=counter.count(tmp)
                ))
            continue

        if cur_atoms and (cur_tokens + a_tok > MAX_TOKENS) and (cur_tokens >= MIN_TOKENS):
            prev_atoms = cur_atoms[:]
            flush()
            # overlap = atom tail (no slicing => metin kaybı yok)
            tail = take_overlap_tail(prev_atoms)
            if tail:
                cur_atoms = tail[:]
                cur_tokens = sum(counter.count(x.text) for x in cur_atoms)

        if not cur_atoms:
            cur_atoms = [a]
            cur_tokens = a_tok
        else:
            cur_atoms.append(a)
            cur_tokens += a_tok

    flush()
    return chunks

# =========================
# End-to-end
# =========================
def chunk_pdf_legal(pdf_path: str, out_jsonl: str, token_model: Optional[str] = None) -> List[Chunk]:
    assert os.path.exists(pdf_path), f"PDF yok: {pdf_path}"
    doc = fitz.open(pdf_path)

    total_chars = sum(len(doc.load_page(i).get_text("text").strip()) for i in range(len(doc)))
    if total_chars == 0:
        raise RuntimeError("PDF text-based değil. OCR gerekir.")

    counter: TokenCounter
    if token_model:
        try:
            counter = HFTokenCounter(token_model)
        except Exception:
            counter = CharFallbackCounter()
    else:
        counter = CharFallbackCounter()

    lines = extract_lines(doc)
    atoms = build_atoms(doc, lines)

    chunks = pack_atoms(atoms, source_file=os.path.basename(pdf_path), counter=counter)

    with open(out_jsonl, "w", encoding="utf-8") as f:
        for c in chunks:
            f.write(json.dumps(asdict(c), ensure_ascii=False) + "\n")

    print("pages:", len(doc))
    print("total extracted chars:", total_chars)
    print("atoms:", len(atoms))
    print("chunks:", len(chunks))
    print("wrote:", out_jsonl)
    return chunks

# RUN
# token_model öneri: "BAAI/bge-m3" veya kullandığın embedding modelinin tokenizer'ı
chunks = chunk_pdf_legal(PDF_PATH, OUT_JSONL, token_model=None)

# preview
for c in chunks[:5]:
    print("="*90)
    print(c.chunk_id, f"pages {c.page_start}-{c.page_end}", "tok", c.token_len)
    print("bolum:", c.bolum)
    print("topic:", c.bolum_topic)
    print("article:", c.article_kind, c.article_no, "title:", c.article_title)
    print(c.text[:900], "..." if len(c.text) > 900 else "")