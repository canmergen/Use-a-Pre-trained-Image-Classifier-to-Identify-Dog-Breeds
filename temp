def align_region_texts_v3(
    paddle_ocr_texts,
    paddle_ocr_regions,
    fuzzy_threshold: int = 90,
    length_tolerance: int = 2,
):
    """
    paddle_ocr_texts: sayfa düzeyi paddle ocr tam metin string veya list[str]
    paddle_ocr_regions: region listesi veya sayfa başına region listesi
    Amaç: region["text"] değerlerini paddle_ocr_texts içindeki düzgün kısımlar ile düzeltmek.
    """

    # -----------------------------------------------------------
    # --- 1) Helper: Türkçe karakter normalizasyonu ------------
    # -----------------------------------------------------------
    import re, unicodedata
    from rapidfuzz import fuzz
    from typing import Any, List, Tuple

    TR_MAP = str.maketrans({
        "İ": "I", "I": "I", "ı": "i",
        "Ş": "S", "ş": "s",
        "Ğ": "G", "ğ": "g",
        "Ü": "U", "ü": "u",
        "Ö": "O", "ö": "o",
        "Ç": "C", "ç": "c",
    })

    def _norm(s: str) -> str:
        if not isinstance(s, str):
            return ""
        s = s.translate(TR_MAP)
        s = unicodedata.normalize("NFKD", s)
        s = "".join(ch for ch in s if not unicodedata.combining(ch))
        s = re.sub(r"[^A-Za-z0-9]+", "", s).lower()
        return s

    def _get_text(x: Any) -> str:
        if isinstance(x, str):
            return x
        if isinstance(x, dict):
            for k in ("text", "extractedText", "extracted_text", "full_text"):
                v = x.get(k)
                if isinstance(v, str):
                    return v
        return ""

    def _build_norm_stream_with_map(src: str) -> Tuple[str, List[int]]:
        norm_chars: List[str] = []
        norm2orig: List[int] = []
        for i, ch in enumerate(src):
            ch_tr = ch.translate(TR_MAP)
            ch_dec = unicodedata.normalize("NFKD", ch_tr)
            base = "".join(c for c in ch_dec if not unicodedata.combining(c))
            base = re.sub(r"[^A-Za-z0-9]+", "", base)
            for _ in base.lower():
                norm_chars.append(_)
                norm2orig.append(i)
        return "".join(norm_chars), norm2orig

    def _has_number(s: str) -> bool:
        return bool(re.search(r"\d", s))

    # -----------------------------------------------------------
    # --- 2) Çok sayfa mı tek sayfa mı -> normalize -------------
    # -----------------------------------------------------------
    is_multi = (
        isinstance(paddle_ocr_regions, list)
        and len(paddle_ocr_regions) > 0
        and isinstance(paddle_ocr_regions[0], list)
    )
    pages = paddle_ocr_regions if is_multi else [paddle_ocr_regions]
    page_texts = (
        paddle_ocr_texts
        if isinstance(paddle_ocr_texts, list)
        else [paddle_ocr_texts]
    )

    # Çıkış listesi
    out_pages = []

    # -----------------------------------------------------------
    # --- 3) Her sayfayı işleme --------------------------------
    # -----------------------------------------------------------
    for p_i, regs in enumerate(pages):

        page_text_raw = _get_text(page_texts[p_i]) if p_i < len(page_texts) else ""
        norm_stream, norm2orig = _build_norm_stream_with_map(page_text_raw)

        page_out = []

        # Tüm region’lar gezilir
        for r in regs:
            r_txt_raw = _get_text(r)
            r_norm = _norm(r_txt_raw)
            if not r_norm:
                page_out.append(r)
                continue

            L = len(r_norm)

            # -----------------------------------------------------------
            # --- Akıllı eşik: rakam varsa daha esnek eşik/tolerans ----
            # -----------------------------------------------------------
            if _has_number(r_txt_raw):
                # sayılar hatalı olabiliyor: eşik düşür
                local_threshold = min(fuzzy_threshold, 80)
                local_len_tol = max(length_tolerance, 4)
            else:
                # normal text -> katı
                local_threshold = fuzzy_threshold
                local_len_tol = length_tolerance

            # -----------------------------------------------------------
            # --- 4) Direkt substring eşleşmesi -------------------------
            # -----------------------------------------------------------
            j = norm_stream.find(r_norm)
            if j != -1:
                s_o = norm2orig[j]
                e_o = norm2orig[j + L - 1]
                r["text"] = page_text_raw[s_o:e_o + 1]
                page_out.append(r)
                continue

            # -----------------------------------------------------------
            # --- 5) Fuzzy pencere tarama (length ± tolerance) ----------
            # -----------------------------------------------------------
            best_score, best_s, best_len = -1, None, None

            min_len = max(1, L - local_len_tol)
            max_len = min(len(norm_stream), L + local_len_tol)

            for win_len in range(min_len, max_len + 1):
                if win_len > len(norm_stream):
                    continue
                for start in range(0, len(norm_stream) - win_len + 1):
                    cand = norm_stream[start:start + win_len]
                    score = fuzz.ratio(cand, r_norm)
                    if score > best_score:
                        best_score, best_s, best_len = score, start, win_len

            if best_score is not None and best_score >= local_threshold:
                s_o = norm2orig[best_s]
                e_o = norm2orig[best_s + best_len - 1]
                r["text"] = page_text_raw[s_o:e_o + 1]

            page_out.append(r)

        out_pages.append(page_out)

    return out_pages if is_multi else out_pages[0]

paddle_ocr_regions_aligned = align_region_texts_v3(
    paddle_ocr_texts=paddle_ocr_texts,
    paddle_ocr_regions=paddle_ocr_regions,
    fuzzy_threshold=90,
    length_tolerance=2
)