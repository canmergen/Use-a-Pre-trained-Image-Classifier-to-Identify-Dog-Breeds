def extract_and_merge_sermaye_v2(
    pages_scaled,
    bottom_df=None,
    fuzzy_threshold: int = 80,
    line_tol: float = 0.8,
    max_dx: int = 1600
):
    import re
    import pandas as pd
    from rapidfuzz import fuzz

    # ---------------- number parsing helpers ----------------
    DIGIT_RX = re.compile(r"\d[\d\s\.,]*")
    _RX_COMMA_DEC  = re.compile(r"^\s*\d{1,3}(?:\.\d{3})*,\d{1,2}\s*$")   # 1.234,56
    _RX_DOT_DEC    = re.compile(r"^\s*\d{1,3}(?:,\d{3})*\.\d{1,2}\s*$")   # 1,234.56
    _RX_DOTS_THOUS = re.compile(r"^\s*\d{1,3}(?:\.\d{3})+\s*$")           # 12.050.000
    _RX_COMM_THOUS = re.compile(r"^\s*\d{1,3}(?:,\d{3})+\s*$")            # 12,050,000
    _RX_INTEGER    = re.compile(r"^\s*\d+\s*$")

    def _extract_number_str(s: str):
        if not s:
            return None
        cands = [m.group(0).strip() for m in DIGIT_RX.finditer(s)]
        return max(cands, key=len) if cands else None

    def _parse_tr_number(num_str: str):
        if not num_str:
            return None
        s = num_str.strip()
        for tok in ["TL","tl","Tl","tL","₺","TRY","try","Try"]:
            s = s.replace(tok, "")
        s = re.sub(r"\s+", "", s)

        if _RX_COMMA_DEC.match(s):
            s = s.replace(".", "").replace(",", ".")
        elif _RX_DOT_DEC.match(s):
            s = s.replace(",", "")
        elif _RX_DOTS_THOUS.match(s):
            s = s.replace(".", "")
        elif _RX_COMM_THOUS.match(s):
            s = s.replace(",", "")
        elif _RX_INTEGER.match(s):
            pass
        else:
            # fallback heuristic
            last_dot, last_comma = s.rfind("."), s.rfind(",")
            if last_comma > last_dot >= 0:
                s = s.replace(".", "").replace(",", ".")
            elif last_dot > last_comma >= 0:
                s = s.replace(",", "")
            else:
                s = s.replace(".", "").replace(",", "")
        try:
            return float(s)
        except ValueError:
            return None

    # ---------------- geometry helpers ----------------
    def _h(b): return b.get("height", b["y_max"] - b["y_min"])
    def _same_line(a, b, tol):
        ya = (a["y_min"] + a["y_max"]) / 2.0
        yb = (b["y_min"] + b["y_max"]) / 2.0
        return abs(ya - yb) <= tol * max(_h(a), 1)
    def _reading_order(items):
        return sorted(range(len(items)), key=lambda i: (items[i]["bbox"]["y_min"], items[i]["bbox"]["x_min"]))
    def _nearest_value_on_right(items, idx_label, max_dx, line_tol):
        A = items[idx_label]["bbox"]
        cxA = (A["x_min"] + A["x_max"]) / 2.0
        best_i, best_dx = None, None
        for i, it in enumerate(items):
            if i == idx_label: 
                continue
            B = it["bbox"]
            if B["x_min"] <= cxA: 
                continue
            if not _same_line(A, B, line_tol): 
                continue
            if not DIGIT_RX.search(it.get("text","")): 
                continue
            dx = B["x_min"] - cxA
            if 0 <= dx <= max_dx and (best_dx is None or dx < best_dx):
                best_i, best_dx = i, dx
        return best_i

    # ---------------- extract sermaye per page ----------------
    rows = []
    for pidx, items in enumerate(pages_scaled or []):
        if not items: 
            continue

        order = _reading_order(items)
        pos = {i: k for k, i in enumerate(order)}

        for i, it in enumerate(items):
            txt = (it.get("text","") or "").casefold()
            score = max(
                fuzz.partial_ratio(txt, "sermaye"),
                fuzz.partial_ratio(txt, "sermayesi"),
                fuzz.partial_ratio(txt, "ödenmiş sermaye"),
                fuzz.partial_ratio(txt, "çıkarılmış sermaye"),
                fuzz.partial_ratio(txt, "sermayenin"),
            )
            if score < fuzzy_threshold:
                continue

            label_bbox = it["bbox"]
            value_txt = None
            value_bbox = None
            value_str  = _extract_number_str(it.get("text",""))
            value_num  = _parse_tr_number(value_str) if value_str else None

            if value_num is None:
                cand = _nearest_value_on_right(items, i, max_dx=max_dx, line_tol=line_tol)
                if cand is not None:
                    value_txt = items[cand].get("text","")
                    value_bbox = items[cand]["bbox"]
                    value_str  = _extract_number_str(value_txt)
                    value_num  = _parse_tr_number(value_str) if value_str else None

            if value_num is None:
                k = pos[i]
                for j in [order[k+1]] if k+1 < len(order) else []:
                    if DIGIT_RX.search(items[j].get("text","")):
                        value_txt  = items[j].get("text","")
                        value_bbox = items[j]["bbox"]
                        value_str  = _extract_number_str(value_txt)
                        value_num  = _parse_tr_number(value_str) if value_str else None
                        if value_num is not None:
                            break
                if value_num is None and k-1 >= 0:
                    j = order[k-1]
                    if DIGIT_RX.search(items[j].get("text","")):
                        value_txt  = items[j].get("text","")
                        value_bbox = items[j]["bbox"]
                        value_str  = _extract_number_str(value_txt)
                        value_num  = _parse_tr_number(value_str) if value_str else None

            if value_txt is None and value_str is not None:
                value_txt  = it.get("text","")
                value_bbox = label_bbox

            rows.append({
                "page_idx":   pidx,
                "label_text": it.get("text",""),
                "label_bbox": label_bbox,
                "value_text": value_txt,
                "value_bbox": value_bbox,
                "value_str":  value_str,
                "value_num":  value_num,
            })

    sermaye_df = pd.DataFrame(rows)

    # ---------------- robust merge into bottom_df ----------------
    n_pages = len(pages_scaled or [])
    # pick best (largest) value per page
    best = (sermaye_df.dropna(subset=["value_num"])
                      .sort_values(["page_idx","value_num"], ascending=[True,False])
                      .groupby("page_idx", as_index=False)
                      .first()[["page_idx","value_num","value_bbox"]]
                      .rename(columns={"value_num":"sermaye_value","value_bbox":"sermaye_bbox"}))

    # If no values found, still return a bottom_df with page_idx column
    if bottom_df is None or (isinstance(bottom_df, pd.DataFrame) and bottom_df.empty):
        skeleton = pd.DataFrame({"page_idx": list(range(n_pages))})
        merged = skeleton.merge(best, on="page_idx", how="left")
        return sermaye_df, merged

    # Ensure there's a page_idx to merge on
    bd = bottom_df.copy()
    if "page_idx" not in bd.columns:
        # If length matches page count, assign indices 0..n_pages-1
        if len(bd) == n_pages:
            bd = bd.reset_index(drop=True)
            bd["page_idx"] = range(n_pages)
        else:
            # Create a skeleton and left-join existing data onto it
            sk = pd.DataFrame({"page_idx": list(range(n_pages))})
            bd = sk.merge(bd, left_index=True, right_index=True, how="left")

    merged = bd.merge(best, on="page_idx", how="left")
    return sermaye_df, merged

# 1) Run extraction + merge (works even if you pass None/empty bottom_df)
sermaye_df, bottom_df2 = extract_and_merge_sermaye_v2(pages_scaled, bottom_df=bottom_df)

# 2) Inspect
display(sermaye_df.head())      # all detections
display(bottom_df2.head())      # your original df + sermaye_value + sermaye_bbox


def extract_name_boxes(
    pages_scaled,
    ner_backend: str = "hf",    # "hf" (transformers) | "spacy" | "regex_only"
    min_conf: float = 0.70,     # NER güven eşiği
    allow_single_token: bool = True,  # Tek kelime PERSON'ları kabul et (imzalar için faydalı)
    blacklist_words=None        # Rol/ünvan kara listesi
):
    """
    Ad-Soyad içerdiği düşünülen bbox'ları döndürür.
    Girdi: pages_scaled -> List[List[{'text': str, 'bbox': {...}}]]
    Çıktı: pandas.DataFrame[page_idx, text, bbox, score, method]
    """
    import re
    import pandas as pd

    # --- Kara liste (ad-soyad olmayan PERSON benzeri kalıplar/ünvanlar) ---
    if blacklist_words is None:
        blacklist_words = {
            "başkanı","başkan","yönetim","kurulu","üyesi","katip",
            "müdür","komiser","noter","vekili","temsilci","imza",
            "denetçi","raportör","sekreter","yeminli","smmm","ymm"
        }

    # --- Regex yedek (Türkçe karakter destekli: En az 2 kelime, Baş harf büyük) ---
    U = "A-ZÇĞİÖŞÜ"
    L = "a-zçğıöşü"
    NAME_RX = re.compile(rf"\b[{U}][{L}]+(?:\s+[{U}][{L}]+)+\b")

    def likely_role(s: str) -> bool:
        s_norm = (s or "").casefold()
        return any(w in s_norm for w in blacklist_words)

    # --- NER backend hazırlığı ---
    nlp_hf = None
    nlp_spacy = None

    if ner_backend == "hf":
        # Çok dilli, Türkçe'yi iyi taşıyan bir model
        from transformers import pipeline
        # Alternatifler:
        #   "Davlan/xlm-roberta-base-ner-hrl"
        #   "savasy/bert-base-turkish-ner-cased"
        nlp_hf = pipeline(
            "token-classification",
            model="Davlan/xlm-roberta-base-ner-hrl",
            aggregation_strategy="simple"
        )
    elif ner_backend == "spacy":
        import spacy
        try:
            nlp_spacy = spacy.load("tr_core_news_lg")
        except Exception:
            nlp_spacy = spacy.load("xx_ent_wiki_sm")  # fallback (zayıf ama çalışır)

    rows = []

    for pidx, items in enumerate(pages_scaled or []):
        if not items:
            continue

        for it in items:
            text = (it.get("text") or "").strip()
            if not text:
                continue
            if likely_role(text):
                continue  # rol/ünvan cümleleri ele

            found = False
            best_score = 0.0
            method = None

            # --- 1) NER (HF) ---
            if ner_backend == "hf" and nlp_hf is not None:
                ents = nlp_hf(text)
                # PERSON olanları grupla
                person_spans = [e for e in ents if e.get("entity_group","") == "PER"]
                # Çoklu token içerenleri veya tek token’ı isteğe bağlı kabul et
                for e in person_spans:
                    # transformers çıktısı: {'word','score','start','end',...}
                    ent_text = text[e['start']:e['end']].strip()
                    if likely_role(ent_text):
                        continue
                    # kaç kelime?
                    wc = len(ent_text.split())
                    if wc >= 2 or (allow_single_token and wc >= 1):
                        score = float(e.get("score", 0.0))
                        if score >= min_conf and score > best_score:
                            best_score = score
                            found = True
                            method = "hf"
                # HF yöntemi PERSON bulduysa kutuyu işaretle
                if found:
                    rows.append({
                        "page_idx": pidx,
                        "text": text,
                        "bbox": it["bbox"],
                        "score": best_score,
                        "method": method
                    })
                    continue

            # --- 2) NER (spaCy) ---
            if ner_backend == "spacy" and nlp_spacy is not None:
                doc = nlp_spacy(text)
                for ent in doc.ents:
                    if ent.label_.upper() in {"PER","PERSON"}:
                        ent_text = ent.text.strip()
                        if likely_role(ent_text):
                            continue
                        wc = len(ent_text.split())
                        if wc >= 2 or (allow_single_token and wc >= 1):
                            found = True
                            best_score = 1.0  # spaCy skor vermiyor; 1.0 kabul
                            method = "spacy"
                            break
                if found:
                    rows.append({
                        "page_idx": pidx,
                        "text": text,
                        "bbox": it["bbox"],
                        "score": best_score,
                        "method": method
                    })
                    continue

            # --- 3) Regex yedek ---
            m = NAME_RX.search(text)
            if m:
                ent_text = m.group(0)
                if not likely_role(ent_text):
                    rows.append({
                        "page_idx": pidx,
                        "text": text,
                        "bbox": it["bbox"],
                        "score": 0.60,   # regex güveni
                        "method": "regex"
                    })

    import pandas as pd
    adsoyad_df = pd.DataFrame(rows).sort_values(["page_idx","method","score"], ascending=[True, True, False]).reset_index(drop=True)
    return adsoyad_df

# 1) Transformers ile (önerilen)
adsoyad_df = extract_name_boxes(pages_scaled, ner_backend="hf", min_conf=0.70)

# 2) spaCy ile
# adsoyad_df = extract_name_boxes(pages_scaled, ner_backend="spacy")

# 3) Sadece regex yedeği (internet/kurulum yoksa)
# adsoyad_df = extract_name_boxes(pages_scaled, ner_backend="regex_only")

display(adsoyad_df.head())

# İstersen en iyi adayları sayfa bazında filtrele:
top_names = (adsoyad_df
             .sort_values(["page_idx","score"], ascending=[True, False])
             .groupby("page_idx", as_index=False)
             .head(5))  # her sayfadan ilk 5 kutu