# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import re
import json
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional, Tuple

import fitz  # PyMuPDF


# =========================
# CONFIG
# =========================
PDF_PATH = "/mnt/data/tcmb_teblig.pdf"
OUT_JSONL = "/mnt/data/tcmb_teblig_chunks.jsonl"

DOC_ID = "tcmb_teblig"
DOC_NAME = "TCMB TEBLİĞ"

# Chunk budget (RAG için karakter bazlı pratik)
MAX_CHARS = 1600
MIN_CHARS = 450

# Overlap: char tail yerine paragraf overlap (headers + meaning korunur)
OVERLAP_PARAS = 1  # sonraki chunk'a öncekinin son 1 paragrafını ekle

# Header/footer tespiti için
HEADER_FOOTER_TOP_Y = 0.14   # sayfanın üst %14'ü
HEADER_FOOTER_BOT_Y = 0.86   # sayfanın alt %14'ü
REPEAT_MIN_PAGES = 3         # en az kaç sayfada tekrar ederse header/footer sayalım

KNOWN_SUBHEADINGS = {
    "AMAÇ","KAPSAM","DAYANAK","TANIMLAR","BİLGİLENDİRME","YÜRÜRLÜK","YÜRÜTME",
    "ÜCRETLERİN İADESİ","ÜCRETLERİN SINIFLANDIRILMASI","ÜCRETLERİN DEĞİŞTİRİLMESİ",
    "SÖZLEŞME ESASLARI","KAMPANYALAR VE ÖZEL HİZMETLER","SON HÜKÜMLER"
}

# =========================
# REGEX (daha toleranslı)
# =========================

# Örn: "BİRİNCİ BÖLÜM" veya "I. BÖLÜM"
BOLUM_RE = re.compile(
    r"^\s*(?:(?P<roman>[IVXLC]+)\.?|(?P<tr>Birinci|İkinci|Üçüncü|Dördüncü|Beşinci|Altıncı|Yedinci|Sekizinci|Dokuzuncu|Onuncu))\s+BÖLÜM\s*$",
    re.IGNORECASE
)

# Örn: "MADDE 5 –" veya "GEÇİCİ MADDE 1 -" veya "MADDE 5-"
MADDE_INLINE_RE = re.compile(
    r"^\s*(?P<kind>GEÇİCİ\s+MADDE|MADDE)\s+(?P<no>\d+)\s*[-–:]?\s*(?P<title>.*)\s*$",
    re.IGNORECASE
)

# Ek: "EK-1 ..." / "EK 1 ..."
EK_INLINE_RE = re.compile(
    r"^\s*EK\s*[-–]?\s*(?P<no>\d+)\s*[-–:]?\s*(?P<title>.*)\s*$",
    re.IGNORECASE
)

# Fıkra başlangıcı: "(1) ..." "(2) ..."
FIKRA_START_RE = re.compile(r"^\s*\((\d+)\)\s+")

# Bent: "a) ..." "b) ..." "ç) ..." "aa) ..." (TR karakterleri dahil)
BENT_START_RE = re.compile(r"^\s*([a-zçğıöşü]{1,3})\)\s+", re.IGNORECASE)

# =========================
# UTILS
# =========================
def normalize_ws(s: str) -> str:
    # soft hyphen kaldır
    s = s.replace("\u00ad", "")
    # satır içi fazla boşluk
    s = re.sub(r"[ \t]+", " ", s)
    # aşırı boş satır
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def up(s: str) -> str:
    return normalize_ws(s).upper()

def slugify(s: str) -> str:
    t = up(s)
    t = re.sub(r"\s+", "_", t)
    t = re.sub(r"[^A-Z0-9ÇĞİÖŞÜ_]+", "", t)
    return t.strip("_")

def is_subheading_line(line: str) -> bool:
    t = up(line)
    if t in KNOWN_SUBHEADINGS:
        return True
    # Title Case gelenler için: "Amaç" / "Kapsam" vb.
    t2 = normalize_ws(line).strip()
    if len(t2) <= 60 and up(t2) in KNOWN_SUBHEADINGS:
        return True
    return False

def is_likely_short_heading(line: str) -> bool:
    t = normalize_ws(line)
    if not t:
        return False
    if len(t) > 80:
        return False
    # başlık satırı genelde noktalama ile bitmez
    if t.endswith((".", ";", ":", ",")):
        return False
    # çok kısa / tek kelime başlıklar
    if is_subheading_line(t):
        return True
    return False


# =========================
# TEXT EXTRACTION (layout-safe)
# =========================
@dataclass
class Line:
    page: int
    text: str
    x0: float
    y0: float
    x1: float
    y1: float

def extract_lines_layout(doc: fitz.Document) -> List[Line]:
    """
    get_text("dict") üzerinden satırları alır.
    - Metin kaçırmayı azaltmak için spans join sırasında boşluk ekler.
    - Okuma sırası: page -> block -> line
    """
    out: List[Line] = []
    for pi in range(len(doc)):
        page = doc.load_page(pi)
        d = page.get_text("dict")
        page_no = pi + 1

        for b in d.get("blocks", []):
            if b.get("type", 0) != 0:
                continue
            for ln in b.get("lines", []):
                spans = ln.get("spans", [])
                if not spans:
                    continue

                parts = []
                for sp in spans:
                    tx = sp.get("text", "")
                    if tx:
                        parts.append(tx)

                txt = "".join(parts)
                txt = txt.replace("\xa0", " ")
                txt = normalize_ws(txt)
                if not txt:
                    continue

                x0 = min(sp.get("bbox", [0, 0, 0, 0])[0] for sp in spans)
                y0 = min(sp.get("bbox", [0, 0, 0, 0])[1] for sp in spans)
                x1 = max(sp.get("bbox", [0, 0, 0, 0])[2] for sp in spans)
                y1 = max(sp.get("bbox", [0, 0, 0, 0])[3] for sp in spans)

                out.append(Line(page=page_no, text=txt, x0=float(x0), y0=float(y0), x1=float(x1), y1=float(y1)))

    out.sort(key=lambda L: (L.page, L.y0, L.x0))
    return out

def detect_repeating_header_footer(lines: List[Line], doc: fitz.Document) -> Tuple[set, set]:
    """
    Sayfa üst/alt bölgesinde tekrar eden satırları header/footer sayıp temizlemek için set döner.
    """
    top_counter: Dict[str, set] = {}
    bot_counter: Dict[str, set] = {}

    for l in lines:
        page = doc.load_page(l.page - 1)
        h = page.rect.height
        y_mid = (l.y0 + l.y1) / 2.0

        key = up(l.text)
        if len(key) < 6:
            continue

        if y_mid <= h * HEADER_FOOTER_TOP_Y:
            top_counter.setdefault(key, set()).add(l.page)
        elif y_mid >= h * HEADER_FOOTER_BOT_Y:
            bot_counter.setdefault(key, set()).add(l.page)

    header = {k for k, pages in top_counter.items() if len(pages) >= REPEAT_MIN_PAGES}
    footer = {k for k, pages in bot_counter.items() if len(pages) >= REPEAT_MIN_PAGES}
    return header, footer

def remove_header_footer(lines: List[Line], header_set: set, footer_set: set) -> List[Line]:
    out = []
    for l in lines:
        key = up(l.text)
        if key in header_set or key in footer_set:
            continue
        out.append(l)
    return out

def join_hyphenated_linebreaks(paragraph: str) -> str:
    """
    PDF extraction bazen:
      "finansal tüketici-\nye" gibi böler.
    Basit birleştirme: "-\n" -> "" (kelime devamı varsayımı)
    """
    paragraph = paragraph.replace("-\n", "")
    return paragraph

# =========================
# STRUCTURE PARSING
# =========================
@dataclass
class Atom:
    """
    Atom: RAG için anlamlı en küçük birim (başlık + takip eden içerik).
    """
    page_start: int
    page_end: int
    bolum: Optional[str]
    bolum_topic: Optional[str]
    article_kind: Optional[str]
    article_no: Optional[int]
    article_title: Optional[str]
    subheading: Optional[str]
    item_key: Optional[str]     # fıkra no veya bent (1), a), vb.
    text: str                   # içerik (başlık satırı dahil değil; biz aşağıda ekleyeceğiz)

def build_atoms(lines: List[Line]) -> List[Atom]:
    """
    1) BÖLÜM yakala (ve hemen altındaki topic satırını bağlam olarak al)
    2) MADDE/GEÇİCİ MADDE/EK yakala
    3) Alt başlık (Amaç/Kapsam/...) yakala
    4) İçerik: bir sonraki aynı/üst seviye başlığa kadar topla
    5) MADDE içeriğini fıkra/bent üzerinden atomlara böl (çok uzun paragrafları kontrol)
    """
    atoms: List[Atom] = []

    cur_bolum = None
    cur_bolum_topic = None

    cur_article_kind = None
    cur_article_no = None
    cur_article_title = None

    cur_sub = None

    # Buffer: current section body lines
    buf: List[Line] = []

    def flush_buf_as_atoms():
        nonlocal buf, atoms
        if not buf:
            return

        ps = buf[0].page
        pe = buf[-1].page

        # metni paragraf yapısına yakın kur: satırları birleştir
        raw = "\n".join(l.text for l in buf).strip()
        raw = normalize_ws(raw)
        raw = join_hyphenated_linebreaks(raw)
        if not raw:
            buf = []
            return

        # Eğer bir MADDE içindeysek: fıkra/bent parçalamaya çalış
        if cur_article_kind and cur_article_no is not None:
            # paragraflara böl
            paras = [p.strip() for p in re.split(r"\n\s*\n", raw) if p.strip()]
            if not paras:
                buf = []
                return

            # Her paragrafın "item_key"ini çıkar (varsa)
            # Sonra küçük atomlar halinde biriktir
            tmp: List[Tuple[str, Optional[str]]] = []
            for p in paras:
                key = None
                m = FIKRA_START_RE.match(p)
                if m:
                    key = f"({m.group(1)})"
                else:
                    m2 = BENT_START_RE.match(p)
                    if m2:
                        key = f"{m2.group(1).lower()})"
                tmp.append((p, key))

            # Atom üretimi: aynı item_key ile devam edenleri birleştir (pratik)
            cur_text_parts: List[str] = []
            cur_item = None

            def emit(text_joined: str, item_key: Optional[str]):
                atoms.append(Atom(
                    page_start=ps, page_end=pe,
                    bolum=cur_bolum, bolum_topic=cur_bolum_topic,
                    article_kind=cur_article_kind, article_no=cur_article_no, article_title=cur_article_title,
                    subheading=cur_sub, item_key=item_key,
                    text=text_joined.strip()
                ))

            for p, key in tmp:
                # yeni fıkra/bent başladıysa önceki atomu bas
                if key is not None and cur_text_parts:
                    emit("\n\n".join(cur_text_parts), cur_item)
                    cur_text_parts = []
                    cur_item = key
                    cur_text_parts.append(p)
                else:
                    if cur_item is None:
                        cur_item = key
                    cur_text_parts.append(p)

            if cur_text_parts:
                emit("\n\n".join(cur_text_parts), cur_item)

        else:
            # Madde dışında (örn BÖLÜM başlığı altı genel açıklamalar)
            atoms.append(Atom(
                page_start=ps, page_end=pe,
                bolum=cur_bolum, bolum_topic=cur_bolum_topic,
                article_kind=cur_article_kind, article_no=cur_article_no, article_title=cur_article_title,
                subheading=cur_sub, item_key=None,
                text=raw
            ))

        buf = []

    i = 0
    n = len(lines)

    while i < n:
        l = lines[i]
        t = normalize_ws(l.text)

        # 1) BÖLÜM
        mbol = BOLUM_RE.match(t)
        if mbol:
            flush_buf_as_atoms()

            cur_bolum = t
            cur_bolum_topic = None
            cur_article_kind = None
            cur_article_no = None
            cur_article_title = None
            cur_sub = None

            # Bölüm başlığından sonra gelen "topic" satırını yakala (genelde bir sonraki satır)
            # Topic satırı: kısa ve başlık gibi, ama "MADDE ..." değil
            if i + 1 < n:
                nxt = normalize_ws(lines[i + 1].text)
                if nxt and not BOLUM_RE.match(nxt) and not MADDE_INLINE_RE.match(nxt) and not EK_INLINE_RE.match(nxt):
                    # "Amaç, Kapsam, Dayanak ve Tanımlar" gibi
                    if len(nxt) <= 120:
                        cur_bolum_topic = nxt
                        i += 1  # topic'i tükettik

            i += 1
            continue

        # 2) MADDE / GEÇİCİ MADDE
        mmad = MADDE_INLINE_RE.match(t)
        if mmad:
            flush_buf_as_atoms()

            kind = up(mmad.group("kind"))
            no = int(mmad.group("no"))
            title = normalize_ws(mmad.group("title") or "")
            title = title if title else None

            cur_article_kind = "GEÇİCİ MADDE" if "GEÇİCİ" in kind else "MADDE"
            cur_article_no = no
            cur_article_title = title
            cur_sub = None

            # MADDE satırını “bağlam” olarak body'e koymak yerine metadata'da tutuyoruz.
            i += 1
            continue

        # 3) EK
        mek = EK_INLINE_RE.match(t)
        if mek:
            flush_buf_as_atoms()

            no = int(mek.group("no"))
            title = normalize_ws(mek.group("title") or "")
            title = title if title else None

            cur_article_kind = "EK"
            cur_article_no = no
            cur_article_title = title
            cur_sub = None

            i += 1
            continue

        # 4) Alt başlıklar (Amaç/Kapsam/Dayanak/Tanımlar/...)
        if is_subheading_line(t):
            flush_buf_as_atoms()
            cur_sub = t
            i += 1
            continue

        # 5) Normal içerik satırı
        buf.append(l)
        i += 1

    flush_buf_as_atoms()
    return atoms


# =========================
# CHUNK PACKING
# =========================
@dataclass
class Chunk:
    chunk_id: str
    doc_id: str
    doc_name: str
    source_file: str
    page_start: int
    page_end: int

    bolum: Optional[str]
    bolum_topic: Optional[str]

    article_kind: Optional[str]
    article_no: Optional[int]
    article_title: Optional[str]

    subheading: Optional[str]
    item_key: Optional[str]

    atom_ids: List[str]
    text: str
    char_len: int

def atom_id(a: Atom) -> str:
    parts = [DOC_ID]
    if a.bolum:
        parts.append(slugify(a.bolum))
    if a.bolum_topic:
        parts.append(slugify(a.bolum_topic))
    if a.article_kind and a.article_no is not None:
        parts.append(f"{slugify(a.article_kind)}_{a.article_no}")
    if a.article_title:
        parts.append(slugify(a.article_title)[:40])
    if a.subheading:
        parts.append(slugify(a.subheading))
    if a.item_key:
        parts.append(slugify(a.item_key))
    parts.append(f"p{a.page_start}-{a.page_end}")
    return "::".join(parts)

def format_atom_for_chunk(a: Atom) -> str:
    """
    RAG için atomu chunk içine koyarken başlık bağlamlarını da metin olarak ekle.
    Bu sayede retrieval sırasında “Amaç nedir?” gibi sorular direkt yakalanır.
    """
    ctx = []
    if a.bolum:
        ctx.append(a.bolum)
    if a.bolum_topic:
        ctx.append(a.bolum_topic)
    if a.article_kind and a.article_no is not None:
        if a.article_title:
            ctx.append(f"{a.article_kind} {a.article_no} – {a.article_title}")
        else:
            ctx.append(f"{a.article_kind} {a.article_no}")
    if a.subheading:
        ctx.append(a.subheading)
    if a.item_key:
        ctx.append(a.item_key)

    header = " / ".join(ctx).strip()
    if header:
        return header + "\n" + a.text.strip()
    return a.text.strip()

def pack_atoms_to_chunks(atoms: List[Atom], source_file: str) -> List[Chunk]:
    chunks: List[Chunk] = []

    cur_parts: List[str] = []
    cur_atom_ids: List[str] = []

    cur_ps = None
    cur_pe = None

    # meta: chunk meta'sunu en güncel atomdan taşır
    cur_meta = dict(
        bolum=None, bolum_topic=None,
        article_kind=None, article_no=None, article_title=None,
        subheading=None, item_key=None
    )

    def flush():
        nonlocal cur_parts, cur_atom_ids, cur_ps, cur_pe, cur_meta
        if not cur_parts:
            return

        text = "\n\n".join(cur_parts).strip()
        text = normalize_ws(text)
        if len(text) < 30:
            cur_parts = []
            cur_atom_ids = []
            cur_ps = None
            cur_pe = None
            return

        chunks.append(Chunk(
            chunk_id=f"{DOC_ID}::chunk_{len(chunks):05d}",
            doc_id=DOC_ID,
            doc_name=DOC_NAME,
            source_file=source_file,
            page_start=cur_ps,
            page_end=cur_pe,
            bolum=cur_meta["bolum"],
            bolum_topic=cur_meta["bolum_topic"],
            article_kind=cur_meta["article_kind"],
            article_no=cur_meta["article_no"],
            article_title=cur_meta["article_title"],
            subheading=cur_meta["subheading"],
            item_key=cur_meta["item_key"],
            atom_ids=cur_atom_ids[:],
            text=text,
            char_len=len(text)
        ))

        cur_parts = []
        cur_atom_ids = []
        cur_ps = None
        cur_pe = None

    for a in atoms:
        a_text = format_atom_for_chunk(a)
        if not a_text:
            continue

        # atom id
        aid = atom_id(a)

        # yeni chunk'a geçme kuralı: char budget aşımı + min chars sağlandıysa
        cur_len = sum(len(x) for x in cur_parts) + (2 * max(0, len(cur_parts) - 1))
        proposed = cur_len + (2 if cur_parts else 0) + len(a_text)

        if cur_parts and proposed > MAX_CHARS and cur_len >= MIN_CHARS:
            flush()

        if cur_ps is None:
            cur_ps = a.page_start
        cur_pe = a.page_end

        # meta'yı güncelle
        cur_meta = dict(
            bolum=a.bolum, bolum_topic=a.bolum_topic,
            article_kind=a.article_kind, article_no=a.article_no, article_title=a.article_title,
            subheading=a.subheading, item_key=a.item_key
        )

        cur_parts.append(a_text)
        cur_atom_ids.append(aid)

    flush()

    # Paragraf bazlı overlap: sonraki chunk'a öncekinin son N paragrafını ekle
    if OVERLAP_PARAS > 0 and len(chunks) >= 2:
        out = [chunks[0]]
        for prev, nxt in zip(chunks[:-1], chunks[1:]):
            prev_paras = [p.strip() for p in re.split(r"\n\s*\n", prev.text) if p.strip()]
            tail = "\n\n".join(prev_paras[-OVERLAP_PARAS:]).strip() if prev_paras else ""
            merged = (tail + "\n\n" + nxt.text).strip() if tail else nxt.text

            nxt_dict = asdict(nxt)
            nxt_dict["text"] = merged
            nxt_dict["char_len"] = len(merged)
            out.append(Chunk(**nxt_dict))
        chunks = out

    return chunks


# =========================
# END-TO-END
# =========================
def chunk_pdf_legal(pdf_path: str, out_jsonl: str) -> List[Chunk]:
    assert os.path.exists(pdf_path), f"PDF yok: {pdf_path}"
    doc = fitz.open(pdf_path)

    # text-based kontrol
    total_chars = 0
    for i in range(len(doc)):
        total_chars += len(doc.load_page(i).get_text("text").strip())

    if total_chars == 0:
        raise RuntimeError("PDF text-based değil (muhtemelen tarama). OCR olmadan chunk üretilemez.")

    lines = extract_lines_layout(doc)

    # header/footer temizliği
    header_set, footer_set = detect_repeating_header_footer(lines, doc)
    lines = remove_header_footer(lines, header_set, footer_set)

    # atom üret
    atoms = build_atoms(lines)

    # chunk üret
    chunks = pack_atoms_to_chunks(atoms, source_file=os.path.basename(pdf_path))

    # yaz
    with open(out_jsonl, "w", encoding="utf-8") as f:
        for c in chunks:
            f.write(json.dumps(asdict(c), ensure_ascii=False) + "\n")

    return chunks


# =========================
# RUN + PREVIEW
# =========================
chunks = chunk_pdf_legal(PDF_PATH, OUT_JSONL)
print("pages:", len(fitz.open(PDF_PATH)))
print("wrote:", OUT_JSONL)
print("chunks:", len(chunks))
print("total extracted chars:", sum(c.char_len for c in chunks))

for c in chunks[:5]:
    print("=" * 90)
    print(c.chunk_id, f"pages {c.page_start}-{c.page_end}", "chars:", c.char_len)
    print("bolum:", c.bolum)
    print("topic:", c.bolum_topic)
    print("article:", c.article_kind, c.article_no, "title:", c.article_title)
    print("sub:", c.subheading, "item:", c.item_key)
    print(c.text[:900], "..." if len(c.text) > 900 else "")