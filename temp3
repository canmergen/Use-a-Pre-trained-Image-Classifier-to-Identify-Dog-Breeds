# -*- coding: utf-8 -*-
import os
import base64
from typing import List, Optional, Sequence, Dict, Any

import cv2
import numpy as np
import requests

# ---- Endpoint & auth (unchanged) ----
TARGET_URL  = "https://internalgw/neomediaoperationsinternal/api/clear-ocr/get/v1"
TOKEN_ENV   = "INTGW_ACCESS_KEY"   # do not hardcode the token


# ---------------- helpers ----------------
def _ensure_uint8(x: np.ndarray) -> np.ndarray:
    if x.dtype != np.uint8:
        x = np.clip(x, 0, 255).astype(np.uint8)
    return x

def _to_bgr(img: np.ndarray) -> np.ndarray:
    """Return BGR/GRAY uint8 that cv2.imencode accepts."""
    img = _ensure_uint8(img)
    if img.ndim == 2:
        return cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
    if img.ndim == 3 and img.shape[2] == 3:
        # If your arrays are RGB, flip channels here:
        # return img[:, :, ::-1]
        return img
    if img.ndim == 3 and img.shape[2] == 4:
        return cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)
    raise ValueError(f"Unsupported image shape: {img.shape}")

def _encode_png_base64(bgr: np.ndarray) -> str:
    ok, buf = cv2.imencode(".png", bgr)
    if not ok:
        raise RuntimeError("PNG encoding failed")
    return base64.b64encode(buf).decode("utf-8")

def _auto_downscale_to_size(img: np.ndarray,
                            max_mb: float = 4.5,
                            min_side: int = 600,
                            verbose: bool = False) -> np.ndarray:
    """
    Iteratively downscale until base64 PNG size <= max_mb or min_side is reached.
    """
    bgr = _to_bgr(img)
    # quick try without scaling
    b64 = _encode_png_base64(bgr)
    size_mb = len(b64) / 1_000_000.0
    if verbose:
        h, w = bgr.shape[:2]
        print(f"[OCR] initial size={size_mb:.2f} MB, shape={h}x{w}")

    if size_mb <= max_mb:
        return bgr

    h, w = bgr.shape[:2]
    scale = 0.85
    while size_mb > max_mb and min(h, w) > min_side:
        new_w, new_h = int(w * scale), int(h * scale)
        bgr = cv2.resize(bgr, (new_w, new_h), interpolation=cv2.INTER_AREA)
        b64 = _encode_png_base64(bgr)
        size_mb = len(b64) / 1_000_000.0
        h, w = new_h, new_w
        if verbose:
            print(f"[OCR] scaled -> size={size_mb:.2f} MB, shape={h}x{w}")
        # tighten scale a bit if still large
        if size_mb > max_mb:
            scale *= 0.90

    return bgr

def _headers() -> Dict[str, str]:
    token = os.environ[TOKEN_ENV]
    return {
        "Access-Token": token,
        "Content-Type": "application/json",
    }

def _payload_with_content(image_b64: str) -> Dict[str, Any]:
    """
    Your payload exactly as provided; only 'content/fileName/fileType' are set here.
    """
    return {
        "requestHeader": {
            "info": {"correlationPair": [{"key": "AppName", "value": "Postman"}]}
        },
        "customerNo": 10651337,
        "branchCode": 936,
        "channelInfo": "Branch",
        "language": "tr-TR",
        "transactionCode": "",
        "clientIP": "1.1.1.1",
        "clientPort": "0",
        "registrationNo": 49001,
        "userInfo": {
            "userID": 153,
            "userCode": "49001",
            "userBranchCode": "936",
            "roles": [{"roleID": "1501"}]
        },
        "performerUserInfo": {
            "userID": 153,
            "userCode": "49001",
            "userBranchCode": "936",
            "roles": [{"roleID": "1501"}]
        },
        "includeBbox": False,
        "fileName": "image.png",
        "fileType": "png",
        "content": image_b64,
    }

def _post_ocr(image_b64: str, *, debug: bool = False) -> str:
    resp = requests.post(
        TARGET_URL,
        headers=_headers(),
        json=_payload_with_content(image_b64),
        verify=False,
        timeout=30,
    )
    if debug:
        print("HTTP", resp.status_code)
        if resp.status_code != 200:
            print(resp.text[:600])
    resp.raise_for_status()

    data = resp.json()
    pages = (
        data.get("ocrPageResultList")
        or data.get("OcrPageResultList")
        or data.get("result", {}).get("ocrPageResultList")
        or []
    )
    return "\n".join([(p.get("extractedText") or "") for p in pages]).strip()


# ---------------- public API ----------------
def call_paddle_ocr_text_only_array(
    img: np.ndarray,
    *,
    max_payload_mb: float = 4.5,
    min_side: int = 600,
    debug: bool = False,
) -> str:
    """
    OCR a single NumPy array. No disk I/O.
    Automatically downscales PNG until base64 size <= max_payload_mb.
    """
    if img is None:
        return ""

    bgr = _auto_downscale_to_size(img, max_mb=max_payload_mb, min_side=min_side, verbose=debug)
    image_b64 = _encode_png_base64(bgr)
    return _post_ocr(image_b64, debug=debug)

def call_paddle_ocr_text_only_batch(
    imgs: Sequence[Optional[np.ndarray]],
    *,
    max_payload_mb: float = 4.5,
    min_side: int = 600,
    debug_each: bool = False,
) -> List[Optional[str]]:
    """
    Batch over a list like final_lower_imgs. Returns list[str|None] aligned with input.
    """
    out: List[Optional[str]] = []
    for i, im in enumerate(imgs):
        if im is None:
            out.append(None); continue
        try:
            txt = call_paddle_ocr_text_only_array(
                im, max_payload_mb=max_payload_mb, min_side=min_side, debug=debug_each
            )
            out.append(txt if txt else None)
        except Exception as e:
            if debug_each:
                print(f"[{i}] OCR failed:", e)
            out.append(None)
    return out

# Single image
# text0 = call_paddle_ocr_text_only_array(final_lower_imgs[0], debug=True)

# Batch
texts = call_paddle_ocr_text_only_batch(final_lower_imgs, debug_each=True)
for i, t in enumerate(texts):
    print(f"\n=== PAGE {i} ===")
    print(t or "(No text detected / OCR failed)")