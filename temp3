def rewrite_regions_texts_strict(paddle_ocr_regions, paddle_ocr_texts,
                                 flex=1, lookahead=60, min_ratio=0.55):
    """
    Her sayfada regions[j]['text']'i, aynı sayfanın paddle_ocr_texts[i]'ine göre,
    KUTU SINIRINI BOZMADAN yeniden yazar.

    Parametreler
    - flex:  her kutu için hedef pencere uzunluğunu ±flex token esnet (örn. n±1)
    - lookahead: arama, sayfa token akışında cursor'dan itibaren en fazla bu kadar token ileriye bakar
    - min_ratio: benzerlik (SequenceMatcher) alt sınırı; altındaysa orijinali temizleyip bırakır

    Not: BBox sırası korunur. Yalnızca 'text' güncellenir.
    """
    import re, unicodedata, difflib

    assert len(paddle_ocr_regions) == len(paddle_ocr_texts), "Uzunluklar uyuşmuyor."

    def _clean(s):
        if not isinstance(s, str): return ""
        s = unicodedata.normalize("NFKC", s)
        s = (s.replace("•"," ").replace("·"," ").replace("§","S")
               .replace("|"," ").replace("¦"," ").replace("—","-").replace("–","-")
               .replace("İ","İ").replace("i̇","i")
               .replace("Ş","Ş").replace("Ğ","Ğ").replace("Ç","Ç").replace("Ü","Ü")
               .replace("ş","ş").replace("ğ","ğ").replace("ç","ç").replace("ü","ü"))
        s = re.sub(r"\s*([.,:;/()\-])\s*", r"\1", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    def _tokens(s):
        s = _clean(s);  return s.split() if s else []

    def _yx_key(r):
        b = r.get("bbox", {})
        y = (b.get("y1",0)+b.get("y2",0)+b.get("y3",0)+b.get("y4",0))//4
        x = (b.get("x1",0)+b.get("x2",0)+b.get("x3",0)+b.get("x4",0))//4
        return (y, x)

    for page_regions, page_text in zip(paddle_ocr_regions, paddle_ocr_texts):
        if not page_regions: 
            continue

        # Yukarıdan-aşağı, soldan-sağa
        regs_sorted = sorted(page_regions, key=_yx_key)

        tgt_tokens = _tokens(page_text)
        if not tgt_tokens:
            # Sayfa düz metin boşsa kutu metinlerini sadece temizle
            for r in regs_sorted:
                r["text"] = _clean(r.get("text",""))
            continue

        cursor = 0  # hedef akışta ilerleyen imleç

        for r in regs_sorted:
            src_clean = _clean(r.get("text",""))
            src_toks  = src_clean.split() if src_clean else []
            n = len(src_toks)

            if n == 0:
                r["text"] = ""
                continue

            # Arama penceresi: cursor'dan itibaren lookahead token
            start = max(0, cursor)
            end   = min(len(tgt_tokens), start + max(lookahead, n + 2*flex))

            best = (0.0, start, start)  # (ratio, j0, j1)
            min_len = max(1, n - flex)
            max_len = min(n + flex, max(1, end - start))

            src_join = " ".join(src_toks)

            # Aynı uzunlukta (±flex) alt-dilimler üzerinde benzerlik ara
            for L in range(min_len, max_len + 1):
                j0 = start
                while j0 + L <= end:
                    cand = " ".join(tgt_tokens[j0:j0+L])
                    ratio = difflib.SequenceMatcher(a=src_join, b=cand, autojunk=False).ratio()
                    if ratio > best[0]:
                        best = (ratio, j0, j0+L)
                    j0 += 1

            ratio, j0, j1 = best
            if ratio >= min_ratio:
                r["text"] = " ".join(tgt_tokens[j0:j1])
                cursor = j1  # sırayı koru, ileri kay
            else:
                # eşleşme zayıf: sadece kendi metnini temizle (sınırları bozma)
                r["text"] = src_clean

    return paddle_ocr_regions

paddle_ocr_regions = rewrite_regions_texts_strict(
    paddle_ocr_regions, paddle_ocr_texts,
    flex=1,          # kutu başına ±1 token esneklik
    lookahead=60,    # ileri bakan pencere
    min_ratio=0.55   # benzerlik alt sınırı
)