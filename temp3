def apply_finalbox_fallback_for_empty_names(
    per_box_df: pd.DataFrame,
    *,
    lower_img,                 # cv2 BGR
    final_boxes: List[Any],
    known_names: Optional[List[str]],
    ocr_url: str,
    lang: str = "tur+eng",
    name_sim_threshold: float = 0.76,
    psm_final: int = 11,
    oem_final: int = 1,
    update_even_if_shorter: bool = False,
    preview_name_overrides: bool = True,   # isim bulunursa text_preview = isim
    max_name_tokens: int = 4,              # isim çıktısında token sınırı
    single_token_autocomplete: bool = True,
    single_token_threshold: float = 0.80   # tek kelimeyi tamamlama eşiği
) -> pd.DataFrame:
    """
    Son-aşama düzeltme (tek fonksiyon).
    - loc=='roles' & name_in_box boş ise final_box OCR + isim/TCKN çıkarımı
    - Gibberish token'lar filtrelenir.
    - (Yeni) name_clean tek kelimeyse ve known_names içinde güçlü eşleşme varsa otomatik tamamlanır.
    """
    import re, json, base64, unicodedata
    import numpy as np
    import cv2, requests
    from rapidfuzz import fuzz

    # ---------- helpers ----------
    def _to_xywh(b: Any) -> Tuple[int,int,int,int]:
        if isinstance(b, (tuple, list, np.ndarray)) and len(b)==4:
            x,y,w,h = [int(round(float(v))) for v in b]; return (x,y,w,h)
        if isinstance(b, dict):
            if all(k in b for k in ("x","y","w","h")):  return int(b["x"]), int(b["y"]), int(b["w"]), int(b["h"])
            if all(k in b for k in ("x0","y0","x1","y1")):  return int(b["x0"]), int(b["y0"]), int(b["x1"]-b["x0"]), int(b["y1"]-b["y0"])
            if all(k in b for k in ("left","top","right","bottom")):  return int(b["left"]), int(b["top"]), int(b["right"]-b["left"]), int(b["bottom"]-b["top"])
        raise ValueError("bbox format not supported")

    def _ocr_remote_png(img_gray, *, ocr_url: str, lang: str, psm: int, oem: int) -> str:
        ok, buf = cv2.imencode(".png", img_gray)
        if not ok: return ""
        payload = {"image": base64.b64encode(buf.tobytes()).decode("ascii"),
                   "lang": lang, "config": f"--psm {int(psm)} --oem {int(oem)}"}
        try:
            r = requests.post(ocr_url, data=json.dumps(payload),
                              headers={"Content-Type":"application/json"}, timeout=60)
            r.raise_for_status()
            return (r.json().get("text") or "").strip()
        except Exception:
            return ""

    def _ocr_box_raw(lower_img, xywh: Tuple[int,int,int,int], *, ocr_url: str, lang: str, psm: int=11, oem: int=1) -> str:
        x,y,w,h = xywh
        crop = lower_img[max(0,y):y+h, max(0,x):x+w]
        if crop is None or getattr(crop, "size", 0) == 0: return ""
        gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY) if crop.ndim==3 else crop
        gray = cv2.fastNlMeansDenoising(gray, None, 15, 7, 21)
        _, bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
        return _ocr_remote_png(bw, ocr_url=ocr_url, lang=lang, psm=psm, oem=oem)

    def _strip_diac(s: str) -> str:
        return "".join(c for c in unicodedata.normalize("NFKD", s) if not unicodedata.combining(c))
    def _U(s: str) -> str:
        s = (s or "").upper()
        return re.sub(r"\s+"," ", _strip_diac(s)).strip()

    _NAME_TOKEN_RE = re.compile(r"[A-Za-zÇĞİÖŞÜçğıöşü’']+")

    _NON_NAME_TOKENS = {
        "TOPLANTIDA","HAZIR","BULUNAN","ŞİRKETİN","ŞİRKET","PAYLARIN","PAYLARI","TOPLAMI","TOPLAM",
        "NİSAP","NİSABI","MEVCUT","ASGARİ","TİCARİ","MERKEZİ","MAD","MADDE","SAYI","KARAR","GÜNDEM",
        "HANELİ","VERGİ","NO","NOSU","MERKEZ","ADRESİ"
    }
    _NON_NAME_TOKENS_U = { _U(t) for t in _NON_NAME_TOKENS }

    _VOWELS = set(list("AEIİOÖUÜaeıioöuü"))
    def _has_vowel(s: str) -> bool: return any(ch in _VOWELS for ch in s)
    def _long_consonant_run(s: str, k: int = 5) -> bool:
        run = 0
        for ch in s:
            if ch.isalpha() and ch.upper() not in _VOWELS:
                run += 1
                if run >= k: return True
            else:
                run = 0
        return False

    def is_gibberish_token(tok: str) -> bool:
        U = _U(tok)
        if U in _NON_NAME_TOKENS_U: return True
        if any(ch.isdigit() for ch in tok): return True
        if len(tok) < 2 or len(tok) > 20: return True
        if not _has_vowel(tok): return True
        if _long_consonant_run(tok, k=5): return True
        if re.search(r"[^\wÇĞİÖŞÜçğıöşü]", tok) and len(tok) <= 3: return True
        return False

    def filter_name_tokens(tokens: List[str]) -> List[str]:
        return [t for t in tokens if (not is_gibberish_token(t))]

    ROLE_REGEX = re.compile(
        r"(YÖNETİM\s+KURULU\s+BAŞKANI|YÖNETİM\s+KURULU\s+ÜYESİ|YÖNETİM\s+KURULU\s+ÜYELERİ|YK\s+BAŞKANI|"
        r"DIVAN\s+BASKANI|DİVAN\s+BAŞKANI|TOPLANTI\s+BAŞKANI|OY\s+TOPLAMA\s+MEMURU|BAKANLIK\s+TEMSİLCİSİ)",
        flags=re.IGNORECASE
    )
    def cut_before_first_role(text: str) -> str:
        if not text: return text
        m = ROLE_REGEX.search(text)
        return text[m.start():] if m else text

    def find_tckn(text: str) -> Optional[str]:
        m = re.findall(r"(?<!\d)(\d{10,11})(?!\d)", (text or "").replace(" ", ""))
        return "; ".join(list(dict.fromkeys(m))) if m else None

    def _extract_names_multi(text: str, pool: List[str], thr: float) -> List[str]:
        if not text: return []
        text = cut_before_first_role(text)
        toks_all = [(m.group(0), m.start(), m.end()) for m in re.finditer(r"[A-Za-zÇĞİÖŞÜçğıöşü’']+", text)]
        toks = [(t,s,e) for (t,s,e) in toks_all if not is_gibberish_token(t)]
        if not toks: return []
        wins=[]
        for L in (3,2):
            for i in range(0, max(0, len(toks)-L+1)):
                wtxt = " ".join(t for (t,_,__) in toks[i:i+L])
                best_sc, mapped = 0.0, None
                for kn in (pool or []):
                    sc = max(fuzz.ratio(_U(wtxt), _U(kn))/100.0, fuzz.partial_ratio(_U(wtxt), _U(kn))/100.0)
                    if sc > best_sc: best_sc, mapped = sc, kn
                if (not pool) or best_sc >= thr:
                    wins.append((toks[i][1], toks[i+L-1][2], wtxt, mapped,
                                 best_sc + (0.10 if 6 <= len(wtxt) <= 40 else 0.0) + (0.05 if L==3 else 0.0)))
        if not wins: return []
        wins.sort(key=lambda x: (x[1]-x[0], -x[0]), reverse=True)
        chosen=[]; used=[]
        for s,e,wtxt,mapped,_ in wins:
            if any(not (e <= us or s >= ue) for (us,ue) in used):
                continue
            used.append((s,e))
            toks2 = filter_name_tokens(_NAME_TOKEN_RE.findall(mapped or wtxt))
            if len(toks2) >= 2:
                nm = " ".join(toks2[:max_name_tokens])
                if nm and all(fuzz.token_set_ratio(nm, ex) < 92 for ex in chosen):
                    chosen.append(nm)
        return chosen

    def _sanitize_names_keep(raw_names: Optional[str], raw_text: str) -> Optional[str]:
        if not (raw_names or raw_text): return None
        parts = [p.strip() for p in re.split(r"[;]+", (raw_names or "")) if p.strip()]
        if not parts: parts = [(raw_text or "").strip()]
        cleaned=[]
        for p in parts:
            toks = filter_name_tokens(_NAME_TOKEN_RE.findall(p))
            if not toks: continue
            cleaned.append(" ".join(toks[:max_name_tokens]))
        out=[]
        for c in cleaned:
            if all(fuzz.token_set_ratio(c, o) < 92 for o in out):
                out.append(c)
        return "; ".join(out) if out else None

    def _clean_text_gibberish(text: str) -> str:
        toks = filter_name_tokens(_NAME_TOKEN_RE.findall(text or ""))
        return " ".join(toks[:max_name_tokens]) if toks else (text or "")

    def _complete_single_token(tok: str, pool: List[str], thr: float) -> Optional[str]:
        """Tek kelimeyi known_names içinden en iyi tamamlama."""
        if not tok or not pool: return None
        u_tok = _U(tok)
        best, best_sc = None, 0.0
        for kn in pool:
            toks_kn = _NAME_TOKEN_RE.findall(kn)
            # token bazlı en iyi eşleşmeyi ölç
            sc_tok = 0.0
            for t in toks_kn:
                sc_tok = max(sc_tok,
                             fuzz.ratio(u_tok, _U(t))/100.0,
                             fuzz.partial_ratio(u_tok, _U(t))/100.0)
                if _U(t).startswith(u_tok):
                    sc_tok = max(sc_tok, 0.99)  # güçlü prefix sinyali
            # full-name kısmi eşleşme de göz önünde bulundur
            sc_full = fuzz.partial_ratio(u_tok, _U(kn))/100.0
            sc = max(sc_tok, sc_full)
            if sc > best_sc:
                best_sc, best = sc, kn
        if best and best_sc >= thr:
            toks = filter_name_tokens(_NAME_TOKEN_RE.findall(best))
            return " ".join(toks[:max_name_tokens]) if toks else None
        return None

    def _blank(x):
        return (x is None) or (isinstance(x, float) and np.isnan(x)) or (str(x).strip()=="")

    # ---------- main ----------
    df = per_box_df.copy()
    pool = list(dict.fromkeys(known_names or []))

    for idx, row in df.iterrows():
        if row.get("loc") != "roles" or not _blank(row.get("name_in_box")):
            continue

        # bbox seçimi
        try:
            b_wh = _to_xywh(row.get("bbox") if row.get("bbox") is not None
                            else final_boxes[int(row.get("i"))])
        except Exception:
            b_wh = _to_xywh(final_boxes[int(row.get("i"))])

        # OCR
        text_fb = _ocr_box_raw(lower_img, b_wh, ocr_url=ocr_url, lang=lang,
                               psm=psm_final, oem=oem_final)
        text_fb = re.sub(r"\s+", " ", (text_fb or "")).strip()
        if not text_fb:
            continue

        # İsim + TCKN
        tckn_p = find_tckn(text_fb)
        multi = _extract_names_multi(text_fb, pool, thr=name_sim_threshold) if text_fb else []
        raw_name = "; ".join(multi) if multi else None
        name_clean = _sanitize_names_keep(raw_name, raw_name or text_fb)

        # ---- Tek kelime otomatik tamamlama ----
        if single_token_autocomplete and name_clean:
            toks_nc = _NAME_TOKEN_RE.findall(name_clean)
            if len(toks_nc) == 1:
                maybe = _complete_single_token(toks_nc[0], pool, single_token_threshold)
                if maybe:
                    name_clean = maybe  # tek kelimeyi known_names'teki tam adıyla değiştir

        # text_preview: isim varsa isim; yoksa gibberish-temiz OCR
        new_preview = name_clean if (name_clean and preview_name_overrides) else _clean_text_gibberish(text_fb)
        old_prev = str(row.get("text_preview") or "")
        if update_even_if_shorter or (len(new_preview) > len(old_prev)):
            df.at[idx, "text_preview"] = new_preview[:800]

        # name_in_box yaz
        new_name = (f"{name_clean} | {tckn_p}" if (name_clean and tckn_p)
                    else (tckn_p if not name_clean else name_clean))
        if not _blank(new_name):
            df.at[idx, "name_in_box"] = new_name

    return df
