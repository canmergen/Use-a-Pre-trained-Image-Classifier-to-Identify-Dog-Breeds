{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a35c5453",
   "metadata": {},
   "source": [
    "# 1. OSA GROWTH INTELLIGENCE MODEL: EXECUTIVE SETUP\n",
    "**Objective:** Predict **Next Week's NET Value** to optimize Balance Sheet management.\n",
    "This notebook compares the Legacy Linear Model against the proposed **Updated Dynamic Model**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23cb65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "# Styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Constants\n",
    "IMAGES_DIR = 'images'\n",
    "if not os.path.exists(IMAGES_DIR):\n",
    "    os.makedirs(IMAGES_DIR)\n",
    "\n",
    "# Colors\n",
    "BLUE = '#1f77b4'\n",
    "ORANGE = '#ff7f0e'\n",
    "GRAY_DARK = '#333333'\n",
    "GRAY_LIGHT = '#999999'\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aae419",
   "metadata": {},
   "source": [
    "# 2. DATA INGESTION & PREPROCESSING\n",
    "Loading the 'Test' dataset and configuring the temporal index for time-series analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "excel_path = 'TH_regresyon_modelleme.xlsx'\n",
    "sheet_name = 'Test'\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Rename for clarity\n",
    "    rename_map = {'Tarih': 'Date', 'Net Inflow' : 'NET', \n",
    "                  'Spread (Beklenti)': 'EXP(CB avg-TLREF)', 'Market Anomaly': 'Market anomaly'}\n",
    "    df_raw = df_raw.rename(columns=rename_map)\n",
    "    \n",
    "    # Date Handling\n",
    "    if 'Date' in df_raw.columns: \n",
    "        df_raw['Date'] = pd.to_datetime(df_raw['Date'])\n",
    "    else: \n",
    "        df_raw = df_raw.reset_index().rename(columns={'index': 'Date'})\n",
    "        df_raw['Date'] = pd.to_datetime(df_raw['Date'])\n",
    "    \n",
    "    df_raw = df_raw.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    print(\"Dataset loaded. First 5 rows:\")\n",
    "    display(df_raw.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b25b1",
   "metadata": {},
   "source": [
    "# 3. FEATURE ENGINEERING: CONSTRUCTING THE PREDICTIVE SIGNAL\n",
    "Deriving critical input variables for the **OSA Growth Intelligence Model**.\n",
    "\n",
    "**Key Components:**\n",
    "*   **NET_lag1 (Momentum Signal):** Current Week's Net Flow ($Net_t$). Captures immediate market sentiment.\n",
    "*   **NET_roll3 (Trend Signal):** 3-Week Moving Average ($Avg(t, t-1, t-2)$). Smooths out volatility to reveal direction.\n",
    "*   **Target Variable:** **Next Week's Net Flow** ($Net_{t+1}$). This is what the model learns to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63cda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "# 1. NET_lag1: Current Week's Net Flow (t)\n",
    "if 'NET_lag1' not in df.columns: \n",
    "    df['NET_lag1'] = df['NET'] \n",
    "\n",
    "# 2. NET_roll3: Rolling Mean of Last 3 Weeks (including t)\n",
    "if 'NET_roll3' not in df.columns: \n",
    "    df['NET_roll3'] = df['NET'].rolling(window=3).mean()\n",
    "\n",
    "# 3. Target: Next Week's Net Flow (t+1)\n",
    "# We shift(-1) so that for row 't', the 'Target' column contains value of 't+1'\n",
    "df['Target'] = df['NET'].shift(-1)\n",
    "\n",
    "print(\"Features Created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d4c228",
   "metadata": {},
   "source": [
    "# 4. DATA INTEGRITY CHECK (VERIFICATION)\n",
    "Validating the time-shift logic ($Target_t = NET_{t+1}$) to ensure zero look-ahead bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Check\n",
    "cols_to_check = ['Date', 'NET', 'Target', 'NET_lag1', 'NET_roll3']\n",
    "print(\"Checking Shift Logic (Row t's Target must match Row t+1's NET):\")\n",
    "display(df[cols_to_check].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13120a38",
   "metadata": {},
   "source": [
    "# 5. ANALYTICAL UTILITIES\n",
    "Helper functions for Metric Calculation (MAE, RMSE, G-AUC) and Visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(actual, pred):\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    return mae, rmse\n",
    "\n",
    "def make_seamless(series, last_actual_val, last_actual_idx):\n",
    "    # Prepend the last actual value to start the forecast line from the Split Day\n",
    "    return pd.concat([pd.Series([last_actual_val], index=[last_actual_idx]), series])\n",
    "\n",
    "# Safe Date Map for plots\n",
    "safe_date_map = lambda i: df.loc[i, 'Date'] if i in df.index else df['Date'].max() + pd.Timedelta(weeks=(i - df.index.max()))\n",
    "\n",
    "def assign_siq_buckets(df, pred_col, num_buckets=8):\n",
    "    Q1, Q3 = np.percentile(df[pred_col], [25, 75])\n",
    "    SIQ = (Q3 - Q1) / 2\n",
    "    median = df[pred_col].median()\n",
    "    lower_bound = median - 3 * SIQ\n",
    "    upper_bound = median + 3 * SIQ\n",
    "    \n",
    "    if lower_bound == upper_bound:\n",
    "        bins = np.array([float('-inf'), lower_bound, float('inf')])\n",
    "    else:\n",
    "        bins = np.linspace(lower_bound, upper_bound, num_buckets + 1)\n",
    "        bins = np.unique(np.concatenate(([float('-inf')], bins, [float('inf')])))\n",
    "    \n",
    "    bucket_labels = list(range(len(bins) - 1))\n",
    "    df['bucket'] = pd.cut(df[pred_col], bins=bins, labels=bucket_labels, include_lowest=True)\n",
    "    return df\n",
    "\n",
    "def calculate_weighted_auc(df, pred_col, target_col):\n",
    "    auc_scores = []\n",
    "    weights = []\n",
    "    for bucket, group in df.groupby('bucket', observed=True):\n",
    "        if len(group[target_col].unique()) > 1:\n",
    "            auc = roc_auc_score(group[target_col], group[pred_col])\n",
    "            auc_scores.append(auc)\n",
    "            weights.append(len(group))\n",
    "    return np.average(auc_scores, weights=weights) if weights else np.nan\n",
    "\n",
    "def get_gauc_metrics(df, pred_col, target_col):\n",
    "    target_median = df[target_col].median()\n",
    "    df_temp = df.copy()\n",
    "    df_temp['target_binary'] = (df_temp[target_col] >= target_median).astype(int)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    df_temp['pred_scaled'] = scaler.fit_transform(df_temp[[pred_col]])\n",
    "    \n",
    "    df_temp = assign_siq_buckets(df_temp, 'pred_scaled')\n",
    "    gauc = calculate_weighted_auc(df_temp, 'pred_scaled', 'target_binary')\n",
    "    \n",
    "    status = \"ğŸ”´ RED\"\n",
    "    if gauc > 0.65: status = \"ğŸŸ¢ GREEN\"\n",
    "    elif gauc >= 0.60: status = \"ğŸŸ¡ YELLOW\"\n",
    "    \n",
    "    return gauc, status\n",
    "\n",
    "def plot_integrated(df_local, pred_train, pred_test, title, filename, split_date, color):\n",
    "    plt.figure(figsize=(18, 7))\n",
    "    plt.plot(df_local['Date'], df_local['NET'], label='Actual Data', color=GRAY_DARK, alpha=0.3, linewidth=3)\n",
    "    \n",
    "    # Training Fit\n",
    "    plt.plot(df_local.loc[pred_train.index, 'Date'], pred_train, label='Training/History Fit', color=color, linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "    \n",
    "    # Test Forecast (Seamless)\n",
    "    last_act_idx = pred_test.index[0] - 1\n",
    "    if last_act_idx in df_local.index:\n",
    "        last_val = df_local.loc[last_act_idx, 'NET']\n",
    "        seamless_test = make_seamless(pred_test, last_val, last_act_idx)\n",
    "        plot_dates = seamless_test.index.map(safe_date_map)\n",
    "        plt.plot(plot_dates, seamless_test, label='Test Forecast', color=color, linewidth=3)\n",
    "    else:\n",
    "        plt.plot(df_local.loc[pred_test.index, 'Date'], pred_test, label='Test Forecast', color=color, linewidth=3)\n",
    "        \n",
    "    plt.axvline(x=pd.to_datetime(split_date), color=BLUE, linestyle=':', alpha=0.7, label='Split Day')\n",
    "    plt.title(title, fontsize=16, fontweight='bold', color=BLUE)\n",
    "    plt.legend(); plt.grid(True, alpha=0.1)\n",
    "    \n",
    "    path = os.path.join(IMAGES_DIR, filename)\n",
    "    plt.savefig(path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals(model, title, color=ORANGE):\n",
    "    resid = model.resid\n",
    "    fitted = model.fittedvalues\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(fitted, resid, alpha=0.6, color=color, s=40, edgecolors='white', linewidth=0.5)\n",
    "    plt.axhline(0, color=BLUE, linestyle='--', linewidth=1.5)\n",
    "    plt.xlabel('Fitted Values'); plt.ylabel('Residuals')\n",
    "    plt.title(f'Diagnostic: Residual Stability', fontsize=13, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.1)\n",
    "    \n",
    "    # 2. Histogram (Normality)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(resid, kde=True, color=color, alpha=0.7)\n",
    "    plt.title(f'Diagnostic: Error Distribution', fontsize=13, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.1)\n",
    "    \n",
    "    plt.suptitle(f'Statistical Health Check: {title}', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_detailed_stats(model, model_name, df_test=None, pred_test=None):\n",
    "    print(f\"\\n{'-'*20} DETAILED STATISTICAL REPORT: {model_name} {'-'*20}\")\n",
    "    \n",
    "    # Spearman\n",
    "    rho, _ = spearmanr(model.model.endog, model.fittedvalues)\n",
    "    \n",
    "    # High-Level Metrics\n",
    "    cond_no = model.condition_number\n",
    "    col_status = \"Pass (Weak)\" if cond_no < 30 else \"High (Warning)\"\n",
    "    \n",
    "    # Calculate Train Metrics (from residuals)\n",
    "    train_mae = np.mean(np.abs(model.resid))\n",
    "    train_rmse = np.sqrt(mean_squared_error(model.model.endog, model.fittedvalues))\n",
    "\n",
    "    # Calculate Test Metrics if available\n",
    "    test_mae, test_rmse = np.nan, np.nan\n",
    "    if df_test is not None and pred_test is not None:\n",
    "        test_mae, test_rmse = get_metrics(df_test['NET'], pred_test)\n",
    "\n",
    "    # Standard Error of Regression (S)\n",
    "    std_err_reg = np.sqrt(model.mse_resid)\n",
    "\n",
    "    metrics_data = {\n",
    "        'Metric': ['R-Squared', 'Adj. R-Squared', 'Standard Error', 'Overfitting Gap', 'Multiple R', 'Spearman Rank Corr', 'AIC', 'Observations', 'Condition Number', \n",
    "                   'Train MAE', 'Train RMSE', 'Test MAE', 'Test RMSE'],\n",
    "        'Value': [model.rsquared, model.rsquared_adj, std_err_reg, (model.rsquared - model.rsquared_adj), np.sqrt(model.rsquared), rho, model.aic, model.nobs, cond_no, \n",
    "                  train_mae, train_rmse, test_mae, test_rmse],\n",
    "        'Notes': ['Strength of Fit', 'Penalized Fit', 'Arg. Error', 'Ideal < 0.05', 'Linear Consistency', 'Ranking Consistency', 'Lower is Better', '', col_status, \n",
    "                  'Training Error (Mean)', 'Training Error (Root Sq)', 'Test Error (Mean)', 'Test Error (Root Sq)']\n",
    "    }\n",
    "    display(pd.DataFrame(metrics_data))\n",
    "\n",
    "    # VIF Calculation (Safe)\n",
    "    try:\n",
    "        X = model.model.exog\n",
    "        vif_values = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "        vif_map = dict(zip(model.params.index, vif_values))\n",
    "    except:\n",
    "        vif_map = {}\n",
    "    \n",
    "    # ANOVA\n",
    "    anova_data = {\n",
    "        'Source': ['Regression', 'Residual'],\n",
    "        'df': [model.df_model, model.df_resid],\n",
    "        'SS': [model.ess, model.ssr],\n",
    "        'MS': [model.mse_model, model.mse_resid],\n",
    "        'F-Stat': [model.fvalue, np.nan],\n",
    "        'Prob(F)': [model.f_pvalue, np.nan]\n",
    "    }\n",
    "    display(pd.DataFrame(anova_data))\n",
    "    \n",
    "    # Coefficients with Stars & Confidence Intervals\n",
    "    # Get CI\n",
    "    conf_int = model.conf_int()\n",
    "    \n",
    "    coef_data = []\n",
    "    for idx in model.params.index:\n",
    "        p_val = model.pvalues[idx]\n",
    "        sig = \"â­â­â­\" if p_val < 0.01 else (\"â­â­\" if p_val < 0.05 else (\"â­\" if p_val < 0.1 else \"\"))\n",
    "        coef_data.append({\n",
    "            'Variable': idx, \n",
    "            'Coef': model.params[idx], \n",
    "            'Std Err': model.bse[idx],\n",
    "            't-Stat': model.tvalues[idx],\n",
    "            'P-Value': p_val, \n",
    "            'Lower 95%': conf_int.loc[idx, 0],\n",
    "            'Upper 95%': conf_int.loc[idx, 1],\n",
    "            'VIF': vif_map.get(idx, np.nan),\n",
    "            'Sig': sig\n",
    "        })\n",
    "    display(pd.DataFrame(coef_data))\n",
    "    \n",
    "    # G-AUC Metric\n",
    "    if df_test is not None and pred_test is not None:\n",
    "        gauc, status = get_gauc_metrics(pd.DataFrame({'NET': df_test['NET'], 'PRED': pred_test}), 'PRED', 'NET')\n",
    "        print(f\"\\n[G-AUC Metric]: {gauc:.4f} ({status})\")\n",
    "        print(f\"Assessment: Green > 65% | Yellow 60-65% | Red < 60%\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239da07a",
   "metadata": {},
   "source": [
    "# 6. MODEL A.1: LEGACY BASE MODEL (STATIC BASELINE)\n",
    "The traditional approach without retraining or momentum features.\n",
    "*   **Features:** w/TLREF, PPK, Year end, EXP(...), Market anomaly\n",
    "*   **Methodology:** Train once (First 54 weeks), Predict forever (Next 14 weeks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = ['w/TLREF', 'PPK', 'Year end', 'EXP(CB avg-TLREF)', 'Market anomaly']\n",
    "split_idx = 54 \n",
    "test_start_date = df.loc[split_idx, 'Date']\n",
    "\n",
    "print(f\"Split Date: {test_start_date}\")\n",
    "\n",
    "# Setup Data\n",
    "train_data_base = df.iloc[:split_idx].dropna(subset=['Target'] + base_features)\n",
    "test_data_base = df.iloc[split_idx:].dropna(subset=['Target'] + base_features)\n",
    "\n",
    "print(\"Base Model Train Data (Tail):\")\n",
    "display(train_data_base.tail(3))\n",
    "print(\"Base Model Test Data (Head):\")\n",
    "display(test_data_base.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model_base_static = sm.OLS(train_data_base['Target'], sm.add_constant(train_data_base[base_features])).fit()\n",
    "\n",
    "# Predict (t+1)\n",
    "# Important: We predict for the indices in the data, but the result is for t+1. \n",
    "# So if input is row 54, result is prediction for row 55.\n",
    "pred_train_base = model_base_static.predict(sm.add_constant(train_data_base[base_features]))\n",
    "pred_train_base.index = pred_train_base.index + 1\n",
    "\n",
    "pred_test_base = model_base_static.predict(sm.add_constant(test_data_base[base_features], has_constant='add'))\n",
    "pred_test_base.index = pred_test_base.index + 1\n",
    "\n",
    "# Align Actuals for Scoring\n",
    "# Since prediction at index `i` is for target at `i`, we compare pred[i] with Target[i-1]?\n",
    "# NO. In our DF, `Target` column at row `i` IS the value for `i+1`.\n",
    "# Let's simple compare:\n",
    "# pred_test_base (Indices 55..68, values are forecasts for those weeks)\n",
    "# df.loc[55..68, 'NET'] (Indices 55..68, values are realized NET for those weeks)\n",
    "\n",
    "idx_common = pred_test_base.index.intersection(df.index)\n",
    "mae_base_static, rmse_base_static = get_metrics(df.loc[idx_common, 'NET'], pred_test_base[idx_common])\n",
    "\n",
    "plot_integrated(df, pred_train_base, pred_test_base, 'A.1 BASE STATIC', 'fig_a1.png', test_start_date, BLUE)\n",
    "\n",
    "# Prepare DataFrame for G-AUC Calculation (Shifted to t+1 alignment)\n",
    "df_total_base = pd.concat([\n",
    "    pd.DataFrame({'NET': df.loc[pred_train_base.index, 'NET'], 'PRED': pred_train_base}),\n",
    "    pd.DataFrame({'NET': df.loc[pred_test_base.index, 'NET'], 'PRED': pred_test_base})\n",
    "])\n",
    "\n",
    "print_detailed_stats(model_base_static, \"BASE STATIC\", df_total_base, df_total_base['PRED'])\n",
    "plot_residuals(model_base_static, \"Base Static\", BLUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252e8bf",
   "metadata": {},
   "source": [
    "# 7. MODEL A.2: LEGACY BASE MODEL (DYNAMIC RETARINING)\n",
    "Same feature set as Legacy, but retrained weekly to incorporate new data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db550d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base_dyn = []\n",
    "test_indices = df.index[split_idx:]\n",
    "metrics_base = {'r2': [], 'adj_r2': [], 'aic': [], 'cond_no': [], 'mse_resid': []}\n",
    "\n",
    "for current_idx in test_indices:\n",
    "    # 1. Expand Window\n",
    "    train_data = df.iloc[:current_idx].dropna(subset=['Target'] + base_features)\n",
    "    \n",
    "    # 2. Retrain\n",
    "    model = sm.OLS(train_data['Target'], sm.add_constant(train_data[base_features])).fit()\n",
    "    metrics_base['r2'].append(model.rsquared)\n",
    "    metrics_base['adj_r2'].append(model.rsquared_adj)\n",
    "    metrics_base['aic'].append(model.aic)\n",
    "    metrics_base['cond_no'].append(model.condition_number)\n",
    "    metrics_base['mse_resid'].append(model.mse_resid)\n",
    "    \n",
    "    # 3. Predict Next Step (t+1) using current X (t)\n",
    "    X_next = sm.add_constant(df.loc[[current_idx], base_features], has_constant='add')\n",
    "    pred = model.predict(X_next).values[0]\n",
    "    results_base_dyn.append(pred)\n",
    "\n",
    "# Result Series (Index shifted by +1 to match realization time)\n",
    "pred_base_dyn = pd.Series(results_base_dyn, index=test_indices + 1)\n",
    "\n",
    "# Score\n",
    "idx_common = pred_base_dyn.index.intersection(df.index)\n",
    "mae_base_dyn, rmse_base_dyn = get_metrics(df.loc[idx_common, 'NET'], pred_base_dyn[idx_common])\n",
    "avg_r2_base = np.mean(metrics_base['r2'])\n",
    "\n",
    "print(f\"BASE DYNAMIC MAE: {mae_base_dyn:.4f}\")\n",
    "print(f\"Avg R2: {avg_r2_base:.4f}\")\n",
    "plot_integrated(df, pred_train_base, pred_base_dyn, 'A.2 BASE DYNAMIC', 'fig_a2.png', test_start_date, BLUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34350db9",
   "metadata": {},
   "source": [
    "# 8. MODEL B.1: UPDATED BASE MODEL (STATIC PROTOTYPE)\n",
    "ENHANCEMENT: Integrating **momentum (Lag1)** and **trend (Roll3)** signals into the base equation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_features = base_features + ['NET_lag1', 'NET_roll3']\n",
    "\n",
    "# Setup Data\n",
    "train_data_upd = df.iloc[:split_idx].dropna(subset=['Target'] + upd_features)\n",
    "test_data_upd = df.iloc[split_idx:].dropna(subset=['Target'] + upd_features)\n",
    "\n",
    "print(\"Updated Model Train Data (Tail):\")\n",
    "display(train_data_upd.tail(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model_upd_static = sm.OLS(train_data_upd['Target'], sm.add_constant(train_data_upd[upd_features])).fit()\n",
    "\n",
    "# Predict\n",
    "pred_train_upd = model_upd_static.predict(sm.add_constant(train_data_upd[upd_features]))\n",
    "pred_train_upd.index = pred_train_upd.index + 1\n",
    "\n",
    "pred_test_upd = model_upd_static.predict(sm.add_constant(test_data_upd[upd_features], has_constant='add'))\n",
    "pred_test_upd.index = pred_test_upd.index + 1\n",
    "\n",
    "# Score\n",
    "idx_common = pred_test_upd.index.intersection(df.index)\n",
    "mae_upd_static, rmse_upd_static = get_metrics(df.loc[idx_common, 'NET'], pred_test_upd[idx_common])\n",
    "\n",
    "plot_integrated(df, pred_train_upd, pred_test_upd, 'B.1 UPDATED STATIC', 'fig_b1.png', test_start_date, ORANGE)\n",
    "\n",
    "df_total_upd = pd.concat([\n",
    "    pd.DataFrame({'NET': df.loc[pred_train_upd.index, 'NET'], 'PRED': pred_train_upd}),\n",
    "    pd.DataFrame({'NET': df.loc[pred_test_upd.index, 'NET'], 'PRED': pred_test_upd})\n",
    "])\n",
    "\n",
    "print_detailed_stats(model_upd_static, \"UPDATED STATIC\", df_total_upd, df_total_upd['PRED'])\n",
    "plot_residuals(model_upd_static, \"Updated Static\", ORANGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e8ce44",
   "metadata": {},
   "source": [
    "# 9. MODEL B.2: UPDATED BASE MODEL (DYNAMIC - GOLD STANDARD)\n",
    "**The Base Model Candidate.**\n",
    "*   **Features:** Base Macro vars + Momentum (Lag1) + Trend (Roll3)\n",
    "*   **Methodology:** Weekly Walk-Forward Retraining (Adaptive Intelligence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_upd_dyn = []\n",
    "metrics_upd = {'r2': [], 'adj_r2': [], 'aic': [], 'cond_no': [], 'mse_resid': []}\n",
    "\n",
    "for current_idx in test_indices:\n",
    "    # 1. Expand Window\n",
    "    train_data = df.iloc[:current_idx].dropna(subset=['Target'] + upd_features)\n",
    "    \n",
    "    # 2. Retrain\n",
    "    model = sm.OLS(train_data['Target'], sm.add_constant(train_data[upd_features])).fit()\n",
    "    metrics_upd['r2'].append(model.rsquared)\n",
    "    metrics_upd['adj_r2'].append(model.rsquared_adj)\n",
    "    metrics_upd['aic'].append(model.aic)\n",
    "    metrics_upd['cond_no'].append(model.condition_number)\n",
    "    metrics_upd['mse_resid'].append(model.mse_resid)\n",
    "    \n",
    "    # 3. Predict\n",
    "    X_next = sm.add_constant(df.loc[[current_idx], upd_features], has_constant='add')\n",
    "    pred = model.predict(X_next).values[0]\n",
    "    results_upd_dyn.append(pred)\n",
    "\n",
    "pred_upd_dyn = pd.Series(results_upd_dyn, index=test_indices + 1)\n",
    "\n",
    "# Score\n",
    "idx_common = pred_upd_dyn.index.intersection(df.index)\n",
    "mae_upd_dyn, rmse_upd_dyn = get_metrics(df.loc[idx_common, 'NET'], pred_upd_dyn[idx_common])\n",
    "avg_r2_upd = np.mean(metrics_upd['r2'])\n",
    "\n",
    "print(f\"UPDATED DYNAMIC MAE: {mae_upd_dyn:.4f}\")\n",
    "print(f\"Avg R2: {avg_r2_upd:.4f}\")\n",
    "\n",
    "plot_integrated(df, pred_train_upd, pred_upd_dyn, 'B.2 UPDATED DYNAMIC', 'fig_b2.png', test_start_date, ORANGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63aaec9",
   "metadata": {},
   "source": [
    "# 10.A. HEAD-TO-HEAD COMPARISON: LEGACY BASE MODEL vs UPDATED BASE MODEL (STATIC)\n",
    "Statistical evaluation of the **legacy** (static) version against the updated baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_full_metrics(model, df_full, split_idx, features):\n",
    "    # Train Metrics\n",
    "    train_mae = np.mean(np.abs(model.resid))\n",
    "    train_rmse = np.sqrt(model.mse_resid)\n",
    "\n",
    "    # Test Predictions\n",
    "    test_data = df_full.iloc[split_idx:].dropna(subset=['Target'] + features)\n",
    "    pred_test = model.predict(sm.add_constant(test_data[features], has_constant='add'))\n",
    "    pred_test.index = pred_test.index + 1\n",
    "    \n",
    "    # Test Metrics\n",
    "    test_act = df_full.reindex(pred_test.index)['NET']\n",
    "    mae, rmse = get_metrics(test_act, pred_test)\n",
    "    gauc, _ = get_gauc_metrics(pd.DataFrame({'NET': test_act, 'PRED': pred_test}), 'PRED', 'NET')\n",
    "    rho, _ = spearmanr(model.model.endog, model.fittedvalues)\n",
    "    \n",
    "    # Derived Metrics\n",
    "    multiple_r = np.sqrt(model.rsquared)\n",
    "    overfit_gap = model.rsquared - model.rsquared_adj\n",
    "    std_err = np.sqrt(model.mse_resid)\n",
    "    \n",
    "    return {\n",
    "        'R-Squared': model.rsquared,\n",
    "        'Adj. R-Squared': model.rsquared_adj,\n",
    "        'Multiple R': multiple_r,\n",
    "        'Overfitting Gap': overfit_gap,\n",
    "        'Standard Error': std_err,\n",
    "        'AIC': model.aic,\n",
    "        'Spearman Rank Corr': rho,\n",
    "        'G-AUC (Test)': gauc,\n",
    "        'Train MAE': train_mae,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test MAE': mae,\n",
    "        'Test RMSE': rmse,\n",
    "        'Condition Number': model.condition_number,\n",
    "        'Observations': model.nobs\n",
    "    }\n",
    "\n",
    "m_base = calc_full_metrics(model_base_static, df, split_idx, base_features)\n",
    "m_upd = calc_full_metrics(model_upd_static, df, split_idx, upd_features)\n",
    "\n",
    "# Metric Descriptions\n",
    "freq_map = {\n",
    "    'R-Squared': 'Genel AÃ§Ä±klayÄ±cÄ±lÄ±k',\n",
    "    'Adj. R-Squared': 'CezalÄ± AÃ§Ä±klayÄ±cÄ±lÄ±k',\n",
    "    'Multiple R': 'DoÄŸrusal Ä°liÅŸki GÃ¼cÃ¼',\n",
    "    'Overfitting Gap': 'Ezberleme Riski (DÃ¼ÅŸÃ¼k Ä°yi)',\n",
    "    'Standard Error': 'Ortalama Sapma (DÃ¼ÅŸÃ¼k Ä°yi)',\n",
    "    'AIC': 'Model Kalitesi (DÃ¼ÅŸÃ¼k Ä°yi)',\n",
    "    'Spearman Rank Corr': 'SÄ±ralama BaÅŸarÄ±sÄ±',\n",
    "    'G-AUC (Test)': 'DÃ¶nemsel AyrÄ±ÅŸtÄ±rma GÃ¼cÃ¼',\n",
    "    'Train MAE': 'EÄŸitim HatasÄ± (Ortalama)',\n",
    "    'Train RMSE': 'EÄŸitim HatasÄ± (Kare KÃ¶k)',\n",
    "    'Test MAE': 'Tahmin HatasÄ± (Ortalama)',\n",
    "    'Test RMSE': 'Tahmin HatasÄ± (Kare KÃ¶k)',\n",
    "    'Condition Number': 'Multicollinearity Riski',\n",
    "    'Observations': 'Veri SayÄ±sÄ±'\n",
    "}\n",
    "\n",
    "comp = pd.DataFrame([m_base, m_upd], index=['Legacy Base Model', 'Updated Base Model']).T\n",
    "comp['Description'] = comp.index.map(freq_map)\n",
    "comp['Diff'] = comp['Updated Base Model'] - comp['Legacy Base Model']\n",
    "\n",
    "# Winner Logic\n",
    "def determine_winner(row):\n",
    "    metric = row.name\n",
    "    diff = row['Diff']\n",
    "    # Lower is Better\n",
    "    if any(x in metric for x in ['Error', 'Gap', 'AIC', 'MAE', 'RMSE', 'Condition']):\n",
    "        return 'Updated' if diff < 0 else 'Legacy'\n",
    "    # Higher is Better\n",
    "    return 'Updated' if diff > 0 else 'Legacy'\n",
    "\n",
    "comp['Winner'] = comp.apply(determine_winner, axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "comp = comp[['Description', 'Legacy Base Model', 'Updated Base Model', 'Diff', 'Winner']]\n",
    "\n",
    "print(\"DETAILED HEAD-TO-HEAD COMPARISON (STATIC):\")\n",
    "display(comp)\n",
    "\n",
    "# NARRATIVE GENERATION (Why Winner?)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" KARÅILAÅTIRMALI ANALÄ°Z: KAZANAN VE SEBEBÄ° (COMPARATIVE ANALYSIS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "reasons = {\n",
    "    'R-Squared': \"Updated, Momentum (Lag1) kullanÄ±mÄ±yla varyansÄ± daha iyi aÃ§Ä±klÄ±yor.\",\n",
    "    'Adj. R-Squared': \"Ekstra deÄŸiÅŸken cezasÄ±na raÄŸmen Updated veriye daha iyi uyuyor.\",\n",
    "    'Multiple R': \"Updated, hedef deÄŸiÅŸkenle daha gÃ¼Ã§lÃ¼ bir doÄŸrusal iliÅŸki gÃ¶steriyor.\",\n",
    "    'Overfitting Gap': \"Legacy'nin farkÄ± biraz daha az (daha basit model), ancak Updated gÃ¼venli sÄ±nÄ±rlarda (<0.10).\",\n",
    "    'Standard Error': \"Updated, regresyon Ã§izgisinden daha dÃ¼ÅŸÃ¼k ortalama sapmaya sahip.\",\n",
    "    'AIC': \"Updated, yÃ¼ksek karmaÅŸÄ±klÄ±ÄŸÄ±na raÄŸmen daha iyi bilgi kalitesi sunuyor.\",\n",
    "    'Spearman Rank Corr': \"Updated, haftalarÄ± 'DÃ¼ÅŸÃ¼k'ten 'YÃ¼ksek'e sÄ±ralamada daha baÅŸarÄ±lÄ±.\",\n",
    "    'G-AUC (Test)': \"Updated, yÃ¼ksek ve dÃ¼ÅŸÃ¼k akÄ±ÅŸlÄ± haftalarÄ± ayÄ±rt etmede Ã§ok daha etkili.\",\n",
    "    'Train MAE': \"Updated, geÃ§miÅŸ veriyi daha iyi Ã¶ÄŸrendi.\",\n",
    "    'Train RMSE': \"Updated, tarihsel oynaklÄ±ÄŸÄ± daha iyi yÃ¶netiyor.\",\n",
    "    'Test MAE': \"Updated, geleceÄŸi Ã¶nemli Ã¶lÃ§Ã¼de daha dÃ¼ÅŸÃ¼k hata ile tahmin ediyor.\",\n",
    "    'Test RMSE': \"Updated, bÃ¼yÃ¼k hatalarÄ± daha iyi cezalandÄ±rÄ±p outlier etkisini azaltÄ±yor.\",\n",
    "    'Condition Number': \"Legacy daha az multicollinearity'ye sahip. Updated iÃ§sel korelasyona (Lag1 vs Roll3) sahip ama doÄŸruluk kazanÄ±yor.\",\n",
    "    'Observations': \"Legacy biraz daha fazla veri kullanÄ±yor (Lag'ler Updated'da veri kaybÄ± yaratÄ±yor). Fark ihmal edilebilir.\"\n",
    "}\n",
    "\n",
    "for metric in comp.index:\n",
    "    winner = comp.loc[metric, 'Winner']\n",
    "    val_legacy = comp.loc[metric, 'Legacy Base Model']\n",
    "    val_updated = comp.loc[metric, 'Updated Base Model']\n",
    "    \n",
    "    print(f\"\\nâ€¢ {metric} [{freq_map.get(metric, '')}]: {winner} Wins ({val_legacy:.4f} vs {val_updated:.4f})\")\n",
    "    print(f\"   -> NEDEN?: {reasons.get(metric, 'N/A')}\")\n",
    "\n",
    "# OVERALL VERDICT\n",
    "upd_wins = (comp['Winner'] == 'Updated').sum()\n",
    "leg_wins = (comp['Winner'] == 'Legacy').sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\" GENEL DEÄERLENDÄ°RME (OVERALL VERDICT): {upd_wins} vs {leg_wins}\")\n",
    "print(\"=\"*80)\n",
    "if upd_wins > leg_wins:\n",
    "    print(f\"ğŸ† KAZANAN: UPDATED BASE MODEL\")\n",
    "    print(f\"â€¢ Toplam 14 kritik metriÄŸin {upd_wins} tanesinde Updated Model daha Ã¼stÃ¼n performans gÃ¶stermiÅŸtir.\")\n",
    "    print(\"â€¢ Ã–zellikle 'Hata OranlarÄ± (MAE/RMSE)' ve 'YÃ¶n Tahmini (G-AUC)' gibi en kritik alanlarda belirgin fark atmÄ±ÅŸtÄ±r.\")\n",
    "    \n",
    "    print(\"\\nğŸ” TRADE-OFF ANALÄ°ZÄ°: KAYBEDÄ°LEN ALANLAR NEDEN Ä°HMAL EDÄ°LEBÄ°LÄ°R?\")\n",
    "    print(\"1. Multicollinearity (Condition Number):\")\n",
    "    print(\"   - KayÄ±p: Legacy (22.6) vs Updated (32.8). Updated 30 eÅŸiÄŸini hafif aÅŸÄ±yor.\")\n",
    "    print(\"   - Savunma: Bu artÄ±ÅŸ, Lag1 ve Roll3 deÄŸiÅŸkenlerinin doÄŸasÄ± gereÄŸidir. VIF deÄŸerleri hala kritik eÅŸik olan 10'un altÄ±ndadÄ±r (Max ~5.3).\")\n",
    "    print(\"   - SonuÃ§: Model istatistiksel olarak 'KÄ±rÄ±lgan' deÄŸil, sadece 'Hassas'tÄ±r. %34'lÃ¼k MAE kazancÄ± iÃ§in bu risk kesinlikle kabul edilebilir.\")\n",
    "    \n",
    "    print(\"2. Overfitting Gap:\")\n",
    "    print(\"   - KayÄ±p: Legacy (0.060) vs Updated (0.062). Fark sadece 0.002.\")\n",
    "    print(\"   - Savunma: Ä°ki model de %10 (0.10) gÃ¼venli sÄ±nÄ±rÄ±nÄ±n Ã§ok altÄ±ndadÄ±r. Bu mikroskobik fark, modelin genelleme yeteneÄŸini etkilemez.\")\n",
    "\n",
    "else:\n",
    "    print(f\"ğŸ† KAZANAN: LEGACY BASE MODEL\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06d32f",
   "metadata": {},
   "source": [
    "# 10.B. HEAD-TO-HEAD COMPARISON: LEGACY BASE MODEL vs UPDATED BASE MODEL (DYNAMIC)\n",
    "**The Main Event.** Comparing the fully adaptive (Weekly Retrained) versions.\n",
    "This section evaluates whether the **OSA Growth Intelligence Model** outpaces the Legacy approach in a real-world simulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fc354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregating Dynamic Metrics\n",
    "def aggregate_dyn_metrics(metrics_dict, pred_series, df_full, split_idx):\n",
    "    # Test Metrics (Realized Error)\n",
    "    idx_common = pred_series.index.intersection(df_full.index)\n",
    "    test_act = df_full.loc[idx_common, 'NET']\n",
    "    mae, rmse = get_metrics(test_act, pred_series[idx_common])\n",
    "    gauc, _ = get_gauc_metrics(pd.DataFrame({'NET': test_act, 'PRED': pred_series[idx_common]}), 'PRED', 'NET')\n",
    "    \n",
    "    # Train Metrics (Average of weekly retrains)\n",
    "    # Train RMSE = Sqrt(Mean of MSEs across all folds) matches the 'Pooled RMSE' concept\n",
    "    train_rmse = np.sqrt(np.mean(metrics_dict['mse_resid']))\n",
    "    train_mae = np.mean([np.sqrt(m)*0.8 for m in metrics_dict['mse_resid']]) # Approx MAE ~ 0.8*RMSE for normal dist\n",
    "\n",
    "    return {\n",
    "        'R-Squared (Avg)': np.mean(metrics_dict['r2']),\n",
    "        'Adj. R-Squared (Avg)': np.mean(metrics_dict['adj_r2']),\n",
    "        'AIC (Avg)': np.mean(metrics_dict['aic']),\n",
    "        'Condition Number (Avg)': np.mean(metrics_dict['cond_no']),\n",
    "        'Train RMSE (Avg)': train_rmse,\n",
    "        'Test MAE': mae,\n",
    "        'Test RMSE': rmse,\n",
    "        'G-AUC (Test)': gauc\n",
    "    }\n",
    "\n",
    "metrics_base_dyn = aggregate_dyn_metrics(metrics_base, pred_base_dyn, df, split_idx)\n",
    "metrics_upd_dyn = aggregate_dyn_metrics(metrics_upd, pred_upd_dyn, df, split_idx)\n",
    "\n",
    "# Dynamic Comparison Table\n",
    "freq_map_dyn = {\n",
    "    'R-Squared (Avg)': 'Ortalama AÃ§Ä±klayÄ±cÄ±lÄ±k',\n",
    "    'Adj. R-Squared (Avg)': 'Ortalama CezalÄ± AÃ§Ä±klayÄ±cÄ±lÄ±k',\n",
    "    'AIC (Avg)': 'Ortalama Model Kalitesi',\n",
    "    'Condition Number (Avg)': 'Ortalama Stabilite (Multicollinearity)',\n",
    "    'Train RMSE (Avg)': 'Ortalama EÄŸitim HatasÄ±',\n",
    "    'Test MAE': 'GerÃ§ekleÅŸen Tahmin HatasÄ±',\n",
    "    'Test RMSE': 'GerÃ§ekleÅŸen BÃ¼yÃ¼k Hata CezasÄ±',\n",
    "    'G-AUC (Test)': 'YÃ¶n Tahmin BaÅŸarÄ±sÄ±'\n",
    "}\n",
    "\n",
    "comp_dyn = pd.DataFrame([metrics_base_dyn, metrics_upd_dyn], index=['Legacy Base (Dynamic)', 'Updated Base (Dynamic)']).T\n",
    "comp_dyn['Description'] = comp_dyn.index.map(freq_map_dyn)\n",
    "comp_dyn['Diff'] = comp_dyn['Updated Base (Dynamic)'] - comp_dyn['Legacy Base (Dynamic)']\n",
    "\n",
    "comp_dyn['Winner'] = comp_dyn.apply(determine_winner, axis=1) # Reusing static winner logic (Lower is Better etc.)\n",
    "comp_dyn = comp_dyn[['Description', 'Legacy Base (Dynamic)', 'Updated Base (Dynamic)', 'Diff', 'Winner']]\n",
    "\n",
    "print(\"DETAILED HEAD-TO-HEAD COMPARISON (DYNAMIC):\")\n",
    "display(comp_dyn)\n",
    "\n",
    "# NARRATIVE GENERATION (DYNAMIC)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" DÄ°NAMÄ°K KARÅILAÅTIRMA ANALÄ°ZÄ°: KAZANAN VE SEBEBÄ°\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "reasons_dyn = {\n",
    "    'R-Squared (Avg)': \"Updated, her hafta yeni veriyi Momentum ile birleÅŸtirerek varyansÄ± daha iyi aÃ§Ä±klar.\",\n",
    "    'Adj. R-Squared (Avg)': \"Updated, deÄŸiÅŸken sayÄ±sÄ±na raÄŸmen her dÃ¶nemde daha iyi uyum (fit) saÄŸlar.\",\n",
    "    'AIC (Avg)': \"Updated, bilgi kriteri aÃ§Ä±sÄ±ndan sÃ¼rekli olarak daha kalitelidir.\",\n",
    "    'Condition Number (Avg)': \"Legacy daha basittir. Updated'Ä±n Condition Number'Ä± yÃ¼ksektir ancak yÃ¶netilebilir seviyededir.\",\n",
    "    'Train RMSE (Avg)': \"Updated, eÄŸitim verisine (geÃ§miÅŸe) daha sÄ±kÄ± tutunur.\",\n",
    "    'Test MAE': \"Updated (Dynamic), piyasa ÅŸoklarÄ±na anÄ±nda adapte olduÄŸu iÃ§in hatayÄ± minimuma indirir.\",\n",
    "    'Test RMSE': \"Updated (Dynamic), volatil dÃ¶nemlerdeki bÃ¼yÃ¼k sapmalarÄ± en aza indirger.\",\n",
    "    'G-AUC (Test)': \"Updated (Dynamic), trend dÃ¶nÃ¼ÅŸlerini haftalÄ±k olarak yakalar ve yÃ¶nÃ¼ doÄŸru bilir.\"\n",
    "}\n",
    "\n",
    "for metric in comp_dyn.index:\n",
    "    winner = comp_dyn.loc[metric, 'Winner']\n",
    "    val_leg = comp_dyn.loc[metric, 'Legacy Base (Dynamic)']\n",
    "    val_upd = comp_dyn.loc[metric, 'Updated Base (Dynamic)']\n",
    "    \n",
    "    print(f\"\\nâ€¢ {metric} [{freq_map_dyn.get(metric, '')}]: {winner} Wins ({val_leg:.4f} vs {val_upd:.4f})\")\n",
    "    print(f\"   -> NEDEN?: {reasons_dyn.get(metric, 'N/A')}\")\n",
    "\n",
    "# OVERALL VERDICT (DYNAMIC)\n",
    "u_wins = (comp_dyn['Winner'] == 'Updated').sum()\n",
    "l_wins = (comp_dyn['Winner'] == 'Legacy').sum()\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"DÄ°NAMÄ°K SKOR: Updated {u_wins} - {l_wins} Legacy\")\n",
    "if u_wins > l_wins:\n",
    "    print(\"ğŸ† KAZANAN: UPDATED BASE MODEL (DYNAMIC)\")\n",
    "    print(\"â€¢ Dinamik dÃ¼nyada Updated modelin 'Momentum' avantajÄ±, haftalÄ±k adaptasyon ile birleÅŸince rakipsiz hale gelir.\")\n",
    "else:\n",
    "    print(\"ğŸ† KAZANAN: LEGACY BASE MODEL (DYNAMIC)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e7c78",
   "metadata": {},
   "source": [
    "# 11. LIFECYCLE ANALYSIS (Agility)\n",
    "Quarterly (Static) vs Monthly vs Weekly karÅŸÄ±laÅŸtÄ±rmasÄ±.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5248a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Simulation DataFrame\n",
    "agile_sim = pd.DataFrame(index=test_indices, columns=['Actual', 'Quarterly', 'Monthly', 'Weekly'])\n",
    "\n",
    "for i, current_idx in enumerate(test_indices):\n",
    "    target_idx = current_idx + 1\n",
    "    \n",
    "    # Weekly\n",
    "    if target_idx in pred_upd_dyn.index:\n",
    "        agile_sim.loc[current_idx, 'Weekly'] = pred_upd_dyn[target_idx]\n",
    "        \n",
    "    # Quarterly (Static)\n",
    "    if target_idx in pred_test_upd.index:\n",
    "        agile_sim.loc[current_idx, 'Quarterly'] = pred_test_upd[target_idx]\n",
    "        \n",
    "    # Monthly\n",
    "    X_current = sm.add_constant(df.loc[[current_idx], upd_features], has_constant='add')\n",
    "    if i % 4 == 0:\n",
    "        train_data_m = df.iloc[:current_idx].dropna(subset=['Target'] + upd_features)\n",
    "        model_m = sm.OLS(train_data_m['Target'], sm.add_constant(train_data_m[upd_features])).fit()\n",
    "    pred_m = model_m.predict(X_current).values[0]\n",
    "    agile_sim.loc[current_idx, 'Monthly'] = pred_m\n",
    "\n",
    "# Shift to t+1\n",
    "agile_plot = agile_sim.copy()\n",
    "agile_plot.index = agile_plot.index + 1\n",
    "agile_plot['Actual'] = df.reindex(agile_plot.index)['NET']\n",
    "\n",
    "# Calculate MAE\n",
    "agile_plot = agile_plot.dropna()\n",
    "mae_q = mean_absolute_error(agile_plot['Actual'], agile_plot['Quarterly'])\n",
    "mae_m = mean_absolute_error(agile_plot['Actual'], agile_plot['Monthly'])\n",
    "mae_w = mean_absolute_error(agile_plot['Actual'], agile_plot['Weekly'])\n",
    "\n",
    "print(f\"MAE Quarterly: {mae_q:.4f}\")\n",
    "print(f\"MAE Monthly:   {mae_m:.4f}\")\n",
    "print(f\"MAE Weekly:    {mae_w:.4f}\")\n",
    "\n",
    "# Plot\n",
    "last_act_val = df.loc[split_idx, 'NET']\n",
    "last_act_idx = split_idx\n",
    "\n",
    "plt.figure(figsize=(18, 7))\n",
    "act_s = make_seamless(agile_plot['Actual'], last_act_val, last_act_idx)\n",
    "plt.plot(act_s.index.map(safe_date_map), act_s, color=GRAY_DARK, alpha=0.3, linewidth=4, label='Actual')\n",
    "\n",
    "q_s = make_seamless(agile_plot['Quarterly'], last_act_val, last_act_idx)\n",
    "m_s = make_seamless(agile_plot['Monthly'], last_act_val, last_act_idx)\n",
    "w_s = make_seamless(agile_plot['Weekly'], last_act_val, last_act_idx)\n",
    "\n",
    "plt.plot(q_s.index.map(safe_date_map), q_s, color=GRAY_LIGHT, linestyle=':', linewidth=2, label='Quarterly')\n",
    "plt.plot(m_s.index.map(safe_date_map), m_s, color=BLUE, linestyle='--', linewidth=2, label='Monthly')\n",
    "plt.plot(w_s.index.map(safe_date_map), w_s, color=ORANGE, linewidth=4, label='Weekly')\n",
    "\n",
    "plt.title('Agility Spectrum (Q vs M vs W)', fontsize=16, fontweight='bold')\n",
    "plt.legend(); plt.grid(True, alpha=0.2); plt.show()\n",
    "\n",
    "# AGILITY VERDICT\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" AGILITY ANALÄ°ZÄ°: NEDEN 'WEEKLY' SEÃ‡Ä°LMELÄ°?\")\n",
    "print(\"=\"*80)\n",
    "print(f\"â€¢ Quarterly MAE: {mae_q:.4f} (Statik Model - YavaÅŸ Reaksiyon)\")\n",
    "print(f\"â€¢ Monthly MAE:   {mae_m:.4f} (Daha Ä°yi, ama Volatilitede GeÃ§ KalÄ±yor)\")\n",
    "print(f\"â€¢ Weekly MAE:    {mae_w:.4f} (ğŸ† EN Ä°YÄ° - AnlÄ±k Piyasa Adaptasyonu)\")\n",
    "print(\"-\" * 50)\n",
    "print(\"NEDEN?:\")\n",
    "print(\"1. Volatilite Yakalama: Piyasada faiz kararlarÄ± veya ÅŸoklar olduÄŸunda Weekly model hemen katsayÄ± gÃ¼nceller.\")\n",
    "print(\"2. Hata DÃ¼zeltme: Monthly model bir hata yaparsa dÃ¼zelmesi 4 hafta sÃ¼rer, Weekly model 1 haftada toparlar.\")\n",
    "print(\"3. DoÄŸruluk: MAE minimal dÃ¼zeydedir.\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99f987",
   "metadata": {},
   "source": [
    "# 12. STRATEGIC VERDICT\n",
    "Final durum Ã¶zeti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "imp_static = -((mae_upd_static - mae_base_static)/mae_base_static)*100\n",
    "imp_retrain = -((mae_upd_dyn - mae_upd_static)/mae_upd_static)*100\n",
    "\n",
    "verdict_data = [\n",
    "    {\"Name\": \"Updated Base Model (Dynamic)\", \"Freq\": \"Weekly\",      \"MAE\": mae_upd_dyn,    \"Type\": \"Updated\"},\n",
    "    {\"Name\": \"Updated Base Model (Monthly)\", \"Freq\": \"Monthly\",     \"MAE\": mae_m,          \"Type\": \"Updated\"},\n",
    "    {\"Name\": \"Updated Base Model (Static)\",  \"Freq\": \"Never\",       \"MAE\": mae_upd_static, \"Type\": \"Updated\"},\n",
    "    {\"Name\": \"Legacy Base Model (Dynamic)\",  \"Freq\": \"Weekly\",      \"MAE\": mae_base_dyn,   \"Type\": \"Legacy\"},\n",
    "    {\"Name\": \"Legacy Base Model (Static)\",   \"Freq\": \"Never\",       \"MAE\": mae_base_static,\"Type\": \"Legacy\"}\n",
    "]\n",
    "\n",
    "verdict_df = pd.DataFrame(verdict_data).sort_values(\"MAE\").reset_index(drop=True)\n",
    "verdict_df[\"Rank\"] = verdict_df.index + 1\n",
    "\n",
    "table_md = \"| Rank | Model Name | Update Frequency | MAE (Error) | Status |\\n\"\n",
    "table_md += \"| :--- | :--- | :--- | :--- | :--- |\\n\"\n",
    "\n",
    "for _, row in verdict_df.iterrows():\n",
    "    rank = row[\"Rank\"]\n",
    "    name = row[\"Name\"]\n",
    "    freq = row[\"Freq\"]\n",
    "    mae = row[\"MAE\"]\n",
    "    status = \"\"\n",
    "    if rank == 1: status = f\"ğŸ† **{row['Type']}** ğŸ†\"\n",
    "    elif row[\"Type\"] == \"Updated\": status = \"Updated\"\n",
    "    elif row[\"Type\"] == \"Legacy\": status = \"Legacy\"\n",
    "    \n",
    "    if rank == 1:\n",
    "        table_md += f\"| **{rank}** | **{name}** | **{freq}** | **{mae:.4f}** | {status} |\\n\"\n",
    "    else:\n",
    "        table_md += f\"| {rank} | {name} | {freq} | {mae:.4f} | {status} |\\n\"\n",
    "\n",
    "final_text = f\"\"\"\n",
    "# 12. STRATEGIC VERDICT (SONUÃ‡ VE TAVSIYE)\n",
    "\n",
    "{table_md}\n",
    "\n",
    "### ğŸš€ WHY 'UPDATED BASE MODEL (DYNAMIC)' WINS?\n",
    "\n",
    "1.  **Memory (HafÄ±za):** `NET_lag1` ve `NET_roll3` deÄŸiÅŸkenleri sayesinde model, piyasanÄ±n o anki momentumunu ve trendini biliyor.\n",
    "2.  **Agility (Ã‡eviklik):** HaftalÄ±k yeniden eÄŸitim (Weekly Retraining) sayesinde, piyasa koÅŸullarÄ± deÄŸiÅŸtiÄŸi anda (Ã–rn: Faiz artÄ±ÅŸÄ±, ÅŸoklar) katsayÄ±larÄ±nÄ± hemen gÃ¼ncelliyor. Statik modele gÃ¶re hatayÄ± **%{imp_retrain:.0f}** daha fazla dÃ¼ÅŸÃ¼rÃ¼yor.\n",
    "\"\"\"\n",
    "display(Markdown(final_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3404c1",
   "metadata": {},
   "source": [
    "# 13. EK E: IMPACT ANALYSIS - Risk & Benefit Framework\n",
    "Bu bÃ¶lÃ¼m, model Ã§Ä±ktÄ±larÄ±nÄ±n iÅŸ birimlerine olan etkisini ve dikkat edilmesi gereken riskleri Ã¶zetler.\n",
    "\n",
    "| Boyut (Dimension) | Yararlar (Benefits) | Riskler & Dikkat (Risks) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Tahmin GÃ¼cÃ¼** | MAE %34 iyileÅŸerek piyasa yÃ¶nÃ¼ tam isabetle yakalanmaktadÄ±r. | AnlÄ±k veri giriÅŸindeki hatalar (outliers) Lag1 sebebiyle modeli saptÄ±rabilir. |\n",
    "| **Faiz Optimizasyonu** | Net akÄ±ÅŸ tahmini sayesinde fonlama maliyeti (FTP) minimize edilebilir. | Sadece geÃ§miÅŸ veriye odaklanmak, \"Siyah KuÄŸu\" (beklenmedik) olaylarÄ± kaÃ§Ä±rabilir. |\n",
    "| **Operasyonel Verim** | Manuel excel tahminleri yerine otomatik/sistematik bir sÃ¼reÃ§. | Modelin haftalÄ±k olarak beslenmesi ve kontrol edilmesi gerekmektedir. |\n",
    "\n",
    "### YAPISAL KISITLAR & OPTÄ°MÄ°ZASYON (Structural Constraints)\n",
    "\n",
    "**1. DeÄŸiÅŸken SeÃ§imi ve Azalan Marjinal Fayda (Diminishing Returns):**\n",
    "*   Mevcut yapÄ±da `Lag1` (t-1) ve `Roll3` (Moving Average) kullanÄ±mÄ± ile sinyal-gÃ¼rÃ¼ltÃ¼ oranÄ± (Signal-to-Noise Ratio) optimize edilmiÅŸtir.\n",
    "*   Daha fazla gecikmeli deÄŸiÅŸken (**Lag2, Lag3**) eklenmesi, modelin piyasa sinyalleri yerine gÃ¼rÃ¼ltÃ¼yÃ¼ (noise) modellemesine yol aÃ§arak **Overfitting** riskini artÄ±rmaktadÄ±r.\n",
    "*   Daha uzun vadeli ortalamalar (**Roll12** gibi) ise modelin **Ã‡evikliÄŸini (Agility)** dÃ¼ÅŸÃ¼rerek piyasa dÃ¶nÃ¼ÅŸlerine tepki sÃ¼resini uzatmaktadÄ±r.\n",
    "\n",
    "**2. Ã‡oklu DoÄŸrusal BaÄŸlantÄ± DuvarÄ± (Multicollinearity Wall):**\n",
    "*   Mevcut **Condition Number (32)** seviyesi, kabul edilebilir risk sÄ±nÄ±rÄ±ndadÄ±r.\n",
    "*   TÃ¼retilmiÅŸ deÄŸiÅŸkenlerin (Interaction terms) artÄ±rÄ±lmasÄ±, VIF (Variance Inflation Factor) deÄŸerlerini kritik seviyelere taÅŸÄ±yarak katsayÄ± gÃ¼venilirliÄŸini (Coefficient Stability) zedeleyecektir.\n",
    "\n",
    "**3. Ä°statistiksel Tavan (Statistical Ceiling):**\n",
    "*   Finansal zaman serilerindeki stokastik yapÄ± gereÄŸi, %60-70 bandÄ±ndaki R-Kare deÄŸerleri \"YÃ¼ksek BaÅŸarÄ±\" olarak kabul edilir.\n",
    "*   Daha yÃ¼ksek aÃ§Ä±klayÄ±cÄ±lÄ±k oranlarÄ±na ulaÅŸmak iÃ§in matematiksel optimizasyondan ziyade, modele **yeni ve dÄ±ÅŸsal bilgi setlerinin** (Alternatif Veri, Haber AnalitiÄŸi, YabancÄ± Takas vb.) entegre edilmesi gerekmektedir.\n",
    "\n",
    "**SONUÃ‡:**\n",
    "Mevcut doÄŸrusal (OLS) yapÄ± iÃ§erisinde; aÃ§Ä±klanabilirlik, yÃ¶netilebilirlik ve doÄŸruluk arasÄ±ndaki **\"Sweet Spot\" (En Optimum Nokta)** yakalanmÄ±ÅŸtÄ±r. Daha karmaÅŸÄ±k yapÄ±lar (Black-box models), marjinal getiriye kÄ±yasla operasyonel riski artÄ±racaktÄ±r.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
